static nvvm_device = 0;

fn @createAccDevice() = AccDevice {
	launch_1d = @|body|@|num_groups, group_size| nvvm_launch_1d(nvvm_device, num_groups, group_size, body),
	synchronize = || synchronize_cuda(nvvm_device),
	alloc = |size| alloc_cuda(nvvm_device, size),
	platform_device = runtime_device(1, nvvm_device),

	print_i32 = @|format: &[u8], arg: i32| {
		nvvm_vprintf(format, bitcast[&[u8]](&arg));
	},

	print_2xi32 = @|format: &[u8], arg1: i32, arg2: i32| {
		let args:&[i32] = [arg1, arg2];
		nvvm_vprintf(format, bitcast[&[u8]](args));
	},

	print_3xi32 = @|format: &[u8], arg1: i32, arg2: i32, arg3: i32| {
		let args:&[i32] = [arg1, arg2, arg3];
		nvvm_vprintf(format, bitcast[&[u8]](args));
	}
};

fn @nvvm_pred(b: bool) -> i32 {
	if b {1} else {0}
}

fn @nvvm_legacy_atomic_memory_order_wrap(order:memory_order, f: fn() -> ()) {
	match order {
		memory_order::release => nvvm_threadfence(),
		memory_order::acq_rel => nvvm_threadfence(),
		memory_order::seq_cst => nvvm_threadfence(),
		_ => ()
	}

	f();

	match order {
		memory_order::acquire => nvvm_threadfence(),
		memory_order::acq_rel => nvvm_threadfence(),
		memory_order::seq_cst => nvvm_threadfence(),
		_ => ()
	}
}

fn @nvvm_legacy_atomic_memory_order_wrap_load[T](f: fn(&addrspace(1) T) -> T) {
	@|location:&addrspace(1) T, order:memory_order| -> T {
		let mut res:T;
		nvvm_legacy_atomic_memory_order_wrap(order, @|| res = f(location));
		res
	}
}

fn @nvvm_legacy_atomic_memory_order_wrap_store[T](f: fn(&mut addrspace(1) T, T) -> ()) {
	@|location:&mut addrspace(1) T, value:T, order:memory_order| -> () {
		nvvm_legacy_atomic_memory_order_wrap(order, @|| f(location, value));
	}
}

fn @nvvm_atomic_rmw[T](op: u32) {
	@|location:&mut addrspace(1) T, value:T, order:memory_order| -> T {
		atomic_p1[T](op, location, value, builtin_memory_order(order), "")
	}
}

fn @nvvm_atomic_cas[T](location:&mut addrspace(1) T, expected: T, desired: T, memory_order_succ: memory_order, memory_order_fail: memory_order) -> T {
	match cmpxchg_p1[T](location, expected, desired, builtin_memory_order(stronger_memory_order(memory_order_succ, memory_order_fail)), "agent") {
		(old, _success) => old
	}
}

fn @nvvm_thread(idx: fn(i32) -> u32, gid: fn() -> u32, body: fn(gpu_thread_context) -> ()) -> () {
	@body(gpu_thread_context {
		idx = idx,

		gid = gid,

		atomic_load_global_i32 = nvvm_legacy_atomic_memory_order_wrap_load(@|location:&addrspace(1) i32| { let mut res:i32; asm("ld.volatile.s32 $0, [$1];" : "=r"(res) : "l"(location) : "memory"); res }),
		atomic_load_global_u32 = nvvm_legacy_atomic_memory_order_wrap_load(@|location:&addrspace(1) u32| { let mut res:u32; asm("ld.volatile.u32 $0, [$1];" : "=r"(res) : "l"(location) : "memory"); res }),
		atomic_load_global_u64 = nvvm_legacy_atomic_memory_order_wrap_load(@|location:&addrspace(1) u64| { let mut res:u64; asm("ld.volatile.u64 $0, [$1];" : "=l"(res) : "l"(location) : "memory"); res }),

		atomic_store_global_i32 = nvvm_legacy_atomic_memory_order_wrap_store(@|location:&mut addrspace(1) i32, value:i32| asm("st.volatile.s32 [$0], $1;" : : "l"(location), "r"(value) : "memory")),
		atomic_store_global_u32 = nvvm_legacy_atomic_memory_order_wrap_store(@|location:&mut addrspace(1) u32, value:u32| asm("st.volatile.u32 [$0], $1;" : : "l"(location), "r"(value) : "memory")),
		atomic_store_global_u64 = nvvm_legacy_atomic_memory_order_wrap_store(@|location:&mut addrspace(1) u64, value:u64| asm("st.volatile.u64 [$0], $1;" : : "l"(location), "l"(value) : "memory")),

		atomic_add_global_i32 = nvvm_atomic_rmw[i32](1),
		atomic_add_global_u32 = nvvm_atomic_rmw[u32](1),
		atomic_add_global_u64 = nvvm_atomic_rmw[u64](1),

		atomic_sub_global_i32 = nvvm_atomic_rmw[i32](2),
		atomic_sub_global_u32 = nvvm_atomic_rmw[u32](2),
		atomic_sub_global_u64 = nvvm_atomic_rmw[u64](2),

		atomic_and_global_i32 = nvvm_atomic_rmw[i32](3),
		atomic_and_global_u32 = nvvm_atomic_rmw[u32](3),
		atomic_and_global_u64 = nvvm_atomic_rmw[u64](3),

		atomic_or_global_i32 = nvvm_atomic_rmw[i32](5),
		atomic_or_global_u32 = nvvm_atomic_rmw[u32](5),
		atomic_or_global_u64 = nvvm_atomic_rmw[u64](5),

		atomic_xor_global_i32 = nvvm_atomic_rmw[i32](6),
		atomic_xor_global_u32 = nvvm_atomic_rmw[u32](6),
		atomic_xor_global_u64 = nvvm_atomic_rmw[u64](6),

		atomic_exch_global_i32 = nvvm_atomic_rmw[i32](0),
		atomic_exch_global_u32 = nvvm_atomic_rmw[u32](0),
		atomic_exch_global_u64 = nvvm_atomic_rmw[u64](0),

		atomic_min_global_i32 = nvvm_atomic_rmw[i32]( 8),
		atomic_min_global_u32 = nvvm_atomic_rmw[u32](10),
		atomic_min_global_u64 = nvvm_atomic_rmw[u64](10),

		atomic_max_global_i32 = nvvm_atomic_rmw[i32](7),
		atomic_max_global_u32 = nvvm_atomic_rmw[u32](9),
		atomic_max_global_u64 = nvvm_atomic_rmw[u64](9),

		atomic_cas_global_i32 = nvvm_atomic_cas[i32],
		atomic_cas_global_u32 = nvvm_atomic_cas[u32],
		atomic_cas_global_u64 = nvvm_atomic_cas[u64],

		atomic_inc_global_u32 = nvvm_atomic_inc_global_u32,
		atomic_dec_global_u32 = nvvm_atomic_dec_global_u32,

		memory_barrier = @|_:memory_order| nvvm_threadfence(),

		// yield = @|| nvvm_nanosleep(0)
		yield = @|_msg:&[u8]| nvvm_threadfence()
	});
}

fn @nvvm_subwarp(idx: fn() -> u32, gid: fn() -> u32, membermask: u32, num_threads: u32, body: fn(gpu_wave_context) -> ()) -> () {
	let thread_idx = @|i: i32| match i { 0 => nvvm_laneid(), _ => 0 };

	//  1 00000001 -> 00011111
	//  2 00000010 -> 00011110
	//  4 00000100 -> 00011100
	//  8 00001000 -> 00011000
	// 16 00010000 -> 00010000
	// 32 00100000 -> 00000000
	//
	fn @shuffle_group_mask(width: u32) -> u32 {
		(!(width - 1)) & 0x1F
	}

	fn @shuffle_mask_bound_upper(width: u32) -> i32 {
		((shuffle_group_mask(width) << 8) | 0x1F) as i32
	}

	fn @shuffle_mask_bound_lower(width: u32) -> i32 {
		(shuffle_group_mask(width) << 8) as i32
	}

	@body(gpu_wave_context{
		idx = idx,

		membermask = @|| membermask as u64,

		threads = @|body|@|| nvvm_thread(thread_idx, @|| gid() * num_threads + thread_idx(0), body),

		num_threads = @|| num_threads,

		barrier = @|| nvvm_warp_sync(membermask),
		barrier_all = @|predicate| nvvm_warp_sync_all(membermask, predicate),
		barrier_any = @|predicate| nvvm_warp_sync_any(membermask, predicate),
		barrier_count = @|predicate| nvvm_popc_i(nvvm_warp_sync_ballot(membermask, predicate) as i32),
		barrier_vote = @|predicate| nvvm_warp_sync_ballot(membermask, predicate) as u64,

		// activemask = nvvm_warp_activemask,

		shfl_i32 = @|x:i32, src_lane:i32, width:u32| nvvm_warp_shfl_sync_idx_i32(membermask, x, src_lane, shuffle_mask_bound_upper(width)),
		shfl_u32 = @|x:u32, src_lane:i32, width:u32| nvvm_warp_shfl_sync_idx_i32(membermask, x as i32, src_lane, shuffle_mask_bound_upper(width)) as u32,
		// shfl_i64 = @|x:i64, src_lane:i32, width:u32| nvvm_warp_shfl_i64(membermask, x, src_lane, shuffle_mask_bound_upper(width)),
		// shfl_u64 = @|x:u64, src_lane:i32, width:u32| nvvm_warp_shfl_u64(membermask, x, src_lane, shuffle_mask_bound_upper(width)),
		// shfl_f32 = @|x:f32, src_lane:i32, width:u32| nvvm_warp_shfl_sync_idx_f32(membermask, x, src_lane, shuffle_mask_bound_upper(width)),
		// shfl_f64 = @|x:f64, src_lane:i32, width:u32| nvvm_warp_shfl_f64(membermask, x, src_lane, shuffle_mask_bound_upper(width)),

		shfl_up_i32 = @|x:i32, delta:u32, width:u32| nvvm_warp_shfl_sync_up_i32(membermask, x, delta as i32, shuffle_mask_bound_lower(width)),
		shfl_up_u32 = @|x:u32, delta:u32, width:u32| nvvm_warp_shfl_sync_up_i32(membermask, x as i32, delta as i32, shuffle_mask_bound_lower(width)) as u32,
		// shfl_up_i64 = @|x:i64, delta:u32, width:u32| nvvm_warp_shfl_up_i64(membermask, x, delta, shuffle_mask_bound_lower(width)),
		// shfl_up_u64 = @|x:u64, delta:u32, width:u32| nvvm_warp_shfl_up_u64(membermask, x, delta, shuffle_mask_bound_lower(width)),
		// shfl_up_f32 = @|x:f32, delta:u32, width:u32| nvvm_warp_shfl_sync_up_f32(membermask, x, delta as i32, shuffle_mask_bound_lower(width)),
		// shfl_up_f64 = @|x:f64, delta:u32, width:u32| nvvm_warp_shfl_up_f64(membermask, x, delta, shuffle_mask_bound_lower(width)),

		shfl_down_i32 = @|x:i32, delta:u32, width:u32| nvvm_warp_shfl_sync_down_i32(membermask, x, delta as i32, shuffle_mask_bound_upper(width)),
		shfl_down_u32 = @|x:u32, delta:u32, width:u32| nvvm_warp_shfl_sync_down_i32(membermask, x as i32, delta as i32, shuffle_mask_bound_upper(width)) as u32,
		// shfl_down_i64 = @|x:i64, delta:u32, width:u32| nvvm_warp_shfl_down_i64(membermask, x, delta, shuffle_mask_bound_upper(width)),
		// shfl_down_u64 = @|x:u64, delta:u32, width:u32| nvvm_warp_shfl_down_u64(membermask, x, delta, shuffle_mask_bound_upper(width)),
		// shfl_down_f32 = @|x:f32, delta:u32, width:u32| nvvm_warp_shfl_sync_down_f32(membermask, x, delta as i32, shuffle_mask_bound_upper(width)),
		// shfl_down_f64 = @|x:f64, delta:u32, width:u32| nvvm_warp_shfl_down_f64(membermask, x, delta, shuffle_mask_bound_upper(width)),

		shfl_bfly_i32 = @|x:i32, lane_mask:i32, width:u32| nvvm_warp_shfl_sync_bfly_i32(membermask, x, lane_mask, shuffle_mask_bound_upper(width)),
		shfl_bfly_u32 = @|x:u32, lane_mask:i32, width:u32| nvvm_warp_shfl_sync_bfly_i32(membermask, x as i32, lane_mask, shuffle_mask_bound_upper(width)) as u32,
		// shfl_bfly_i64 = @|x:i64, lane_mask:i32, width:u32| nvvm_warp_shfl_bfly_i64(membermask, x, lane_mask, shuffle_mask_bound_upper(width)),
		// shfl_bfly_u64 = @|x:u64, lane_mask:i32, width:u32| nvvm_warp_shfl_bfly_u64(membermask, x, lane_mask, shuffle_mask_bound_upper(width)),
		// shfl_bfly_f32 = @|x:f32, lane_mask:i32, width:u32| nvvm_warp_shfl_sync_bfly_f32(membermask, x, lane_mask, shuffle_mask_bound_upper(width)),
		// shfl_bfly_f64 = @|x:f64, lane_mask:i32, width:u32| nvvm_warp_shfl_bfly_f64(membermask, x, lane_mask, shuffle_mask_bound_upper(width)),

		// match_any_i32 = @|x:i32| nvvm_warp_match_any_i32(membermask, x),
		// match_any_u32 = @|x:u32| nvvm_warp_match_any_u32(membermask, x),
		// match_any_i64 = @|x:i64| nvvm_warp_match_any_i64(membermask, x),
		// match_any_u64 = @|x:u64| nvvm_warp_match_any_u64(membermask, x),
		// match_any_f32 = @|x:f32| nvvm_warp_match_any_f32(membermask, x),
		// match_any_f64 = @|x:f64| nvvm_warp_match_any_f64(membermask, x),

		// match_all_i32 = @|x:i32, predicate:&mut i32| nvvm_warp_match_all_i32(membermask, x, predicate),
		// match_all_u32 = @|x:u32, predicate:&mut i32| nvvm_warp_match_all_u32(membermask, x, predicate),
		// match_all_i64 = @|x:i64, predicate:&mut i32| nvvm_warp_match_all_i64(membermask, x, predicate),
		// match_all_u64 = @|x:u64, predicate:&mut i32| nvvm_warp_match_all_u64(membermask, x, predicate),
		// match_all_f32 = @|x:f32, predicate:&mut i32| nvvm_warp_match_all_f32(membermask, x, predicate),
		// match_all_f64 = @|x:f64, predicate:&mut i32| nvvm_warp_match_all_f64(membermask, x, predicate),

		lanemask = @|| nvvm_lanemask() as u64,
		lanemask_le = @|| nvvm_lanemask_le() as u64,
		lanemask_lt = @|| nvvm_lanemask_lt() as u64,
		lanemask_ge = @|| nvvm_lanemask_ge() as u64,
		lanemask_gt = @|| nvvm_lanemask_gt() as u64
	});
}

fn @nvvm_block(idx: fn(i32) -> u32, gid: fn() -> u32, thread_idx: fn(i32) -> u32, block_size: fn(i32) -> u32, warp_size: u32, body: fn(gpu_group_context) -> ()) -> () {
	let linear_thread_idx = @|| {
		(thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0)
	};

	let warp_idx = @|| {
		linear_thread_idx() / warp_size
	};

	let num_threads = @|| block_size(0) * block_size(1) * block_size(2);
	let num_warps = @|| (num_threads() + warp_size - 1) / warp_size;

	@body(gpu_group_context {
		idx = idx,
		waves = @|body|@|| nvvm_subwarp(warp_idx, @|| gid() * num_warps() + warp_idx(), get_member_mask_u32(warp_size), warp_size, body),
		threads = @|body|@|| nvvm_thread(thread_idx, @|| gid() * num_threads() + linear_thread_idx(), body),
		num_waves = num_warps,
		num_threads = block_size,
		barrier = nvvm_block_sync,
		barrier_all = @|predicate| nvvm_block_sync_all(nvvm_pred(predicate)) != 0,
		barrier_any = @|predicate| nvvm_block_sync_any(nvvm_pred(predicate)) != 0,
		barrier_count = @|predicate| nvvm_block_sync_count(nvvm_pred(predicate))
	});
}

fn @nvvm_launch(device: i32, (grid_dim_x: i32, grid_dim_y: i32, grid_dim_z: i32), (block_dim_x: i32, block_dim_y: i32, block_dim_z: i32), wrap_index: index_wrapper, wrap_dim: dim_wrapper, body: fn(gpu_grid_context) -> ()) -> () {
	// TODO: assert(warp_size == 32)
	let warp_size: u32 = 32;

	let block_size = wrap_dim(@|i: i32| {
		match i {
			0 => if ?block_dim_x { block_dim_x as u32 } else { nvvm_read_ptx_sreg_ntid_x() as u32 },
			1 => if ?block_dim_y { block_dim_y as u32 } else { nvvm_read_ptx_sreg_ntid_y() as u32 },
			2 => if ?block_dim_z { block_dim_z as u32 } else { nvvm_read_ptx_sreg_ntid_z() as u32 },
			_ => 1
		}
	});

	let num_blocks = wrap_dim(@|i: i32| {
		match i {
			0 => if ?grid_dim_x { grid_dim_x as u32 } else { nvvm_read_ptx_sreg_nctaid_x() as u32 },
			1 => if ?grid_dim_y { grid_dim_y as u32 } else { nvvm_read_ptx_sreg_nctaid_y() as u32 },
			2 => if ?grid_dim_y { grid_dim_z as u32 } else { nvvm_read_ptx_sreg_nctaid_z() as u32 },
			_ => 1
		}
	});

	let num_threads_per_block = @|| block_size(0) * block_size(1) * block_size(2);

	let num_warps_per_block = @|| (num_threads_per_block() + warp_size - 1) / warp_size;

	let num_warps = @|| (num_blocks(0) * num_blocks(1) * num_blocks(2)) * num_warps_per_block();

	let num_threads = @|i: i32| num_blocks(i) * block_size(i);

	let block_idx = wrap_index(@|i: i32| {
		match i { 0 => nvvm_read_ptx_sreg_ctaid_x() as u32, 1 => nvvm_read_ptx_sreg_ctaid_y() as u32, 2 => nvvm_read_ptx_sreg_ctaid_z() as u32, _ => 0 }
	});

	let linear_block_idx = @|| (block_idx(2) * num_blocks(1) + block_idx(1)) * num_blocks(0) + block_idx(0);

	let thread_idx = wrap_index(@|i: i32| {
		match i { 0 => nvvm_read_ptx_sreg_tid_x() as u32, 1 => nvvm_read_ptx_sreg_tid_y() as u32, 2 => nvvm_read_ptx_sreg_tid_z() as u32, _ => 0 }
	});

	let global_thread_idx = @|i: i32| block_idx(i) * block_size(i) + thread_idx(i);

	let linear_thread_idx = @|| (thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0);

	let global_linear_thread_idx = @|| linear_block_idx() * num_threads_per_block() + linear_thread_idx();

	let global_warp_idx = @|| linear_block_idx() * num_warps_per_block() + linear_thread_idx() / warp_size;

	nvvm(device, (grid_dim_x * block_dim_x, grid_dim_y * block_dim_y, grid_dim_z * block_dim_z), (block_dim_x, block_dim_y, block_dim_z), @|| @body(gpu_grid_context {
		device = device,
		groups = @|body|@|| nvvm_block(block_idx, linear_block_idx, thread_idx, block_size, warp_size, body),
		waves = @|body|@|| nvvm_subwarp(global_warp_idx, global_warp_idx, get_member_mask_u32(warp_size), warp_size, body),
		threads = @|body|@|| nvvm_thread(global_thread_idx, global_linear_thread_idx, body),
		num_groups = num_blocks,
		num_waves = num_warps,
		num_threads = num_threads
	}));
}

fn @nvvm_launch_1d(device: i32, grid_dim: i32, block_dim: i32, body: fn(gpu_grid_context) -> ()) = nvvm_launch(device, (grid_dim, 1, 1), (block_dim, 1, 1), wrap_index_1d, wrap_dim_1d, body);
fn @nvvm_launch_2d(device: i32, (grid_dim_x: i32, grid_dim_y: i32), (block_dim_x: i32, block_dim_y: i32), body: fn(gpu_grid_context) -> ()) = nvvm_launch(device, (grid_dim_x, grid_dim_y, 1), (block_dim_x, block_dim_y, 1), wrap_index_2d, wrap_dim_2d, body);
fn @nvvm_launch_3d(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), body: fn(gpu_grid_context) -> ()) = nvvm_launch(device, grid_dim, block_dim, wrap_index_3d, wrap_dim_3d, body);
