#[import(cc = "device", name = "llvm.trap")] fn trap() -> ();

fn @createAccDevice(device: i32) = AccDevice {
    its = false,
    launch_1d = @|body| @|num_groups, group_size| amdgpu_launch_1d(device, num_groups, group_size, body),
    synchronize = || synchronize_hsa(device),
    alloc = |size| alloc_hsa(device, size),
    platform_device = runtime_device(3, device),
    platform_name = "amdgpu",
    print_i32 = @|_format: &[u8], _arg: i32| { /*trap()*/ },
    print_2xi32 = @|_format: &[u8], _arg1: i32, _arg2: i32| { /*trap()*/ },
    print_3xi32 = @|_format: &[u8], _arg1: i32, _arg2: i32, _arg3: i32| { /*trap()*/ }
};

fn @createDefaultAccDevice() = createAccDevice(1);


fn @amdgpu_pred(b: bool) -> i32 {
    if b { 1 } else { 0 }
}

fn @amdgpu_atomic_load[T](location: &addrspace(1) T, order: memory_order) = atomic_load_p1[T](location, builtin_memory_order(order), "agent");

fn @amdgpu_atomic_store[T](location: &mut addrspace(1) T, value: T, order: memory_order) = atomic_store_p1[T](location, value, builtin_memory_order(order), "agent");

fn @amdgpu_atomic_rmw[T](op: u32) {
    @|location:&mut addrspace(1) T, value:T, order:memory_order| -> T {
        atomic_p1[T](op, location, value, builtin_memory_order(order), "agent")
    }
}

fn @amdgpu_atomic_cas[T](location: &mut addrspace(1) T, expected: T, desired: T, memory_order_succ: memory_order, memory_order_fail: memory_order)
    = cmpxchg_p1[T](location, expected, desired, builtin_memory_order(memory_order_succ), builtin_memory_order(memory_order_fail), "agent");

fn @amdgpu_atomic_cas_weak[T](location: &mut addrspace(1) T, expected: T, desired: T, memory_order_succ: memory_order, memory_order_fail: memory_order)
    = cmpxchg_weak_p1[T](location, expected, desired, builtin_memory_order(memory_order_succ), builtin_memory_order(memory_order_fail), "agent");

fn @amdgpu_atomic_wait_and_transition[T](location:&mut addrspace(1) T, expected:T, desired:T, order:memory_order, _debug_msg:&[u8]) -> () {
    while !amdgpu_atomic_cas_weak[T](location, expected, desired, order, memory_order::relaxed).1 {
        amdgcn_s_sleep(0);
    }
}

fn @amdgpu_thread(idx: fn(i32)->u32, gid: fn() -> u32, body: fn(thread_context)->()) -> () {
    body(thread_context {
        idx = idx,

        gid = gid,

        atomic_load_global_i32 = amdgpu_atomic_load[i32],
        atomic_load_global_u32 = amdgpu_atomic_load[u32],
        atomic_load_global_i64 = amdgpu_atomic_load[i64],
        atomic_load_global_u64 = amdgpu_atomic_load[u64],

        atomic_load_global_i32_coalesced = amdgpu_atomic_load[i32],
        atomic_load_global_u32_coalesced = amdgpu_atomic_load[u32],
        atomic_load_global_i64_coalesced = amdgpu_atomic_load[i64],
        atomic_load_global_u64_coalesced = amdgpu_atomic_load[u64],

        atomic_store_global_i32 = amdgpu_atomic_store[i32],
        atomic_store_global_u32 = amdgpu_atomic_store[u32],
        atomic_store_global_i64 = amdgpu_atomic_store[i64],
        atomic_store_global_u64 = amdgpu_atomic_store[u64],

        atomic_store_global_i32_coalesced = amdgpu_atomic_store[i32],
        atomic_store_global_u32_coalesced = amdgpu_atomic_store[u32],
        atomic_store_global_i64_coalesced = amdgpu_atomic_store[i64],
        atomic_store_global_u64_coalesced = amdgpu_atomic_store[u64],

        atomic_add_global_i32 = amdgpu_atomic_rmw[i32](1),
        atomic_add_global_u32 = amdgpu_atomic_rmw[u32](1),
        atomic_add_global_i64 = amdgpu_atomic_rmw[i64](1),
        atomic_add_global_u64 = amdgpu_atomic_rmw[u64](1),

        atomic_sub_global_i32 = amdgpu_atomic_rmw[i32](2),
        atomic_sub_global_u32 = amdgpu_atomic_rmw[u32](2),
        atomic_sub_global_u64 = amdgpu_atomic_rmw[u64](2),

        atomic_and_global_i32 = amdgpu_atomic_rmw[i32](3),
        atomic_and_global_u32 = amdgpu_atomic_rmw[u32](3),
        atomic_and_global_u64 = amdgpu_atomic_rmw[u64](3),

        atomic_or_global_i32 = amdgpu_atomic_rmw[i32](5),
        atomic_or_global_u32 = amdgpu_atomic_rmw[u32](5),
        atomic_or_global_u64 = amdgpu_atomic_rmw[u64](5),

        atomic_xor_global_i32 = amdgpu_atomic_rmw[i32](6),
        atomic_xor_global_u32 = amdgpu_atomic_rmw[u32](6),
        atomic_xor_global_u64 = amdgpu_atomic_rmw[u64](6),

        atomic_exch_global_i32 = amdgpu_atomic_rmw[i32](0),
        atomic_exch_global_u32 = amdgpu_atomic_rmw[u32](0),
        atomic_exch_global_u64 = amdgpu_atomic_rmw[u64](0),

        atomic_min_global_i32 = amdgpu_atomic_rmw[i32]( 8),
        atomic_min_global_u32 = amdgpu_atomic_rmw[u32](10),
        atomic_min_global_u64 = amdgpu_atomic_rmw[u64](10),

        atomic_max_global_i32 = amdgpu_atomic_rmw[i32](7),
        atomic_max_global_u32 = amdgpu_atomic_rmw[u32](9),
        atomic_max_global_u64 = amdgpu_atomic_rmw[u64](9),

        atomic_cas_global_i32 = amdgpu_atomic_cas[i32],
        atomic_cas_global_u32 = amdgpu_atomic_cas[u32],
        atomic_cas_global_i64 = amdgpu_atomic_cas[i64],
        atomic_cas_global_u64 = amdgpu_atomic_cas[u64],
        atomic_cas_global_i32_weak = amdgpu_atomic_cas_weak[i32],
        atomic_cas_global_u32_weak = amdgpu_atomic_cas_weak[u32],
        atomic_cas_global_i64_weak = amdgpu_atomic_cas_weak[i64],
        atomic_cas_global_u64_weak = amdgpu_atomic_cas_weak[u64],

        atomic_inc_global_u32 = @|addr, val| amdgcn_atomic_inc_global_u32(addr, val, 0, 0, false),

        memory_barrier = @|order| fence(builtin_memory_order(order), "agent"),

        timestamp = @|| amdgcn_s_memrealtime() as i64,
        timestamp32 = @|| amdgcn_s_memrealtime() as i32,

        wait = @|f, _debug_msg| {
            let mut t = 2;
            while !f() {
                amdgcn_s_sleep(t);
                if t < 200 { t = t * 3 / 2; }
            }
        }
    });
}

fn @amdgpu_subwarp(idx: fn()->u32, gid: fn() -> u32, membermask: u64, num_threads: u32, body: fn(wave_context)->()) -> () {
    let thread_idx = @|i: i32| match i { 0 => amdgcn_lane_id() as u32, _ => 0 };

    body(wave_context{
        idx = idx,

        membermask = @|| membermask,

        threads = @|body| @|| amdgpu_thread(thread_idx, @|| gid() * num_threads + thread_idx(0), body),

        num_threads = @|| num_threads,

        barrier =       amdgcn_wave_barrier,
        barrier_all =   @|predicate| amdgcn_sync_all(amdgpu_pred(predicate)) != 0,
        barrier_any =   @|predicate| amdgcn_sync_any(amdgpu_pred(predicate)) != 0,
        barrier_count = @|predicate| amdgcn_sync_count(amdgpu_pred(predicate)),
        barrier_vote =  @|predicate| amdgcn_sync_vote(amdgpu_pred(predicate)),

        shfl_i32 = @|var:i32, src_lane:i32, width:u32| amdgcn_shfl_i32(var, src_lane, width as i32),
        shfl_u32 = @|var:u32, src_lane:i32, width:u32| amdgcn_shfl_u32(var, src_lane, width as i32),
        // shfl_f32 = @|var:f32, src_lane:i32, width:u32| amdgcn_shfl_f32(var, src_lane, width as i32),

        shfl_up_i32 = @|var:i32, delta:u32, width:u32| amdgcn_shfl_up_i32(var, delta, width as i32),
        shfl_up_u32 = @|var:u32, delta:u32, width:u32| amdgcn_shfl_up_u32(var, delta, width as i32),
        // shfl_up_f32 = @|var:f32, delta:u32, width:u32| amdgcn_shfl_up_f32(var, delta, width as i32),

        shfl_down_i32 = @|var:i32, delta:u32, width:u32| amdgcn_shfl_down_i32(var, delta, width as i32),
        shfl_down_u32 = @|var:u32, delta:u32, width:u32| amdgcn_shfl_down_u32(var, delta, width as i32),
        // shfl_down_f32 = @|var:f32, delta:u32, width:u32| amdgcn_shfl_down_f32(var, delta, width as i32),

        shfl_bfly_i32 = @|var:i32, lane_mask:i32, width:u32| amdgcn_shfl_xor_i32(var, lane_mask, width as i32),
        shfl_bfly_u32 = @|var:u32, lane_mask:i32, width:u32| amdgcn_shfl_xor_u32(var, lane_mask, width as i32),
        // shfl_bfly_f32 = @|var:f32, lane_mask:i32, width:u32| amdgcn_shfl_xor_f32(var, lane_mask, width as i32),

        lanemask = amdgcn_lanemask_eq,
        lanemask_le = amdgcn_lanemask_le,
        lanemask_lt = amdgcn_lanemask_lt,
        lanemask_ge = amdgcn_lanemask_ge,
        lanemask_gt = amdgcn_lanemask_gt
    });
}

fn @amdgpu_block(idx: fn(i32)->u32, gid: fn() -> u32, thread_idx: fn(i32)->u32, block_size: fn(i32)->u32, warp_size: u32, body: fn(group_context)->()) -> () {
    let linear_thread_idx = @|| {
        (thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0)
    };

    let warp_idx = @|| {
        linear_thread_idx() / warp_size
    };

    let num_threads = @|| block_size(0) * block_size(1) * block_size(2);
    let num_warps = @|| (num_threads() + warp_size - 1) / warp_size;

    fn @amdgcn_barrier_all(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_all(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res &= lds(i);
        }
        res
    }

    fn @amdgcn_barrier_any(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_any(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res |= lds(i);
        }
        res
    }

    fn @amdgcn_barrier_count(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_count(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res += lds(i);
        }
        res
    }

    body(group_context {
        idx = idx,
        waves = @|body| @|| amdgpu_subwarp(warp_idx, @|| gid() * num_warps() + warp_idx(), get_member_mask_u64(warp_size), warp_size, body),
        threads = @|body| @|| amdgpu_thread(thread_idx, @|| gid() * num_threads() + linear_thread_idx(), body),
        num_waves = num_warps,
        num_threads = block_size,
        barrier = amdgcn_s_barrier,
        barrier_all = @|predicate| amdgcn_barrier_all(amdgpu_pred(predicate)) != 0,
        barrier_any = @|predicate| amdgcn_barrier_any(amdgpu_pred(predicate)) != 0,
        barrier_count = @|predicate| amdgcn_barrier_count(amdgpu_pred(predicate))
    })
}

fn @amdgpu_launch(device: i32, (grid_dim_x: i32, grid_dim_y: i32, grid_dim_z: i32), (block_dim_x: i32, block_dim_y: i32, block_dim_z: i32), wrap_index: index_wrapper, wrap_dim: dim_wrapper, body: fn(grid_context)->()) -> () {
    let wavefront_size: u32 = 32;

    let block_size = wrap_dim(@|i| {
        match i {
            0 => if ?block_dim_x { block_dim_x as u32 } else { (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(2) as u32 },
            1 => if ?block_dim_y { block_dim_y as u32 } else { (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(3) as u32 },
            2 => if ?block_dim_z { block_dim_z as u32 } else { (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(4) as u32 },
            _ => 1
        }
    });

    let num_blocks = wrap_dim(@|i| {
        match i {
            0 => if ?grid_dim_x { grid_dim_x as u32 } else { div_up((amdgcn_dispatch_ptr() as &addrspace(4)[u32])(3) as i32, (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(2) as i32) as u32 },
            1 => if ?grid_dim_y { grid_dim_y as u32 } else { div_up((amdgcn_dispatch_ptr() as &addrspace(4)[u32])(4) as i32, (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(3) as i32) as u32 },
            2 => if ?grid_dim_y { grid_dim_z as u32 } else { div_up((amdgcn_dispatch_ptr() as &addrspace(4)[u32])(5) as i32, (amdgcn_dispatch_ptr() as &addrspace(4)[u16])(4) as i32) as u32 },
            _ => 1
        }
    });

    let num_threads_per_block = @|| block_size(0) * block_size(1) * block_size(2);

    let num_warps_per_block = @|| (num_threads_per_block() + wavefront_size - 1) / wavefront_size;

    let num_warps = @|| (num_blocks(0) * num_blocks(1) * num_blocks(2)) * num_warps_per_block();

    let num_threads = @|i: i32| num_blocks(i) * block_size(i);

    let block_idx = wrap_index(@|i| {
        match i { 0 => amdgcn_workgroup_id_x() as u32, 1 => amdgcn_workgroup_id_y() as u32, 2 => amdgcn_workgroup_id_z() as u32, _ => 0 }
    });

    let linear_block_idx = @|| (block_idx(2) * num_blocks(1) + block_idx(1)) * num_blocks(0) + block_idx(0);
    let thread_idx = wrap_index(@|i| match i { 0 => amdgcn_workitem_id_x() as u32, 1 => amdgcn_workitem_id_y() as u32, 2 => amdgcn_workitem_id_z() as u32, _ => 0 });
    let global_thread_idx = @|i: i32| block_idx(i) * block_size(i) + thread_idx(i);
    let linear_thread_idx = @|| (thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0);
    let global_linear_thread_idx = @|| linear_block_idx() * num_threads_per_block() + linear_thread_idx();
    let global_warp_idx = @|| linear_block_idx() * num_warps_per_block() + linear_thread_idx() / wavefront_size;

    amdgpu(device, (grid_dim_x * block_dim_x, grid_dim_y * block_dim_y, grid_dim_z * block_dim_z), (block_dim_x, block_dim_y, block_dim_z), @|| body(grid_context {
        device  = device,
        groups  = @|body| @|| amdgpu_block(block_idx, linear_block_idx, thread_idx, block_size, wavefront_size, body),
        waves   = @|body| @|| amdgpu_subwarp(global_warp_idx, global_warp_idx, get_member_mask_u64(wavefront_size), wavefront_size, body),
        threads = @|body| @|| amdgpu_thread(global_thread_idx, global_linear_thread_idx, body),
        num_groups  = num_blocks,
        num_waves   = num_warps,
        num_threads = num_threads
    }));
}

fn @amdgpu_launch_1d(device: i32, grid_dim: i32, block_dim: i32, body: fn(grid_context)->()) = amdgpu_launch(device, (grid_dim, 1, 1), (block_dim, 1, 1), wrap_index_1d, wrap_dim_1d, body);
fn @amdgpu_launch_2d(device: i32, (grid_dim_x: i32, grid_dim_y: i32), (block_dim_x: i32, block_dim_y: i32), body: fn(grid_context)->()) = amdgpu_launch(device, (grid_dim_x, grid_dim_y, 1), (block_dim_x, block_dim_y, 1), wrap_index_2d, wrap_dim_2d, body);
fn @amdgpu_launch_3d(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), body: fn(grid_context)->()) = amdgpu_launch(device, grid_dim, block_dim, wrap_index_3d, wrap_dim_3d, body);
