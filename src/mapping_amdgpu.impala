static amdgpu_device = 1;

extern "device" {
    fn "llvm.trap" trap() -> ();
}

fn @createAccDevice() -> AccDevice {
    AccDevice {
        launch_1d: @|num_groups, group_size, body| { amdgpu_launch_1d(amdgpu_device, num_groups, group_size, body) },
        synchronize: || synchronize_hsa(amdgpu_device),
        alloc: |size| alloc_hsa(amdgpu_device, size),
        platform_device: runtime_device(3, amdgpu_device),
        print_i32: @|format: &[u8], arg: i32| { trap() }
        print_i32a: @|format: &[u8], args: &[i32]| { trap() }
    }
}


fn @amdgpu_thread(idx: fn(i32)->u32, body: fn(gpu_thread_context)->()) -> () {
    body(gpu_thread_context {
        idx: idx,

        atomic_load_global_i32: amdgcn_atomic_load_global_i32,
        atomic_load_global_u32: amdgcn_atomic_load_global_u32,
        atomic_load_global_u64: amdgcn_atomic_load_global_u64,

        atomic_store_global_i32: amdgcn_atomic_store_global_i32,
        atomic_store_global_u32: amdgcn_atomic_store_global_u32,
        atomic_store_global_u64: amdgcn_atomic_store_global_u64,

        atomic_add_global_i32: amdgcn_atomic_add_global_i32,
        atomic_add_global_u32: amdgcn_atomic_add_global_u32,
        atomic_add_global_u64: amdgcn_atomic_add_global_u64,

        atomic_sub_global_i32: amdgcn_atomic_sub_global_i32,
        atomic_sub_global_u32: amdgcn_atomic_sub_global_u32,
        atomic_sub_global_u64: amdgcn_atomic_sub_global_u64,

        atomic_and_global_i32: amdgcn_atomic_and_global_i32,
        atomic_and_global_u32: amdgcn_atomic_and_global_u32,
        atomic_and_global_u64: amdgcn_atomic_and_global_u64,

        atomic_or_global_i32: amdgcn_atomic_or_global_i32,
        atomic_or_global_u32: amdgcn_atomic_or_global_u32,
        atomic_or_global_u64: amdgcn_atomic_or_global_u64,

        atomic_xor_global_i32: amdgcn_atomic_xor_global_i32,
        atomic_xor_global_u32: amdgcn_atomic_xor_global_u32,
        atomic_xor_global_u64: amdgcn_atomic_xor_global_u64,

        atomic_exch_global_i32: amdgcn_atomic_exch_global_i32,
        atomic_exch_global_u32: amdgcn_atomic_exch_global_u32,
        atomic_exch_global_u64: amdgcn_atomic_exch_global_u64,

        atomic_min_global_i32: amdgcn_atomic_min_global_i32,
        atomic_min_global_u32: amdgcn_atomic_min_global_u32,
        atomic_min_global_u64: amdgcn_atomic_min_global_u64,

        atomic_max_global_i32: amdgcn_atomic_max_global_i32,
        atomic_max_global_u32: amdgcn_atomic_max_global_u32,
        atomic_max_global_u64: amdgcn_atomic_max_global_u64,

        // atomic_cas_global_u16: amdgcn_atomic_cas_global_u16,
        atomic_cas_global_i32: amdgcn_atomic_cas_global_i32,
        atomic_cas_global_u32: amdgcn_atomic_cas_global_u32,
        atomic_cas_global_i64: amdgcn_atomic_cas_global_u64,

        atomic_inc_global_u32: @|addr, val| amdgcn_atomic_inc_global_u32(addr, val, 0u32, 0u32, false),
        atomic_dec_global_u32: @|addr, val| amdgcn_atomic_dec_global_u32(addr, val, 0u32, 0u32, false),

        yield: @|| amdgcn_s_sleep(0)
    });
}

fn @amdgpu_subwarp(idx: fn()->u32, membermask: u64, num_threads: u32, body: fn(gpu_wave_context)->()) -> () {
    body(gpu_wave_context{
        idx: idx,

        membermask: @|| membermask,

        threads: @|body| amdgpu_thread(@|i| match i { 0 => amdgcn_lane_id() as u32, _ => 0u32 }, body),

        num_threads: @|| num_threads,

        barrier:       amdgcn_wave_barrier,
        barrier_all:   amdgcn_sync_all,
        barrier_any:   amdgcn_sync_any,
        barrier_count: amdgcn_sync_count,
        barrier_vote:  amdgcn_sync_vote,

        shfl_i32: @|var:i32, src_lane:i32, width:u32| amdgcn_shfl_i32(var, src_lane, width as i32),
        shfl_u32: @|var:u32, src_lane:i32, width:u32| amdgcn_shfl_u32(var, src_lane, width as i32),
        // shfl_f32: @|var:f32, src_lane:i32, width:u32| amdgcn_shfl_f32(var, src_lane, width as i32),

        shfl_up_i32: @|var:i32, delta:u32, width:u32| amdgcn_shfl_up_i32(var, delta, width as i32),
        shfl_up_u32: @|var:u32, delta:u32, width:u32| amdgcn_shfl_up_u32(var, delta, width as i32),
        // shfl_up_f32: @|var:f32, delta:u32, width:u32| amdgcn_shfl_up_f32(var, delta, width as i32),

        shfl_down_i32: @|var:i32, delta:u32, width:u32| amdgcn_shfl_down_i32(var, delta, width as i32),
        shfl_down_u32: @|var:u32, delta:u32, width:u32| amdgcn_shfl_down_u32(var, delta, width as i32),
        // shfl_down_f32: @|var:f32, delta:u32, width:u32| amdgcn_shfl_down_f32(var, delta, width as i32),

        shfl_bfly_i32: @|var:i32, lane_mask:i32, width:u32| amdgcn_shfl_xor_i32(var, lane_mask, width as i32),
        shfl_bfly_u32: @|var:u32, lane_mask:i32, width:u32| amdgcn_shfl_xor_u32(var, lane_mask, width as i32),
        // shfl_bfly_f32: @|var:f32, lane_mask:i32, width:u32| amdgcn_shfl_xor_f32(var, lane_mask, width as i32),

        lanemask: @|| cuda_lanemask() as u64,
        lanemask_le: @|| cuda_lanemask_le() as u64,
        lanemask_lt: @|| cuda_lanemask_lt() as u64,
        lanemask_ge: @|| cuda_lanemask_ge() as u64,
        lanemask_gt: @|| cuda_lanemask_gt() as u64
    });
}

fn @amdgpu_block(idx: fn(i32)->u32, thread_idx: fn(i32)->u32, num_threads: fn(i32)->u32, warp_size: u32, body: fn(gpu_group_context)->()) -> () {
    let linear_thread_idx = @|| {
        (thread_idx(2) * num_threads(1) + thread_idx(1)) * num_threads(0) + thread_idx(0)
    };

    let warp_idx = @|| {
        linear_thread_idx() / warp_size
    };

    let num_warps = @|| ((num_threads(0) * num_threads(1) * num_threads(2)) + warp_size - 1u32) / warp_size;

    fn @amdgcn_barrier_all(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_all(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res &= lds(i);
        }
        res
    }

    fn @amdgcn_barrier_any(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_any(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res |= lds(i);
        }
        res
    }

    fn @amdgcn_barrier_count(p: i32) -> i32 {
        let lds = reserve_shared[i32](num_warps() as i32);
        let warp_res = amdgcn_sync_count(p);
        amdgcn_s_barrier();
        if amdgcn_activelane() == 0 {
            lds(warp_idx()) = warp_res;
        }
        amdgcn_s_barrier();
        let mut res = lds(0);
        for i in range(1, num_warps() as i32) {
            res += lds(i);
        }
        res
    }

    body(gpu_group_context {
        idx: idx,
        waves: @|body| amdgpu_subwarp(warp_idx, -1 as u32 as u64, warp_size, body),
        threads: @|body| amdgpu_thread(thread_idx, body),
        num_waves: num_warps,
        num_threads: num_threads,
        barrier: amdgcn_s_barrier,
        barrier_all: amdgcn_barrier_all,
        barrier_any: amdgcn_barrier_any,
        barrier_count: amdgcn_barrier_count
    })
}

fn @amdgpu_launch(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), wrap_index: index_wrapper, wrap_dim: dim_wrapper, body: fn(gpu_grid_context, Intrinsics)->()) -> () {
    let wavefront_size = 32u32;
    fn @div_round_up(num: i32, multiple: i32) -> i32 { (num + multiple - 1) / multiple }

    let num_threads_per_block = wrap_dim(@|i| {
        match i {
            0 => if ?block_dim(0) { block_dim(0) as u32 } else { bitcast[&[4][u16]](amdgcn_dispatch_ptr())(2) as u32 },
            1 => if ?block_dim(1) { block_dim(1) as u32 } else { bitcast[&[4][u16]](amdgcn_dispatch_ptr())(3) as u32 },
            2 => if ?block_dim(2) { block_dim(2) as u32 } else { bitcast[&[4][u16]](amdgcn_dispatch_ptr())(4) as u32 },
            _ => 1u32
        }
    });

    let num_warps_per_block = @|| ((num_threads_per_block(0) * num_threads_per_block(1) * num_threads_per_block(2)) + wavefront_size - 1u32) / wavefront_size;

    let num_blocks = wrap_dim(@|i| {
        match i {
            0 => if ?grid_dim(0) { grid_dim(0) as u32 } else { div_round_up(bitcast[&[4][u32]](amdgcn_dispatch_ptr())(3) as i32, bitcast[&[4][u16]](amdgcn_dispatch_ptr())(2) as i32) as u32 },
            1 => if ?grid_dim(1) { grid_dim(1) as u32 } else { div_round_up(bitcast[&[4][u32]](amdgcn_dispatch_ptr())(4) as i32, bitcast[&[4][u16]](amdgcn_dispatch_ptr())(3) as i32) as u32 },
            2 => if ?grid_dim(1) { grid_dim(2) as u32 } else { div_round_up(bitcast[&[4][u32]](amdgcn_dispatch_ptr())(5) as i32, bitcast[&[4][u16]](amdgcn_dispatch_ptr())(4) as i32) as u32 },
            _ => 1u32
        }
    });

    let num_warps = @|| (num_blocks(0) * num_blocks(1) * num_blocks(2)) * num_warps_per_block();

    let num_threads = @|i| num_blocks(i) * num_threads_per_block(i);

    let block_idx = wrap_index(@|i| {
        match i { 0 => amdgcn_workgroup_id_x() as u32, 1 => amdgcn_workgroup_id_y() as u32, 2 => amdgcn_workgroup_id_z() as u32, _ => 0u32 }
    });

    let linear_block_idx = @|| (block_idx(2) * num_blocks(1) + block_idx(1)) * num_blocks(0) + block_idx(0);

    let thread_idx = wrap_index(@|i| {
        match i { 0 => amdgcn_workitem_id_x() as u32, 1 => amdgcn_workitem_id_y() as u32, 2 => amdgcn_workitem_id_z() as u32, _ => 0u32 }
    });

    let global_thread_idx = @|i| block_idx(i) * num_threads_per_block(i) + thread_idx(i);

    let linear_thread_idx = @|| (thread_idx(2) * num_threads_per_block(1) + thread_idx(1)) * num_threads_per_block(0) + thread_idx(0);

    let global_warp_idx = @|| linear_block_idx() * num_warps_per_block() + linear_thread_idx() / wavefront_size;

    amdgpu(device, (grid_dim(0) * block_dim(0), grid_dim(1) * block_dim(1), grid_dim(2) * block_dim(2)), (block_dim(0), block_dim(1), block_dim(2)), @|| body(gpu_grid_context {
        device:  device,
        groups:  @|body| amdgpu_block(block_idx, thread_idx, num_threads_per_block, wavefront_size, body),
        waves:   @|body| amdgpu_subwarp(global_warp_idx, -1 as u32 as u64, wavefront_size, body),
        threads: @|body| amdgpu_thread(global_thread_idx, body),
        num_groups:  num_blocks,
        num_waves:   num_warps,
        num_threads: num_threads
    }, amdgpu_intrinsics));
}

fn @amdgpu_launch_1d(device: i32, grid_dim: i32, block_dim: i32, body: fn(gpu_grid_context, Intrinsics)->()) -> () {
    amdgpu_launch(device, (grid_dim, 1, 1), (block_dim, 1, 1), wrap_index_1d, wrap_dim_1d, body);
    synchronize_hsa(amdgpu_device);
}

fn @amdgpu_launch_2d(device: i32, grid_dim: (i32, i32), block_dim: (i32, i32), body: fn(gpu_grid_context, Intrinsics)->()) -> () {
    amdgpu_launch(device, (grid_dim(0), grid_dim(1), 1), (block_dim(0), block_dim(1), 1), wrap_index_2d, wrap_dim_2d, body)
}

fn @amdgpu_launch_3d(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), body: fn(gpu_grid_context, Intrinsics)->()) -> () {
    amdgpu_launch(device, grid_dim, block_dim, wrap_index_3d, wrap_dim_3d, body)
}
