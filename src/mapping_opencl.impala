// require device with OpenCL 2.1 support (and cl_intel_subgroups extension)
static opencl_device = 1;

fn @createAccDevice() -> AccDevice {
	AccDevice {
		launch_1d: @|num_groups, group_size, body| {
			opencl_launch_1d(opencl_device, num_groups, group_size, body)
		},
		synchronize: || synchronize_opencl(opencl_device),
		alloc: |size| alloc_opencl(opencl_device, size),
		platform_device: runtime_device(2, opencl_device),
		print_i32: @|format: &[u8], arg: i32| { opencl_print_i32(format, arg); }
		print_i32a: @|format: &[u8], args: &[i32]| { opencl_print_i32("print_i32a not supported in opencl", args(0)); }
	}
}

fn AtomicFn_u64(&mut[1] u64, u64) -> u64 { undef[u64]() }
fn AtomicFn_f32(&mut[1] f32, f32) -> f32 { undef[f32]() }

extern "device" {
    fn "atomic_add"     opencl_atomic_add_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_add"     opencl_atomic_add_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_sub"     opencl_atomic_sub_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_sub"     opencl_atomic_sub_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_and"     opencl_atomic_and_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_and"     opencl_atomic_and_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_or"      opencl_atomic_or_global_i32      (&mut[1]i32, i32) -> i32;
    fn "atomic_or"      opencl_atomic_or_global_u32      (&mut[1]u32, u32) -> u32;
    fn "atomic_xor"     opencl_atomic_xor_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_xor"     opencl_atomic_xor_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_xchg"    opencl_atomic_xchg_global_i32    (&mut[1]i32, i32) -> i32;
    fn "atomic_xchg"    opencl_atomic_xchg_global_u32    (&mut[1]u32, u32) -> u32;
    fn "atomic_cmpxchg" opencl_atomic_cmpxchg_global_i32 (&mut[1]i32, i32, i32) -> i32;
    fn "atomic_cmpxchg" opencl_atomic_cmpxchg_global_u32 (&mut[1]u32, u32, u32) -> u32;
    fn "atomic_min"     opencl_atomic_min_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_min"     opencl_atomic_min_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_max"     opencl_atomic_max_global_i32     (&mut[1]i32, i32) -> i32;
    fn "atomic_max"     opencl_atomic_max_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_inc"     opencl_atomic_inc_global_u32     (&mut[1]u32, u32) -> u32;
    fn "atomic_dec"     opencl_atomic_dec_global_u32     (&mut[1]u32, u32) -> u32;

    fn "work_group_barrier"      opencl_work_group_barrier     (u32) -> ();
    fn "sub_group_barrier"       opencl_sub_group_barrier      (u32) -> ();
    fn "get_sub_group_size"      opencl_get_sub_group_size     () -> u32;
    fn "get_num_sub_groups"      opencl_get_num_sub_groups     () -> u32;
    fn "get_sub_group_id"        opencl_get_sub_group_id       () -> u32;
    fn "get_sub_group_local_id"  opencl_get_sub_group_local_id () -> u32;
    fn "get_max_sub_group_size " opencl_get_max_sub_group_size () -> u32;

    fn "sub_group_all"           opencl_sub_group_all (i32) -> i32;
    fn "sub_group_any"           opencl_sub_group_any (i32) -> i32;

    fn "intel_sub_group_shuffle"      opencl_intel_sub_group_shuffle_i32 (i32, u32) -> i32;
    fn "intel_sub_group_shuffle"      opencl_intel_sub_group_shuffle_u32 (u32, u32) -> u32;
    fn "intel_sub_group_shuffle"      opencl_intel_sub_group_shuffle_f32 (f32, u32) -> f32;
    fn "intel_sub_group_shuffle_down" opencl_intel_sub_group_shuffle_down_i32 (i32, i32, u32) -> i32;
    fn "intel_sub_group_shuffle_down" opencl_intel_sub_group_shuffle_down_u32 (u32, u32, u32) -> u32;
    fn "intel_sub_group_shuffle_down" opencl_intel_sub_group_shuffle_down_f32 (f32, f32, u32) -> f32;
    fn "intel_sub_group_shuffle_up"   opencl_intel_sub_group_shuffle_up_i32   (i32, i32, u32) -> i32;
    fn "intel_sub_group_shuffle_up"   opencl_intel_sub_group_shuffle_up_u32   (u32, u32, u32) -> u32;
    fn "intel_sub_group_shuffle_up"   opencl_intel_sub_group_shuffle_up_f32   (f32, f32, u32) -> f32;
    fn "intel_sub_group_shuffle_xor"  opencl_intel_sub_group_shuffle_xor_i32  (i32, u32) -> i32;
    fn "intel_sub_group_shuffle_xor"  opencl_intel_sub_group_shuffle_xor_u32  (u32, u32) -> u32;
    fn "intel_sub_group_shuffle_xor"  opencl_intel_sub_group_shuffle_xor_f32  (f32, u32) -> f32;

    fn "printf"  opencl_print_i32 (&[u8], i32) -> i32;
}

fn @opencl_pred(b: bool) -> i32 {
	if b {1} else {0}
}

fn @opencl_thread(idx: fn (i32) -> u32, body: fn (gpu_thread_context) -> ()) -> () {
	@@body(gpu_thread_context {
		idx: idx,

		atomic_load_global_i32: @|location:&mut[1]i32| opencl_atomic_add_global_i32(location, 0i32),
		atomic_load_global_u32: @|location:&mut[1]u32| opencl_atomic_add_global_u32(location, 0u32),
		atomic_load_global_u64: @|location:&mut[1]u64| undef[u64](),

		atomic_store_global_i32: @|location:&mut[1]i32, value:i32| -> () { opencl_atomic_xchg_global_i32(location, value); },
		atomic_store_global_u32: @|location:&mut[1]u32, value:u32| -> () { opencl_atomic_xchg_global_u32(location, value); },
		atomic_store_global_u64: @|location:&mut[1]u64, value:u64| -> () { *location = value; },  // fixme

		atomic_add_global_i32: opencl_atomic_add_global_i32,
		atomic_add_global_u32: opencl_atomic_add_global_u32,
		atomic_add_global_u64: AtomicFn_u64,

		atomic_sub_global_i32: opencl_atomic_sub_global_i32,
		atomic_sub_global_u32: opencl_atomic_sub_global_u32,
		atomic_sub_global_u64: AtomicFn_u64,

		atomic_and_global_i32: opencl_atomic_and_global_i32,
		atomic_and_global_u32: opencl_atomic_and_global_u32,
		atomic_and_global_u64: AtomicFn_u64,

		atomic_or_global_i32: opencl_atomic_or_global_i32,
		atomic_or_global_u32: opencl_atomic_or_global_u32,
		atomic_or_global_u64: AtomicFn_u64,

		atomic_xor_global_i32: opencl_atomic_xor_global_i32,
		atomic_xor_global_u32: opencl_atomic_xor_global_u32,
		atomic_xor_global_u64: AtomicFn_u64,

		atomic_exch_global_i32: opencl_atomic_xchg_global_i32,
		atomic_exch_global_u32: opencl_atomic_xchg_global_u32,
		atomic_exch_global_u64: AtomicFn_u64,

		atomic_min_global_i32: opencl_atomic_min_global_i32,
		atomic_min_global_u32: opencl_atomic_min_global_u32,
		atomic_min_global_u64: AtomicFn_u64,

		atomic_max_global_i32: opencl_atomic_max_global_i32,
		atomic_max_global_u32: opencl_atomic_max_global_u32,
		atomic_max_global_u64: AtomicFn_u64,

		atomic_cas_global_i32: opencl_atomic_cmpxchg_global_i32,
		atomic_cas_global_u32: opencl_atomic_cmpxchg_global_u32,
		atomic_cas_global_i64: /* TODO */ @|p: &mut[1] u64, cmp: u64, val: u64| -> u64 { undef[u64]() },

		atomic_inc_global_u32: opencl_atomic_inc_global_u32,
		atomic_dec_global_u32: opencl_atomic_dec_global_u32,

		yield: @|| { }//cuda_nanosleep(0u32)
	});
}

fn @ShuffleDirFn_i32(i32, u32, u32) -> i32 { undef[i32]() }
fn @ShuffleDirFn_u32(u32, u32, u32) -> u32 { undef[u32]() }
fn @ShuffleDirFn_f32(f32, u32, u32) -> f32 { undef[f32]() }


fn @opencl_sub_group(idx: fn () -> u32, membermask: u64, num_threads: fn () -> u32, body: fn (gpu_wave_context) -> ()) -> () {
	@@body(gpu_wave_context{
		idx: idx,

		membermask: @|| membermask,

		threads: @|body| opencl_thread(@|i| match i { 1 => opencl_get_sub_group_local_id(), _ => 0u32 }, body),

		num_threads: num_threads,

		barrier: @|| opencl_sub_group_barrier(1u32),
		barrier_all: @|predicate| opencl_sub_group_all(opencl_pred(predicate)) != 0,
		barrier_any: @|predicate| opencl_sub_group_any(opencl_pred(predicate)) != 0,
		barrier_count: @|predicate:bool| { undef[i32]() }, //cuda_popc_u32(cuda_warp_sync_vote(membermask, predicate)),
		barrier_vote: @|predicate:bool| { undef[u64]() }, //cuda_warp_sync_vote(membermask, predicate),

		// activemask: cuda_warp_activemask,

		/* TODO: assert width = num_threads */
		shfl_i32: @|x:i32, src_lane:i32, width:u32| opencl_intel_sub_group_shuffle_i32(x, src_lane as u32),
		shfl_u32: @|x:u32, src_lane:i32, width:u32| opencl_intel_sub_group_shuffle_u32(x, src_lane as u32),
		// shfl_i64: @|x:i64, src_lane:i32, width:u32| cuda_warp_shfl_i64(membermask, x, src_lane, width),
		// shfl_u64: @|x:u64, src_lane:i32, width:u32| cuda_warp_shfl_u64(membermask, x, src_lane, width),
		// shfl_f32: @|x:f32, src_lane:i32, width:u32| opencl_intel_sub_group_shuffle_f32(x, src_lane as u32),
		// shfl_f64: @|x:f64, src_lane:i32, width:u32| cuda_warp_shfl_f64(membermask, x, src_lane, width),

		shfl_up_i32: ShuffleDirFn_i32, //@|x:i32, delta:u32, width:u32| cuda_warp_shfl_up_i32(membermask, x, delta, width),
		shfl_up_u32: ShuffleDirFn_u32, //@|x:u32, delta:u32, width:u32| cuda_warp_shfl_up_u32(membermask, x, delta, width),
		// shfl_up_i64: @|x:i64, delta:u32, width:u32| cuda_warp_shfl_up_i64(membermask, x, delta, width),
		// shfl_up_u64: @|x:u64, delta:u32, width:u32| cuda_warp_shfl_up_u64(membermask, x, delta, width),
		// shfl_up_f32: ShuffleDirFn_f32, //@|x:f32, delta:u32, width:u32| cuda_warp_shfl_up_f32(membermask, x, delta, width),
		// shfl_up_f64: @|x:f64, delta:u32, width:u32| cuda_warp_shfl_up_f64(membermask, x, delta, width),

		shfl_down_i32: ShuffleDirFn_i32, //@|x:i32, delta:u32, width:u32| cuda_warp_shfl_down_i32(membermask, x, delta, width),
		shfl_down_u32: ShuffleDirFn_u32, //@|x:u32, delta:u32, width:u32| cuda_warp_shfl_down_u32(membermask, x, delta, width),
		// shfl_down_i64: @|x:i64, delta:u32, width:u32| cuda_warp_shfl_down_i64(membermask, x, delta, width),
		// shfl_down_u64: @|x:u64, delta:u32, width:u32| cuda_warp_shfl_down_u64(membermask, x, delta, width),
		// shfl_down_f32: ShuffleDirFn_f32, //@|x:f32, delta:u32, width:u32| cuda_warp_shfl_down_f32(membermask, x, delta, width),
		// shfl_down_f64: @|x:f64, delta:u32, width:u32| cuda_warp_shfl_down_f64(membermask, x, delta, width),

		shfl_bfly_i32: @|x:i32, lane_mask:i32, width:u32| opencl_intel_sub_group_shuffle_xor_i32(x, lane_mask as u32),
		shfl_bfly_u32: @|x:u32, lane_mask:i32, width:u32| opencl_intel_sub_group_shuffle_xor_u32(x, lane_mask as u32),
		// shfl_bfly_i64: @|x:i64, lane_mask:i32, width:u32| cuda_warp_shfl_bfly_i64(membermask, x, lane_mask, width),
		// shfl_bfly_u64: @|x:u64, lane_mask:i32, width:u32| cuda_warp_shfl_bfly_u64(membermask, x, lane_mask, width),
		// shfl_bfly_f32: @|x:f32, lane_mask:i32, width:u32| opencl_intel_sub_group_shuffle_xor_f32(x, lane_mask as u32),
		// shfl_bfly_f64: @|x:f64, lane_mask:i32, width:u32| cuda_warp_shfl_bfly_f64(membermask, x, lane_mask, width),

		// match_any_i32: @|x:i32| cuda_warp_match_any_i32(membermask, x),
		// match_any_u32: @|x:u32| cuda_warp_match_any_u32(membermask, x),
		// match_any_i64: @|x:i64| cuda_warp_match_any_i64(membermask, x),
		// match_any_u64: @|x:u64| cuda_warp_match_any_u64(membermask, x),
		// match_any_f32: @|x:f32| cuda_warp_match_any_f32(membermask, x),
		// match_any_f64: @|x:f64| cuda_warp_match_any_f64(membermask, x),

		// match_all_i32: @|x:i32, predicate:&mut i32| cuda_warp_match_all_i32(membermask, x, predicate),
		// match_all_u32: @|x:u32, predicate:&mut i32| cuda_warp_match_all_u32(membermask, x, predicate),
		// match_all_i64: @|x:i64, predicate:&mut i32| cuda_warp_match_all_i64(membermask, x, predicate),
		// match_all_u64: @|x:u64, predicate:&mut i32| cuda_warp_match_all_u64(membermask, x, predicate),
		// match_all_f32: @|x:f32, predicate:&mut i32| cuda_warp_match_all_f32(membermask, x, predicate),
		// match_all_f64: @|x:f64, predicate:&mut i32| cuda_warp_match_all_f64(membermask, x, predicate),

		lanemask: @|| cuda_lanemask() as u64,
		lanemask_le: @|| cuda_lanemask_le() as u64,
		lanemask_lt: @|| cuda_lanemask_lt() as u64,
		lanemask_ge: @|| cuda_lanemask_ge() as u64,
		lanemask_gt: @|| cuda_lanemask_gt() as u64
	});
}

fn @opencl_group(idx: fn (i32) -> u32, thread_idx: fn (i32) -> u32, num_threads: fn (i32) -> u32, sub_group_size: fn () -> u32, num_sub_groups: fn () -> u32, body: fn (gpu_group_context) -> ()) -> () {
	@@body(gpu_group_context {
		idx: idx,
		waves: @|body| opencl_sub_group(opencl_get_sub_group_id, get_member_mask_u64(sub_group_size()), sub_group_size, body),
		threads: @|body| opencl_thread(thread_idx, body),
		num_waves: num_sub_groups,
		num_threads: num_threads,
		barrier: @|| opencl_barrier(2u32),
		barrier_all: @|_:bool| { undef[bool]() }, //cuda_block_sync_all,
		barrier_any: @|_:bool| { undef[bool]() }, //cuda_block_sync_any,
		barrier_count: @|_:bool| { undef[i32]() } //cuda_block_sync_count
	});
}

fn opencl_launch(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), wrap_index: index_wrapper, wrap_dim: dim_wrapper, body: fn (gpu_grid_context, Intrinsics) -> ()) -> () {
	// TODO: assert(sub_group_size <= 64)
	// TODO: OpenCL's wave size
	// let sub_group_size = @|| opencl_get_sub_group_size();
	let sub_group_size = @|| 32u32;

	let num_threads_per_group = wrap_dim(@|i| {
		if ?block_dim && ?i {
			match i { 0 => block_dim(0) as u32, 1 => block_dim(1) as u32, 2 => block_dim(2) as u32, _ => 1u32 }
		}
		else {
			opencl_get_local_size(i as u32) as u32
		}
	});

	let num_subgroups_per_group = opencl_get_num_sub_groups;

	let num_groups = wrap_dim(@|i| {
		if ?grid_dim && ?i {
			match i { 0 => grid_dim(0) as u32, 1 => grid_dim(1) as u32, 2 => grid_dim(2) as u32, _ => 1u32 }
		}
		else {
			opencl_get_num_groups(i as u32) as u32
		}
	});

	let num_subgroups = @|| (num_groups(0) * num_groups(1) * num_groups(2)) * num_subgroups_per_group();

	let num_threads = wrap_dim(@|i| opencl_get_global_size(i as u32) as u32);

	let group_idx = wrap_index(@|i| opencl_get_group_id(i as u32) as u32);

	let linear_group_idx = @|| (group_idx(2) * num_groups(1) + group_idx(1)) * num_groups(0) + group_idx(0);

	let thread_idx = wrap_index(@|i| opencl_get_local_id(i as u32) as u32);

	let global_thread_idx = wrap_index(@|i| opencl_get_global_id(i as u32) as u32);

	//let linear_thread_idx = get_local_linear_id;

	let global_sub_group_idx = @|| linear_group_idx() * num_subgroups_per_group() + opencl_get_sub_group_id();

	opencl(device, (grid_dim(0) * block_dim(0), grid_dim(1) * block_dim(1), grid_dim(2) * block_dim(2)), (block_dim(0), block_dim(1), block_dim(2)), @|| @@body(gpu_grid_context {
		device: device,
		groups: @|body| opencl_group(group_idx, thread_idx, num_threads_per_group, sub_group_size, num_subgroups_per_group, body),
		waves: @|body| opencl_sub_group(global_sub_group_idx, get_member_mask_u64(sub_group_size()), sub_group_size, body),
		threads: @|body| opencl_thread(global_thread_idx, body),
		num_groups: num_groups,
		num_waves: num_subgroups,
		num_threads: num_threads
	}, opencl_intrinsics));
}

fn opencl_launch_1d(device: i32, grid_dim: i32, block_dim: i32, body: fn (gpu_grid_context, Intrinsics) -> ()) -> () {
	@@opencl_launch(device, (grid_dim, 1, 1), (block_dim, 1, 1), wrap_index_1d, wrap_dim_1d, body)
}

fn opencl_launch_2d(device: i32, grid_dim: (i32, i32), block_dim: (i32, i32), body: fn (gpu_grid_context, Intrinsics) -> ()) -> () {
	@@opencl_launch(device, (grid_dim(0), grid_dim(1), 1), (block_dim(0), block_dim(1), 1), wrap_index_2d, wrap_dim_2d, body)
}

fn opencl_launch_3d(device: i32, grid_dim: (i32, i32, i32), block_dim: (i32, i32, i32), body: fn (gpu_grid_context, Intrinsics) -> ()) -> () {
	@@opencl_launch(device, grid_dim, block_dim, wrap_index_3d, wrap_dim_3d, body)
}
