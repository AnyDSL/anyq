
#[export]
fn main() -> i32 {
	let device = createAccDevice();

	let num_elements = (100 * 1024 * 1024) as u32;
	let queue_size = 1024 * 1024;
	// let queue_test = createQueueTest[u32](device, genericQueueElementType[u32](), (num_elements + 2) * sizeof[u32]() as u32);
	let queue_test = createQueueTest[u32](device, indexQueueElementType(), (num_elements + 2) * sizeof[u32]() as u32);
	let block_dim = 256;

	let ref_test = createBaseTest(device, (num_elements + 0) * sizeof[u32]() as u32);

	fn element(_intrinsics: Intrinsics, i: u32) -> u32 {
		let rand_state = xorshift32(i);
		let _octaves = (rand_state % 8) as i32;
		i*i // (simplexNoiseFractal([randf(i, intrinsics), randf(i * 2, intrinsics), randf(i * 3, intrinsics)], octaves, intrinsics) * 8.0f32) as u32
	}

	for device_memory, _ in ref_test.run_test() {
		let _ref_output = bitcast[&mut addrspace(1) [u32]](device_memory);

		for queue, device_memory, device_failed_flag in queue_test.run_test(queue_size) {
			let output = bitcast[&mut addrspace(1) [u32]](device_memory);
			let next_input = &mut output(num_elements);
			let next_output = &mut output(num_elements + 1);

			for grid, _intrinsics in device.launch_1d(1, 1) {
				for _thread in grid.threads() {
					*next_input  = 0;
					*next_output = 0;
				}
			}

			for grid, intrinsics in device.launch_1d(42, block_dim) {
				for wave in grid.waves() {
						// let mut rand_state = xorshift32(((wave.idx() + 23) * 42) as u32);

						fn producer(sink: Sink[u32, gpu_thread_context], thread: gpu_thread_context) -> bool {
							let el = thread.atomic_add_global_u32(next_input, 1);

							if el < num_elements {
								//device.print_i32("produce - el: %d\n", el as i32);
								sink(element(intrinsics, el), thread);
								true
							}
							else {
								thread.atomic_exch_global_u32(next_input, num_elements);
								false
							}
						}

						fn consumer(el: u32, thread: gpu_thread_context) -> () {
							let i = thread.atomic_add_global_u32(next_output, 1);
							output(i) = el;
						}


						fn should_flush(thread: gpu_thread_context) -> bool {
							// let should_push = if rand_state as i32 < 0 { 1 } else { 0 };
							// if drain || wave.shfl_i32(should_push, 0, wave.num_threads()) == 0 {  // note: cannot use barrier_any(), would mess up the probabilities
							queue.size(thread) >= wave.num_threads() as i32
						}

						producer_consumer_loop_wave[u32](wave, producer, consumer, queue, should_flush);
				}
			}

			for grid, _intrinsics in device.launch_1d(1, 1) {
				for thread in grid.threads() {
					if *next_input != num_elements {
						device.print_i32a("inconsistent next_input %d != %d\n", [*next_input as i32, num_elements as i32]);
						thread.atomic_exch_global_u32(device_failed_flag, -1);
					}
				}
			}

			/*
			let num_per_block = 256;
			let num_blocks = div_up(num_elements as i32, block_dim);

			for grid, intrinsics in device.launch_1d(num_blocks / num_per_block, block_dim) {
				for thread in grid.threads() {
					for i in range_step(thread.idx(0) as i32, num_elements as i32, grid.num_threads(0) as i32) {
						let el = element(intrinsics, i as u32);
						ref_output(i) = el;
					}
				}
			}

			for grid, _intrinsics in device.launch_1d(num_blocks / num_per_block, block_dim) {
				for thread in grid.threads() {
					for i in range_step(thread.idx(0) as i32, num_elements as i32, grid.num_threads(0) as i32) {
						if ref_output(i) != output(i) {
							device.print_i32a("Difference in queue output for element(%d) = %d\n", [i, output(i) as i32]);
						}
					}
				}
			}
			*/

			true
		}

		queue_test.finish() == 0
	}

	ref_test.finish()
}
