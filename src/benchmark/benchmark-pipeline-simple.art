

fn @simple_pipeline_benchmark(device: AccDevice, block_size: i32, num_input_elements: i32, workload_size_producer: i32, workload_size_consumer: i32) {
	let pipeline_state_alloc = device.alloc(1 * sizeof[i32]());
	let pipeline_state = pipeline_state_alloc.data as &mut addrspace(1) [i32];
	let input_elements = &mut pipeline_state(0);
	let completed_elements = &mut pipeline_state(1);

	// TODO: release resources


	QueueBenchmark {
		enum_param_names = @|enum_param| {
			enum_param("block_size"); enum_param("num_input_elements"); enum_param("workload_size_producer"); enum_param("workload_size_consumer");
		},

		enum_param_values = @|v| {
			v.enum_i32(block_size); v.enum_i32(num_input_elements); v.enum_i32(workload_size_producer); v.enum_i32(workload_size_consumer);
		},

		reset = @|grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					*input_elements = num_input_elements;
					*completed_elements = 0;
				}
			}
		},

		run = @|queue_instrumentation, num_threads, i| {
			for grid in device.launch_1d(div_up(num_threads, block_size), block_size) {
				for wave in grid.waves() {
					for thread in wave.threads() {
						let thread_id = wave.idx() * wave.num_threads() + thread.idx(0);

						if thread_id < num_threads as u32 {
							for q in queue_instrumentation.record(thread) {
								let mut next_value = rng::xorseed32(thread_id + i as u32 * num_threads as u32);

								let simulate_workload = @|workload_size: i32| {
									for _ in range(0, workload_size) {
										next_value = rng::xorshift32(next_value);
									}

									next_value
								};

								while (thread.atomic_load_global_i32(completed_elements, memory_order::relaxed) < num_input_elements) {
									let should_drain = @|thread: gpu_thread_context| q.size(thread) >= wave.num_threads() as i32 || thread.atomic_load_global_i32(input_elements, memory_order::relaxed) <= 0;

									if wave.barrier_any(if thread.idx(0) == 0 { should_drain(thread) } else { false }) {
										if for value in q.pop(thread) {
											next_value = value;
										} > 0 {
											simulate_workload(workload_size_consumer);
											thread.atomic_add_global_i32(completed_elements, 1, memory_order::relaxed);
										}
									}
									else {
										if thread.atomic_sub_global_i32(input_elements, 1, memory_order::relaxed) > 0 {
											next_value = simulate_workload(workload_size_producer);

											while for q.push(thread) {
												next_value
											} < 1 {};
										}
										else {
											thread.atomic_add_global_i32(input_elements, 0, memory_order::relaxed);
										}
									}
								}
							}
						}
					}
				}
			}
		}
	}
}
