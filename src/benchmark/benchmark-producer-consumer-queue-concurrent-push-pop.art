
#[import(cc = "device")] fn magic_p_id(u32) -> u32;  // HACK to force evaluation of expression on the host


fn producer_consumer_queue_concurrent_push_pop_benchmark(device_id: i32, block_size: i32, attempts_per_thread: i32, p_enq: f32, p_deq: f32, workload_size: i32, queue_size: i32, queue_name: &[u8], create_queue: fn(AccDevice, i32) -> ProducerConsumerQueue[u32]) {
	print_string("queue;queue_size;block_size;p_enq;p_deq;workload_size\n");
	print_string(queue_name); print_char(';');
	print_i32(queue_size); print_char(';');
	print_i32(block_size); print_char(';');
	print_f32(p_enq); print_char(';');
	print_f32(p_deq); print_char(';');
	print_i32(workload_size); print_char('\n'); print_char('\n');

	let device = createAccDevice(device_id);

	let stats_buffer_alloc = device.alloc(sizeof[QueueBenchmarkStatistics]());
	let stats_buffer = bitcast[&mut addrspace(1) QueueBenchmarkStatistics](stats_buffer_alloc.data);

	let queue = create_queue(device, queue_size);

	// TODO: release resources


	let rand_mask = @|p: f32| (((-1 as u32) as f64 * p as f64) as u32);
	let c_enq = magic_p_id(rand_mask(p_enq));
	let c_deq = magic_p_id(rand_mask(p_deq));

	QueueBenchmark {
		reset = @|| {
			for grid in device.launch_1d(1, 1) {
				for thread in grid.threads() {
					if thread.idx(0) == 0 {
						*stats_buffer = init_benchmark_stats();
					}
				}

				queue.reset(grid);
			}
		},

		run = @|num_threads: i32| {
			for grid in device.launch_1d(div_up(num_threads, block_size), block_size) {
				for wave in grid.waves() {
					for thread in wave.threads() {
						let thread_id = wave.idx() * wave.num_threads() + thread.idx(0);

						let mut stats = init_benchmark_stats();

						let mut rand_state = rng::xorseed32(thread_id);

						let sample = @|c: u32| -> bool {
							rng::xorshift32(rand_state) < c
						};

						let simulate_workload = @|| {
							for _ in range(0, workload_size) {
								rand_state = rng::xorshift32(rand_state);
							}
						};

						for _ in range(0, attempts_per_thread) {
							if sample(c_enq) {
								if for _ in queue.push(thread) {
									simulate_workload();
									rand_state
								} > 0 {
									++stats.num_enqueues;
								}

								++stats.num_enqueue_attempts;
							}

							wave.barrier();

							if sample(c_deq) {
								if for _, _ in queue.pop(thread) {
									simulate_workload();
								} > 0 {
									++stats.num_dequeues;
								}

								++stats.num_dequeue_attempts;
							}

							wave.barrier();
						}

						thread.atomic_add_global_i32(stats_buffer.num_enqueues, stats.num_enqueues, memory_order::relaxed);
						thread.atomic_add_global_i32(stats_buffer.num_enqueue_attempts, stats.num_enqueue_attempts, memory_order::relaxed);
						thread.atomic_add_global_i32(stats_buffer.num_dequeues, stats.num_dequeues, memory_order::relaxed);
						thread.atomic_add_global_i32(stats_buffer.num_dequeue_attempts, stats.num_dequeue_attempts, memory_order::relaxed);
					}
				}
			}
		},

		read_stats = @|| {
			let mut stats: QueueBenchmarkStatistics;
			let stats_ref = &mut stats;

			runtime_copy(device.platform_device, stats_buffer_alloc.data, 0, 0, stats_ref as &mut[i8], 0, sizeof[QueueBenchmarkStatistics]());

			stats
		}
	}
}
