#[import(cc = "device", name = "[]{return __CUDA_ARCH__;}")] fn cuda_device_arch() -> i32;  // HACK

fn @createAccDevice(device: i32) {
	let platform_device = runtime_device(1, device);
	let its = runtime_device_check_feature_support(platform_device, "ITS");

	AccDevice {
		supports_its = its,
		supports_npot_atomic_inc = true,

		launch_1d = @|body|@|num_groups, group_size| cuda_launch(device, (num_groups, 1, 1), (group_size, 1, 1), wrap_index_1d, wrap_dim_1d, body),
		synchronize = || synchronize_cuda(device),
		alloc = |size| alloc_cuda(device, size),
		platform_device = platform_device,

		platform_name = "cuda",

		print_i32 = @|format: &[u8], arg: i32| {
			cuda_vprintf(format, &arg as &[u8]);
		},

		print_2xi32 = @|format: &[u8], arg1: i32, arg2: i32| {
			let args:&[i32] = [arg1, arg2];
			cuda_vprintf(format, args as &[u8]);
		},

		print_3xi32 = @|format: &[u8], arg1: i32, arg2: i32, arg3: i32| {
			let args:&[i32] = [arg1, arg2, arg3];
			cuda_vprintf(format, args as &[u8]);
		}
	}
}

fn @createDefaultAccDevice() = createAccDevice(0);


fn @cuda_pred(b: bool) -> i32 {
	if b { 1 } else { 0 }
}

fn @cuda_memory_barrier(order: memory_order) {
	if cuda_device_arch() >= 700 {
		(@|| {
			match order {
				memory_order::acquire => asm("fence.acq_rel.gpu;" :::: "volatile"),
				memory_order::release => asm("fence.acq_rel.gpu;" :::: "volatile"),
				memory_order::acq_rel => asm("fence.acq_rel.gpu;" :::: "volatile"),
				memory_order::seq_cst => asm("fence.sc.gpu;" :::: "volatile"),
				_ => ()
			}
		})()
	}
	else {
		cuda_threadfence()
	}
}

fn @cuda_legacy_atomic_memory_barrier_pre(order:memory_order) {
	match order {
		memory_order::release => cuda_memory_barrier(memory_order::release),
		memory_order::acq_rel => cuda_memory_barrier(memory_order::acq_rel),
		memory_order::seq_cst => cuda_memory_barrier(memory_order::seq_cst),
		_ => ()
	}
}

fn @cuda_legacy_atomic_memory_barrier_post(order:memory_order) {
	match order {
		memory_order::acquire => cuda_memory_barrier(memory_order::acquire),
		memory_order::acq_rel => cuda_memory_barrier(memory_order::acq_rel),
		memory_order::seq_cst => cuda_memory_barrier(memory_order::seq_cst),
		_ => ()
	}
}

fn @cuda_legacy_atomic_memory_order_wrap_load[T](f: fn(&addrspace(1) T) -> T) {
	@|location:&addrspace(1) T, order:memory_order| -> T {
		cuda_legacy_atomic_memory_barrier_pre(order);
		let res = f(location);
		cuda_legacy_atomic_memory_barrier_post(order);
		res
	}
}

fn @cuda_legacy_atomic_memory_order_wrap_store[T](f: fn(&mut addrspace(1) T, T) -> ()) {
	@|location:&mut addrspace(1) T, value:T, order:memory_order| -> () {
		cuda_legacy_atomic_memory_barrier_pre(order);
		f(location, value);
		cuda_legacy_atomic_memory_barrier_post(order);
	}
}

fn @cuda_legacy_atomic_memory_order_wrap_rmw[T](f: fn(&mut addrspace(1) T, T) -> T) {
	@|location:&mut addrspace(1) T, value:T, order:memory_order| -> T {
		cuda_legacy_atomic_memory_barrier_pre(order);
		let old = f(location, value);
		cuda_legacy_atomic_memory_barrier_post(order);
		old
	}
}

fn @cuda_legacy_atomic_memory_order_wrap_cas[T](f: fn(&mut addrspace(1) T, T, T) -> T, cmp: fn(T, T) -> bool) {
	@|location:&mut addrspace(1) T, expected:T, desired:T, memory_order_succ:memory_order, memory_order_fail:memory_order| {
		cuda_legacy_atomic_memory_barrier_pre(stronger_memory_order(memory_order_succ, memory_order_fail));

		let old = f(location, expected, desired);

		if cmp(old, expected) {
			cuda_legacy_atomic_memory_barrier_post(memory_order_succ);
			(old, true)
		}
		else {
			cuda_legacy_atomic_memory_barrier_post(memory_order_fail);
			(old, false)
		}
	}
}


fn @atomic_load_global_i32(location: &addrspace(1) i32, order: memory_order) -> i32 = atomic_load_global_u32(location as &addrspace(1) u32, order) as i32;
fn @atomic_load_global_u32(location: &addrspace(1) u32, order: memory_order) -> u32 {
	let mut value: u32;

	if cuda_device_arch() >= 700 {
		(@|| {
			match order {
				memory_order::relaxed => asm("ld.relaxed.gpu.b32 %0, [%1];" : "=r"(value) : "l"(location) : "memory"),
				memory_order::acquire => asm("ld.acquire.gpu.b32 %0, [%1];" : "=r"(value) : "l"(location) : "memory"),
				_ => cuda_trap()
			}
		})();
	}
	else {
		cuda_legacy_atomic_memory_barrier_pre(order);
		asm("ld.volatile.global.b32 %0, [%1];" : "=r"(value) : "l"(location) : "memory");
		cuda_legacy_atomic_memory_barrier_post(order);
	}

	value
}

fn @atomic_load_global_i64(location: &addrspace(1) i64, order: memory_order) = atomic_load_global_u64(location as &addrspace(1) u64, order) as i64;
fn @atomic_load_global_u64(location: &addrspace(1) u64, order: memory_order) -> u64 {
	let mut value: u64;

	if cuda_device_arch() >= 700 {
		(@|| {
			match order {
				memory_order::relaxed => asm("ld.relaxed.gpu.b64 %0, [%1];" : "=l"(value) : "l"(location) : "memory"),
				memory_order::acquire => asm("ld.acquire.gpu.b64 %0, [%1];" : "=l"(value) : "l"(location) : "memory"),
				_ => cuda_trap()
			}
		})();
	}
	else {
		cuda_legacy_atomic_memory_barrier_pre(order);
		asm("ld.volatile.global.b64 %0, [%1];" : "=l"(value) : "l"(location) : "memory");
		cuda_legacy_atomic_memory_barrier_post(order);
	}

	value
}

fn @atomic_store_global_i32(location: &addrspace(1) i32, value: i32, order: memory_order) = atomic_store_global_u32(location as &addrspace(1) u32, value as u32, order);
fn @atomic_store_global_u32(location: &addrspace(1) u32, value: u32, order: memory_order) -> () {
	if cuda_device_arch() >= 700 {
		(@|| {
			match order {
				memory_order::relaxed => asm("st.relaxed.gpu.b32 [%0], %1;" : : "l"(location), "r"(value) : "memory"),
				memory_order::release => asm("st.release.gpu.b32 [%0], %1;" : : "l"(location), "r"(value) : "memory"),
				_ => cuda_trap()
			}
		})()
	}
	else {
		cuda_legacy_atomic_memory_barrier_pre(order);
		asm("st.volatile.global.b32 [%0], %1;" : : "l"(location), "r"(value) : "memory");
		cuda_legacy_atomic_memory_barrier_post(order);
	}
}

fn @atomic_store_global_i64(location: &addrspace(1) i64, value: i64, order: memory_order) = atomic_store_global_u64(location as &addrspace(1) u64, value as u64, order);
fn @atomic_store_global_u64(location: &addrspace(1) u64, value: u64, order: memory_order) -> () {
	if cuda_device_arch() >= 700 {
		(@|| {
			match order {
				memory_order::relaxed => asm("st.relaxed.gpu.b64 [%0], %1;" : : "l"(location), "l"(value) : "memory"),
				memory_order::release => asm("st.release.gpu.b64 [%0], %1;" : : "l"(location), "l"(value) : "memory"),
				_ => cuda_trap()
			}
		})()
	}
	else {
		cuda_legacy_atomic_memory_barrier_pre(order);
		asm("st.volatile.global.b64 [%0], %1;" : : "l"(location), "l"(value) : "memory");
		cuda_legacy_atomic_memory_barrier_post(order);
	}
}


// fn @cuda_atomic_wait_and_transition[T](f: fn(&mut addrspace(1) T, T, T) -> T, cmp: fn(T, T) -> bool) {
// 	@|location:&mut addrspace(1) T, expected:T, desired:T, order:memory_order, _debug_msg:&[u8]| -> () {
// 		cuda_legacy_atomic_memory_barrier_pre(order);
// 		while !cmp(f(location, expected, desired), expected) {
// 			cuda_threadfence();
// 		}
// 		cuda_legacy_atomic_memory_barrier_post(order);
// 	}
// }

fn @cuda_thread(idx: fn(i32) -> u32, gid: fn() -> u32, body: fn(thread_context) -> ()) -> () {
	let sleep = @|t: u32| {
		if cuda_device_arch() >= 700 {
			asm("nanosleep.u32 %0;\n" :: "r"(t) :: "volatile");  // use asm to avoid compilation error on devices < 700 due to __nanosleep not being defined
		}
		else {
			cuda_threadfence();
		}
	};

	@body(thread_context {
		idx = idx,

		gid = gid,
		uid = @|| gid() as i32,

		atomic_load_global_i32 = atomic_load_global_i32,
		atomic_load_global_u32 = atomic_load_global_u32,
		atomic_load_global_i64 = atomic_load_global_i64,
		atomic_load_global_u64 = atomic_load_global_u64,

		atomic_load_global_i32_coalesced = cuda_legacy_atomic_memory_order_wrap_load[i32](@|location| { let mut res:i32; asm("ld.volatile.global.b32 %0, [%1];" : "=r"(res) : "l"(location) : "memory"); res }),
		atomic_load_global_u32_coalesced = cuda_legacy_atomic_memory_order_wrap_load[u32](@|location| { let mut res:u32; asm("ld.volatile.global.b32 %0, [%1];" : "=r"(res) : "l"(location) : "memory"); res }),
		atomic_load_global_i64_coalesced = cuda_legacy_atomic_memory_order_wrap_load[i64](@|location| { let mut res:i64; asm("ld.volatile.global.b64 %0, [%1];" : "=l"(res) : "l"(location) : "memory"); res }),
		atomic_load_global_u64_coalesced = cuda_legacy_atomic_memory_order_wrap_load[u64](@|location| { let mut res:u64; asm("ld.volatile.global.b64 %0, [%1];" : "=l"(res) : "l"(location) : "memory"); res }),

		atomic_store_global_i32 = atomic_store_global_i32,
		atomic_store_global_u32 = atomic_store_global_u32,
		atomic_store_global_i64 = atomic_store_global_i64,
		atomic_store_global_u64 = atomic_store_global_u64,

		atomic_store_global_i32_coalesced = cuda_legacy_atomic_memory_order_wrap_store[i32](@|location, value| asm("st.volatile.global.b32 [%0], %1;" : : "l"(location), "r"(value) : "memory")),
		atomic_store_global_u32_coalesced = cuda_legacy_atomic_memory_order_wrap_store[u32](@|location, value| asm("st.volatile.global.b32 [%0], %1;" : : "l"(location), "r"(value) : "memory")),
		atomic_store_global_i64_coalesced = cuda_legacy_atomic_memory_order_wrap_store[i64](@|location, value| asm("st.volatile.global.b64 [%0], %1;" : : "l"(location), "l"(value) : "memory")),
		atomic_store_global_u64_coalesced = cuda_legacy_atomic_memory_order_wrap_store[u64](@|location, value| asm("st.volatile.global.b64 [%0], %1;" : : "l"(location), "l"(value) : "memory")),

		atomic_add_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_add_global_i32),
		atomic_add_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_add_global_u32),
		atomic_add_global_i64 = cuda_legacy_atomic_memory_order_wrap_rmw[i64](@|location, value| cuda_atomic_add_global_u64(location as &mut addrspace(1) u64, value as u64) as i64),
		atomic_add_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_add_global_u64),

		atomic_sub_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_sub_global_i32),
		atomic_sub_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_sub_global_u32),
		atomic_sub_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_sub_global_u64),

		atomic_and_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_and_global_i32),
		atomic_and_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_and_global_u32),
		atomic_and_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_and_global_u64),

		atomic_or_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_or_global_i32),
		atomic_or_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_or_global_u32),
		atomic_or_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_or_global_u64),

		atomic_xor_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_xor_global_i32),
		atomic_xor_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_xor_global_u32),
		atomic_xor_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_xor_global_u64),

		atomic_exch_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_exch_global_i32),
		atomic_exch_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_exch_global_u32),
		atomic_exch_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_exch_global_u64),

		atomic_min_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_min_global_i32),
		atomic_min_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_min_global_u32),
		atomic_min_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_min_global_u64),

		atomic_max_global_i32 = cuda_legacy_atomic_memory_order_wrap_rmw[i32](cuda_atomic_max_global_i32),
		atomic_max_global_u32 = cuda_legacy_atomic_memory_order_wrap_rmw[u32](cuda_atomic_max_global_u32),
		atomic_max_global_u64 = cuda_legacy_atomic_memory_order_wrap_rmw[u64](cuda_atomic_max_global_u64),

		atomic_cas_global_i32 = cuda_legacy_atomic_memory_order_wrap_cas[i32](cuda_atomic_cas_global_i32, @|a, b| a == b),
		atomic_cas_global_u32 = cuda_legacy_atomic_memory_order_wrap_cas[u32](cuda_atomic_cas_global_u32, @|a, b| a == b),
		atomic_cas_global_i64 = cuda_legacy_atomic_memory_order_wrap_cas[i64](@|location, expected, desired| cuda_atomic_cas_global_u64(location as &mut addrspace(1) u64, expected as u64, desired as u64) as i64, @|a, b| a == b),
		atomic_cas_global_u64 = cuda_legacy_atomic_memory_order_wrap_cas[u64](cuda_atomic_cas_global_u64, @|a, b| a == b),
		atomic_cas_global_i32_weak = cuda_legacy_atomic_memory_order_wrap_cas[i32](cuda_atomic_cas_global_i32, @|a, b| a == b),
		atomic_cas_global_u32_weak = cuda_legacy_atomic_memory_order_wrap_cas[u32](cuda_atomic_cas_global_u32, @|a, b| a == b),
		atomic_cas_global_i64_weak = cuda_legacy_atomic_memory_order_wrap_cas[i64](@|location, expected, desired| cuda_atomic_cas_global_u64(location as &mut addrspace(1) u64, expected as u64, desired as u64) as i64, @|a, b| a == b),
		atomic_cas_global_u64_weak = cuda_legacy_atomic_memory_order_wrap_cas[u64](cuda_atomic_cas_global_u64, @|a, b| a == b),

		atomic_inc_global_u32 = cuda_atomic_inc_global_u32,

		memory_barrier = cuda_memory_barrier,

		timestamp = @|| cuda_globaltimer() as i64,
		timestamp32 = @|| cuda_globaltimer_lo() as i32,

		wait = @|f, _debug_msg| {
			for t in exponential_backoff(2, 128) {
				if f() { false } else { sleep(t as u32); true }
			}
		}
	})
}

fn @cuda_subwarp(idx: fn() -> u32, gid: fn() -> u32, membermask: u32, num_threads: u32, body: fn(wave_context) -> ()) -> () {
	let thread_idx = @|i: i32| match i { 0 => cuda_laneid(), _ => 0 };

	@body(wave_context {
		idx = idx,

		membermask = @|| membermask as u64,

		threads = @|body|@|| cuda_thread(thread_idx, @|| gid() * num_threads + thread_idx(0), body),

		num_threads = @|| num_threads,

		barrier = @|| cuda_warp_sync(membermask),
		barrier_all = @|predicate| cuda_warp_sync_all(membermask, cuda_pred(predicate)) != 0,
		barrier_any = @|predicate| cuda_warp_sync_any(membermask, cuda_pred(predicate)) != 0,
		barrier_count = @|predicate| cuda_popc_u32(cuda_warp_sync_vote(membermask, cuda_pred(predicate))),
		barrier_vote = @|predicate| cuda_warp_sync_vote(membermask, cuda_pred(predicate)) as u64,

		// activemask = cuda_warp_activemask,

		shfl_i32 = @|x:i32, src_lane:i32, width:u32| cuda_warp_shfl_i32(membermask, x, src_lane, width as i32),
		shfl_u32 = @|x:u32, src_lane:i32, width:u32| cuda_warp_shfl_u32(membermask, x, src_lane, width as i32),
		// shfl_i64 = @|x:i64, src_lane:i32, width:u32| cuda_warp_shfl_i64(membermask, x, src_lane, width as i32),
		// shfl_u64 = @|x:u64, src_lane:i32, width:u32| cuda_warp_shfl_u64(membermask, x, src_lane, width as i32),
		// shfl_f32 = @|x:f32, src_lane:i32, width:u32| cuda_warp_shfl_f32(membermask, x, src_lane, width as i32),
		// shfl_f64 = @|x:f64, src_lane:i32, width:u32| cuda_warp_shfl_f64(membermask, x, src_lane, width as i32),

		shfl_up_i32 = @|x:i32, delta:u32, width:u32| cuda_warp_shfl_up_i32(membermask, x, delta, width as i32),
		shfl_up_u32 = @|x:u32, delta:u32, width:u32| cuda_warp_shfl_up_u32(membermask, x, delta, width as i32),
		// shfl_up_i64 = @|x:i64, delta:u32, width:u32| cuda_warp_shfl_up_i64(membermask, x, delta, width as i32),
		// shfl_up_u64 = @|x:u64, delta:u32, width:u32| cuda_warp_shfl_up_u64(membermask, x, delta, width as i32),
		// shfl_up_f32 = @|x:f32, delta:u32, width:u32| cuda_warp_shfl_up_f32(membermask, x, delta, width as i32),
		// shfl_up_f64 = @|x:f64, delta:u32, width:u32| cuda_warp_shfl_up_f64(membermask, x, delta, width as i32),

		shfl_down_i32 = @|x:i32, delta:u32, width:u32| cuda_warp_shfl_down_i32(membermask, x, delta, width as i32),
		shfl_down_u32 = @|x:u32, delta:u32, width:u32| cuda_warp_shfl_down_u32(membermask, x, delta, width as i32),
		// shfl_down_i64 = @|x:i64, delta:u32, width:u32| cuda_warp_shfl_down_i64(membermask, x, delta, width as i32),
		// shfl_down_u64 = @|x:u64, delta:u32, width:u32| cuda_warp_shfl_down_u64(membermask, x, delta, width as i32),
		// shfl_down_f32 = @|x:f32, delta:u32, width:u32| cuda_warp_shfl_down_f32(membermask, x, delta, width as i32),
		// shfl_down_f64 = @|x:f64, delta:u32, width:u32| cuda_warp_shfl_down_f64(membermask, x, delta, width as i32),

		shfl_bfly_i32 = @|x:i32, lane_mask:i32, width:u32| cuda_warp_shfl_xor_i32(membermask, x, lane_mask, width as i32),
		shfl_bfly_u32 = @|x:u32, lane_mask:i32, width:u32| cuda_warp_shfl_xor_u32(membermask, x, lane_mask, width as i32),
		// shfl_bfly_i64 = @|x:i64, lane_mask:i32, width:u32| cuda_warp_shfl_xor_i64(membermask, x, lane_mask, width as i32),
		// shfl_bfly_u64 = @|x:u64, lane_mask:i32, width:u32| cuda_warp_shfl_xor_u64(membermask, x, lane_mask, width as i32),
		// shfl_bfly_f32 = @|x:f32, lane_mask:i32, width:u32| cuda_warp_shfl_xor_f32(membermask, x, lane_mask, width as i32),
		// shfl_bfly_f64 = @|x:f64, lane_mask:i32, width:u32| cuda_warp_shfl_xor_f64(membermask, x, lane_mask, width as i32),

		// match_any_i32 = @|x:i32| cuda_warp_match_any_i32(membermask, x),
		// match_any_u32 = @|x:u32| cuda_warp_match_any_u32(membermask, x),
		// match_any_i64 = @|x:i64| cuda_warp_match_any_i64(membermask, x),
		// match_any_u64 = @|x:u64| cuda_warp_match_any_u64(membermask, x),
		// match_any_f32 = @|x:f32| cuda_warp_match_any_f32(membermask, x),
		// match_any_f64 = @|x:f64| cuda_warp_match_any_f64(membermask, x),

		// match_all_i32 = @|x:i32, predicate:&mut i32| cuda_warp_match_all_i32(membermask, x, predicate),
		// match_all_u32 = @|x:u32, predicate:&mut i32| cuda_warp_match_all_u32(membermask, x, predicate),
		// match_all_i64 = @|x:i64, predicate:&mut i32| cuda_warp_match_all_i64(membermask, x, predicate),
		// match_all_u64 = @|x:u64, predicate:&mut i32| cuda_warp_match_all_u64(membermask, x, predicate),
		// match_all_f32 = @|x:f32, predicate:&mut i32| cuda_warp_match_all_f32(membermask, x, predicate),
		// match_all_f64 = @|x:f64, predicate:&mut i32| cuda_warp_match_all_f64(membermask, x, predicate),

		lanemask = @|| cuda_lanemask() as u64,
		lanemask_le = @|| cuda_lanemask_le() as u64,
		lanemask_lt = @|| cuda_lanemask_lt() as u64,
		lanemask_ge = @|| cuda_lanemask_ge() as u64,
		lanemask_gt = @|| cuda_lanemask_gt() as u64
	})
}

fn @cuda_block(idx: fn(i32) -> u32, gid: fn() -> u32, thread_idx: fn(i32) -> u32, block_size: fn(i32) -> u32, warp_size: u32, body: fn(group_context) -> ()) -> () {
	let linear_thread_idx = @|| {
		(thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0)
	};

	let warp_idx = @|| {
		linear_thread_idx() / warp_size
	};

	let num_threads = @|| block_size(0) * block_size(1) * block_size(2);
	let num_warps = @|| (num_threads() + warp_size - 1) / warp_size;

	@body(group_context {
		idx = idx,
		waves = @|body|@|| cuda_subwarp(warp_idx, @|| gid() * num_warps() + warp_idx(), get_member_mask_u32(warp_size), warp_size, body),
		threads = @|body|@|| cuda_thread(thread_idx, @|| gid() * num_threads() + linear_thread_idx(), body),
		num_waves = num_warps,
		num_threads = block_size,
		barrier = cuda_block_sync,
		barrier_all = @|predicate| cuda_block_sync_all(cuda_pred(predicate)) != 0,
		barrier_any = @|predicate| cuda_block_sync_any(cuda_pred(predicate)) != 0,
		barrier_count = @|predicate| cuda_block_sync_count(cuda_pred(predicate))
	})
}

fn @cuda_launch(device: i32, (grid_dim_x: i32, grid_dim_y: i32, grid_dim_z: i32), (block_dim_x: i32, block_dim_y: i32, block_dim_z: i32), wrap_index: index_wrapper, wrap_dim: dim_wrapper, body: fn(grid_context) -> ()) -> () {
	// TODO: assert(warp_size == 32)
	let warp_size: u32 = 32;

	let block_size = wrap_dim(@|i: i32| {
		match i {
			0 => if ?block_dim_x { block_dim_x as u32 } else { cuda_blockDim_x() as u32 },
			1 => if ?block_dim_y { block_dim_y as u32 } else { cuda_blockDim_y() as u32 },
			2 => if ?block_dim_z { block_dim_z as u32 } else { cuda_blockDim_z() as u32 },
			_ => 1
		}
	});

	let num_blocks = wrap_dim(@|i: i32| {
		match i {
			0 => if ?grid_dim_x { grid_dim_x as u32 } else { cuda_gridDim_x() as u32 },
			1 => if ?grid_dim_y { grid_dim_y as u32 } else { cuda_gridDim_y() as u32 },
			2 => if ?grid_dim_y { grid_dim_z as u32 } else { cuda_gridDim_z() as u32 },
			_ => 1
		}
	});

	let num_threads_per_block = @|| block_size(0) * block_size(1) * block_size(2);

	let num_warps_per_block = @|| (num_threads_per_block() + warp_size - 1) / warp_size;

	let num_warps = @|| (num_blocks(0) * num_blocks(1) * num_blocks(2)) * num_warps_per_block();

	let num_threads = @|i: i32| num_blocks(i) * block_size(i);

	let block_idx = wrap_index(@|i: i32| {
		match i { 0 => cuda_blockIdx_x() as u32, 1 => cuda_blockIdx_y() as u32, 2 => cuda_blockIdx_z() as u32, _ => 0 }
	});

	let linear_block_idx = @|| (block_idx(2) * num_blocks(1) + block_idx(1)) * num_blocks(0) + block_idx(0);

	let thread_idx = wrap_index(@|i: i32| {
		match i { 0 => cuda_threadIdx_x() as u32, 1 => cuda_threadIdx_y() as u32, 2 => cuda_threadIdx_z() as u32, _ => 0 }
	});

	let global_thread_idx = @|i: i32| block_idx(i) * block_size(i) + thread_idx(i);

	let linear_thread_idx = @|| (thread_idx(2) * block_size(1) + thread_idx(1)) * block_size(0) + thread_idx(0);

	let global_linear_thread_idx = @|| linear_block_idx() * num_threads_per_block() + linear_thread_idx();

	let global_warp_idx = @|| linear_block_idx() * num_warps_per_block() + linear_thread_idx() / warp_size;

	cuda(device, (grid_dim_x * block_dim_x, grid_dim_y * block_dim_y, grid_dim_z * block_dim_z), (block_dim_x, block_dim_y, block_dim_z), @|| @body(grid_context {
		device = device,
		max_concurrency = @|| (num_blocks(0) * num_blocks(1) * num_blocks(2)) as i32 * num_threads_per_block() as i32,
		groups = @|body|@|| cuda_block(block_idx, linear_block_idx, thread_idx, block_size, warp_size, body),
		waves = @|body|@|| cuda_subwarp(global_warp_idx, global_warp_idx, get_member_mask_u32(warp_size), warp_size, body),
		threads = @|body|@|| cuda_thread(global_thread_idx, global_linear_thread_idx, body),
		num_groups = num_blocks,
		num_waves = num_warps,
		num_threads = num_threads
	}))
}
