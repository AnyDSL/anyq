static cpu_threads = 0;
//static vec_width = 8u;
static warp_size:u32 = 8;
static num_blocks_in_flight:i32 = 32;

fn @createAccDevice() = AccDevice {
	launch_1d = @|body|@|num_groups, group_size| {
		cpu_launch_1d(num_groups, group_size, body)
	},
	synchronize = @|| { },
	alloc = @|size| alloc_cpu(size),
	platform_device = runtime_device(0, 0),

	print_i32 = @|msg: &[u8], i: i32| {
		anyq_print_i32a(msg, i, 0);

		//let active_lanes = rv_ballot(rv_mask());
		//for j in unroll(0, vec_width as i32) {
		//	let lane_mask = (1 << j);
		//	if (active_lanes & lane_mask) == lane_mask {
		//		let i0 = rv_extract_i32(i, j);
		//		print_string(msg); print_i32(i0); print_char('\n');
		//	}
		//}
	},

	print_i32a = @|format: &[u8], args: &[i32]| {
		anyq_print_i32a(format, args(0), args(1));

		//let active_lanes = rv_ballot(rv_mask());
		//let num_active_lanes = cpu_popcount32(active_lanes);
		//print_string("num_active_lanes: "); print_i32(num_active_lanes); print_char('\n');
		//for j in unroll(0, vec_width as i32) {
		//	let lane_mask = (1 << j);
		//	if (active_lanes & lane_mask) == lane_mask {
		//		let arg0 = rv_extract_i32(args(0), j);
		//		let arg1 = rv_extract_i32(args(1), j);
		//		print_string(format); print_i32(arg0); print_char('\n'); print_i32(arg1); print_char('\n');
		//	}
		//}
	}
};


#[import(cc = "thorin", name = "fibers")] fn thorin_fibers(_num_threads: i32, _num_blocks: i32, _num_warps: i32, _body: fn(i32, i32) -> ()) -> ();
fn @fibers(body: fn(i32, i32) -> ()) = @|num_threads: i32, num_blocks: i32, num_warps: i32| thorin_fibers(num_threads, num_blocks, num_warps, body);

#[import(cc = "C", name = "anydsl_fibers_sync_block"              )] fn fibers_sync_block(i32) -> ();
#[import(cc = "C", name = "anydsl_fibers_sync_block_with_result"  )] fn fibers_sync_block_with_result(&mut i32, &mut i32, i32, i32) -> ();
#[import(cc = "C", name = "anydsl_fibers_yield"                   )] fn fibers_yield() -> ();
#[import(cc = "C")] fn anyq_print_i32a(&[u8], i32, i32) -> i32;


#[import(cc = "C", name = "rv_extract")] fn rv_extract_i32(i32, i32) -> i32;
#[import(cc = "C", name = "rv_extract")] fn rv_extract_u32(u32, i32) -> u32;
#[import(cc = "C", name = "rv_insert")] fn rv_insert_i32(i32, i32, i32) -> i32;
#[import(cc = "C", name = "rv_insert")] fn rv_insert_u32(u32, i32, u32) -> u32;
#[import(cc = "C", name = "rv_ballot")] fn rv_ballot_u32(bool) -> u32;

#[import(cc = "device", name = "llvm.ctpop.i32")] fn cpu_popcount_u32(u32) -> i32;


fn @shuffle_i32(self: u32, vec_width: i32) -> fn(i32, i32) -> i32 {
	@|value: i32, src_lane: i32| -> i32 {
		let mut result:i32 = self as i32; // self is needed to make result marked as varying
		for l in unroll(0, vec_width) {
			let lane_src_lane = rv_extract_i32(src_lane, l);
			let lane_value = rv_extract_i32(value, lane_src_lane);
			result = rv_insert_i32(result, l, lane_value);
		}
		result
	}
}

fn @shuffle_u32(self: u32, vec_width: i32) -> fn(u32, i32) -> u32 {
	@|value: u32, src_lane: i32| -> u32 {
		let mut result:u32 = self; // self is needed to make result marked as varying
		for l in unroll(0, vec_width) {
			let lane_src_lane = rv_extract_i32(src_lane, l);
			let lane_value = rv_extract_u32(value, lane_src_lane);
			result = rv_insert_u32(result, l, lane_value);
		}
		result
	}
}

fn @unroll_reduction_step(body: fn(i32) -> ()) {
	fn @(?width) loop(mask: i32, width: i32) -> () {
		if width > 0 {
			@body(mask);
			loop(mask << 1, width >> 1)
		}
	}
	loop
}

fn @atomic_red_i32(self: u32, vec_width: i32) -> fn(u32, &mut i32, i32, i32, fn(i32, i32)->i32) -> i32 {
	let shuffle = @|v: i32, l: u32| shuffle_i32(self, vec_width)(v, l as i32);

	@|op, ptr, val, nop, combine| {
		let active_lane = rv_ballot_u32(true) & (1 << self) != 0;
		let mut my_value = if active_lane { val } else { nop };

		for mask in unroll_reduction_step(0x1, vec_width) {
			let other_value = shuffle(my_value, self ^ mask as u32);
			my_value = combine(other_value, my_value);
		}

		let result0 = rv_extract_i32(my_value, 0);
		let result = atomic[i32](op, ptr, result0, 7, "");

		result
	}
}

fn @atomic_red_u32(self: u32, vec_width: i32) -> fn(u32, &mut u32, u32, u32, fn(u32, u32)->u32) -> u32 {
	let shuffle = @|v: u32, l: u32| shuffle_u32(self, vec_width)(v, l as i32);

	@|op, ptr, val, nop, combine| {
		let active_lane = rv_ballot_u32(true) & (1 << self) != 0;
		let mut my_value = if active_lane { val } else { nop };

		for mask in unroll_reduction_step(0x1, vec_width) {
			let other_value = shuffle(my_value, self ^ mask as u32);
			my_value = combine(other_value, my_value);
		}

		let result0 = rv_extract_u32(my_value, 0);
		let result = atomic[u32](op, ptr, result0, 7, "");

		result
	}
}

fn @atomic_op_i32(self: u32, vec_width: i32) -> fn(u32, &mut i32, i32, i32, fn(i32, i32)->i32, fn(i32, i32)->i32) -> i32 {
	@|op, ptr, val, nop, combine, extract| {
		let mut aggregate = 0;
		let mut intermediate:i32 = self as i32; // self is needed to make result marked as varying
		let mut old:i32;

		for l in unroll(0, vec_width) {
			let lane_val = if rv_ballot_u32(true) & (1 << l as u32) != 0 { rv_extract_i32(val, l) } else { nop };
			intermediate = rv_insert_i32(intermediate, l, aggregate);
			aggregate = combine(aggregate, lane_val);
		}

		old = atomic[i32](op, ptr, aggregate, 7, "");

		extract(old, intermediate)
	}
}

fn @atomic_op_u32(self: u32, vec_width: i32) -> fn(u32, &mut u32, u32, u32, fn(u32, u32)->u32, fn(u32, u32)->u32) -> u32 {
	@|op, ptr, val, nop, combine, extract| {
		let mut aggregate:u32 = 0;
		let mut intermediate:u32 = self; // self is needed to make result marked as varying
		let mut old:u32;

		for l in unroll(0, vec_width) {
			let lane_val = if rv_ballot_u32(true) & (1 << l as u32) != 0 { rv_extract_u32(val, l) } else { nop };
			intermediate = rv_insert_u32(intermediate, l, aggregate);
			aggregate = combine(aggregate, lane_val);
		}

		old = atomic[u32](op, ptr, aggregate, 7, "");

		extract(old, intermediate)
	}
}

fn @cpu_thread(tid: fn(i32) -> u32, num_threads: fn() -> u32, idx: Index, body: fn(gpu_thread_context) -> ()) -> () {
	let self = idx.local_id;
	let vec_width = num_threads() as i32;

	let _atomic_i32 = atomic_op_i32(self, vec_width);
	let _atomic_u32 = atomic_op_u32(self, vec_width);

	let _reduction_i32 = atomic_red_i32(self, vec_width);
	let _reduction_u32 = atomic_red_u32(self, vec_width);

	let _warp_shuffle_i32 = shuffle_i32(self, vec_width);
	let _warp_shuffle_u32 = shuffle_u32(self, vec_width);

	@body(gpu_thread_context {
		idx = tid,

		atomic_load_global_i32 = @|addr: &mut addrspace(1) i32| atomic_load[i32](bitcast[&mut i32](addr), 7, ""),
		atomic_load_global_u32 = @|addr: &mut addrspace(1) u32| atomic_load[u32](bitcast[&mut u32](addr), 7, ""),
		atomic_load_global_u64 = @|addr: &mut addrspace(1) u64| atomic_load[u64](bitcast[&mut u64](addr), 7, ""),

		atomic_store_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic_store[i32](bitcast[&mut i32](addr), val, 7, ""),
		atomic_store_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic_store[u32](bitcast[&mut u32](addr), val, 7, ""),
		atomic_store_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic_store[u64](bitcast[&mut u64](addr), val, 7, ""),

		atomic_add_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32](1, bitcast[&mut i32](addr), val, 7, ""),
		atomic_add_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](1, bitcast[&mut u32](addr), val, 7, ""),
		atomic_add_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](1, bitcast[&mut u64](addr), val, 7, ""),

		atomic_sub_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32](2, bitcast[&mut i32](addr), val, 7, ""),
		atomic_sub_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](2, bitcast[&mut u32](addr), val, 7, ""),
		atomic_sub_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](2, bitcast[&mut u64](addr), val, 7, ""),

		atomic_and_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32](3, bitcast[&mut i32](addr), val, 7, ""),
		atomic_and_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](3, bitcast[&mut u32](addr), val, 7, ""),
		atomic_and_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](3, bitcast[&mut u64](addr), val, 7, ""),

		atomic_or_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32](5, bitcast[&mut i32](addr), val, 7, ""),
		atomic_or_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](5, bitcast[&mut u32](addr), val, 7, ""),
		atomic_or_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](5, bitcast[&mut u64](addr), val, 7, ""),

		atomic_xor_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32](6, bitcast[&mut i32](addr), val, 7, ""),
		atomic_xor_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](6, bitcast[&mut u32](addr), val, 7, ""),
		atomic_xor_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](6, bitcast[&mut u64](addr), val, 7, ""),

		atomic_exch_global_i32 = @|addr: &mut addrspace(1)  i32, val: i32| -> i32 {
			/*
			let mut result:i32 = self as i32; // mark result as varying for rv
			let lane_mask = (vec_width as u32 - 1);
			if rv_ballot_u32(true) & lane_mask == lane_mask {
				// first shuffle all lanes of warp (left shift) and then atomic_xchg once
				result = warp_shuffle_i32(val, ((self + 1) & lane_mask) as i32);
				let new = rv_extract_i32(val, 0);
				let old = atomic(0, bitcast[&mut i32](addr), new, 7, "");
				result = rv_insert_i32(result, vec_width - 1, old);
			} else {
				for l in unroll(0, vec_width) {
					if rv_ballot_u32(true) & (1 << l as u32) != 0 {
						let lane_val = rv_extract_i32(val, l);
						let old = atomic(0, bitcast[&mut i32](addr), lane_val, 7, "");
						result = rv_insert_i32(result, l, old);
					}
				}
			}
			result
			*/
			/*
			let mut val_per_lane:i32 = self as i32; // mark new_per_lane as varying for rv
			for l in unroll(0, vec_width) {
				let lane_val = rv_extract_i32(val, l);
				val_per_lane = rv_insert_i32(val_per_lane, l, lane_val);
			}

			atomic[i32](0, bitcast[&mut i32](addr), val_per_lane, 7, "")
			*/
			atomic[i32](0, bitcast[&mut i32](addr), val, 7, "")
		},
		atomic_exch_global_u32 = @|addr: &mut addrspace(1)  u32, val: u32| -> u32 {
			/*
			let mut result:u32 = self; // mark result as varying for rv
			let lane_mask = (vec_width as u32 - 1);
			if rv_ballot_u32(true) & lane_mask == lane_mask {
				// first shuffle all lanes of warp (left shift) and then atomic_xchg once
				result = warp_shuffle_u32(val, ((self + 1) & lane_mask) as i32);
				let new = rv_extract_u32(val, 0);
				let old = atomic[u32](0, bitcast[&mut u32](addr), new, 7, "");
				result = rv_insert_u32(result, vec_width - 1, old);
			} else {
				for l in unroll(0, vec_width) {
					if rv_ballot_u32(true) & (1 << l as u32) != 0 {
						let lane_val = rv_extract_u32(val, l);
						let old = atomic[u32](0, bitcast[&mut u32](addr), lane_val, 7, "");
						result = rv_insert_u32(result, l, old);
					}
				}
			}
			result
			*/
			/*
			let mut val_per_lane:u32 = self; // mark val_per_lane as varying for rv
			for l in unroll(0, vec_width) {
				let lane_val = rv_extract_u32(val, l);
				val_per_lane = rv_insert_u32(val_per_lane, l, lane_val);
			}

			atomic[u32](0, bitcast[&mut u32](addr), val_per_lane, 7, "")
			*/
			atomic[u32](0, bitcast[&mut u32](addr), val, 7, "")
		},
		atomic_exch_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](0, bitcast[&mut u64](addr), val, 7, ""),

		atomic_min_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32]( 8, bitcast[&mut i32](addr), val, 7, ""),
		atomic_min_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32](10, bitcast[&mut u32](addr), val, 7, ""),
		atomic_min_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64](10, bitcast[&mut u64](addr), val, 7, ""),

		atomic_max_global_i32 = @|addr: &mut addrspace(1) i32, val: i32| atomic[i32]( 7, bitcast[&mut i32](addr), val, 7, ""),
		atomic_max_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| atomic[u32]( 9, bitcast[&mut u32](addr), val, 7, ""),
		atomic_max_global_u64 = @|addr: &mut addrspace(1) u64, val: u64| atomic[u64]( 9, bitcast[&mut u64](addr), val, 7, ""),

		// atomic_cas_global_u16: cuda_atomic_cas_global_u16,
		atomic_cas_global_i32 = @|addr: &mut addrspace(1) i32, cmp: i32, new: i32| -> i32 {
			/*
			let mut result:i32 = self as i32; // mark result as varying for rv
			for l in unroll(0, vec_width) {
				if rv_ballot_u32(true) & (1 << l as u32) != 0 {
					let lane_cmp = rv_extract_i32(cmp, l);
					let lane_new = rv_extract_i32(new, l);
					let old = cmpxchg[i32](bitcast[&mut i32](addr), lane_cmp, lane_new, 7, "");
					result = rv_insert_i32(result, l, old(0));
				}
			}
			result
			*/
			/*
			let mut new_per_lane:i32 = self as i32; // mark new_per_lane as varying for rv
			for l in unroll(0, vec_width) {
				let lane_new = rv_extract_i32(new, l);
				new_per_lane = rv_insert_i32(new_per_lane, l, lane_new);
			}

			match cmpxchg[i32](bitcast[&mut i32](addr), cmp, new_per_lane, 7, "") { (old, _success) => old }
			*/
			match cmpxchg[i32](bitcast[&mut i32](addr), cmp, new, 7, "") { (old, _success) => old }
		},
		atomic_cas_global_u32 = @|addr: &mut addrspace(1) u32, cmp: u32, new: u32| -> u32 {
			/*
			let mut result:u32 = self; // mark result as varying for rv
			for l in unroll(0, vec_width) {
				if rv_ballot_u32(true) & (1 << l as u32) != 0 {
					let lane_cmp = rv_extract_u32(cmp, l);
					let lane_new = rv_extract_u32(new, l);
					let old = cmpxchg[i32](bitcast[&mut u32](addr), lane_cmp, lane_new, 7, "");
					result = rv_insert_u32(result, l, old(0));
				}
			}
			result
			*/
			/*
			let mut new_per_lane:u32 = self; // mark new_per_lane as varying for rv
			for l in unroll(0, vec_width) {
				let lane_new = rv_extract_u32(new, l);
				new_per_lane = rv_insert_u32(new_per_lane, l, lane_new);
			}

			match cmpxchg[u32](bitcast[&mut u32](addr), cmp, new_per_lane, 7, "") { (old, _success) => old }
			*/
			match cmpxchg[u32](bitcast[&mut u32](addr), cmp, new, 7, "") { (old, _success) => old }
		},
		atomic_cas_global_i64 = @|addr: &mut addrspace(1) u64, cmp: u64, new: u64| match cmpxchg[u64](bitcast[&mut u64](addr), cmp, new, 7, "") { (old, _success) => old },
		atomic_inc_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| -> u32 {
			let ptr = bitcast[&mut u32](addr);
			let active_lanes = rv_ballot_u32(true);
			let _num_active_lanes = cpu_popcount_u32(active_lanes) as u32;

			let _wrap = val + 1;

			let mut result:u32 = self; // mark result as varying for rv
			//let mut active_lane:u32 = 0;

			// this is the generic version
			/*
			let mut old:u32 = atomic_load[u32](ptr, 7, "");
			let mut cmp:bool = false;

			while(!cmp) {
				let new = if old >= val { num_active_lanes - 1 } else { old + num_active_lanes } % wrap;
				let (res_old, res_cmp) = cmpxchg[u32](ptr, old, new, 7, "");
				old = res_old;
				cmp = res_cmp;
			}

			for l in unroll(0, vec_width) {
				if active_lanes & (1 << l as u32) != 0 {
					result = rv_insert_u32(result, l, (old + active_lane++) % wrap);
				}
			}
			*/

			// this is optimized for wrap being 2**n
			/*
			// atomic_add
			let old:u32 = atomic[u32](1, ptr, num_active_lanes, 7, "");
			if old >= val {
				// atomic_and to mask overflow bits
				// however, we must ensure that addr will not be read outside of this function
				atomic[u32](3, ptr, val, 7, "");
			}

			for l in unroll(0, vec_width) {
				if active_lanes & (1 << l as u32) != 0 {
					result = rv_insert_u32(result, l, (old + active_lane++) & val);
				}
			}
			*/
			let old:u32 = atomic[u32](1, ptr, 1, 7, "");
			if old >= val {
				// atomic_and to mask overflow bits
				// however, we must ensure that addr will not be read outside of this function
				atomic[u32](3, ptr, val, 7, "");
			}
			result = old & val;

			result
		},
		atomic_dec_global_u32 = @|addr: &mut addrspace(1) u32, val: u32| -> u32 {
			let ptr = bitcast[&mut u32](addr);

			// this is the generic version
			let mut old:u32 = atomic_load[u32](ptr, 7, "");
			let mut cmp:bool = false;
			while(!cmp) {
				let new = if (old == 0) || (old > val) { val } else { old - 1 };
				let (res_old, res_cmp) = cmpxchg[u32](ptr, old, new, 7, "");
				old = res_old;
				cmp = res_cmp;
			}

			// this is optimized for val being 2**n-1
			/*
			// atomic sub
			let old:u32 = atomic[u32](2, ptr, 1, 7, "");
			if old == 0 {
				// atomic and to mask overflow bits
				// however, we must ensure that addr will not be read outside of this function
				atomic[u32](3, ptr, val, 7, "");
			}
			*/

			old
		},

		memory_barrier = @|| {},  // TODO

		yield = @|| { if idx.local_id == 0 { fibers_yield(); } }
	});
}

fn @cpu_subwarp(membermask: u64, num_threads: fn() -> u32, idx: Index, body: fn(gpu_wave_context) -> ()) -> () {

	let self = idx.local_id;
	let vec_width = num_threads() as i32;

	let warp_shuffle_i32 = shuffle_i32(self, vec_width);
	let warp_shuffle_u32 = shuffle_u32(self, vec_width);

	fn @lanemask_eq() -> u64 {
		1 << self as u64
	}

	fn @lanemask_lt() -> u64 {
		let ballot = rv_ballot_u32(true) as u64;
		let mask = (1 << self as u64) - 1;
		mask & ballot
	}

	fn @lanemask_gt() -> u64 {
		if (vec_width == 64 && self == 63) || (vec_width == 32 && self == 31) {
			0
		} else {
			let ballot = rv_ballot_u32(true) as u64;
			let mask = (!0:u64) << (self as u64 + 1);
			mask & ballot
		}
	}

	@body(gpu_wave_context{
		idx = @|| idx.warp_id,

		membermask = @|| membermask,

		// TODO: return simd_lane
		threads = @|body| @|| cpu_thread(@|_i:i32| self, num_threads, idx, body),

		num_threads = num_threads,

		barrier = @|| { },
		barrier_all = @|predicate| rv_all(predicate),
		barrier_any = @|predicate| rv_any(predicate),
		barrier_count = @|predicate| cpu_popcount_u32(rv_ballot_u32(predicate)),
		barrier_vote = @|predicate| rv_ballot_u32(predicate) as u64,

		/* TODO: assert width <= num_threads == warp_size */
		/* TODO: assert width 2^n */
		shfl_i32 = @|x:i32, src_lane:i32, _width:u32| warp_shuffle_i32(x, src_lane),
		shfl_u32 = @|x:u32, src_lane:i32, _width:u32| warp_shuffle_u32(x, src_lane),
		//shfl_f32 = @|x:f32, src_lane:i32, width:u32| undef[f32](),

		//shfl_up_i32 = @|x:i32, delta:u32, width:u32| rv_shuffle_i32(x, -(delta as i32)),
		//shfl_up_u32 = @|x:u32, delta:u32, width:u32| rv_shuffle_u32(x, -(delta as i32)),
		shfl_up_i32 = @|x:i32, delta:u32, width:u32| warp_shuffle_i32(x, if (self & (width - 1)) < delta { self } else { self - delta } as i32),
		shfl_up_u32 = @|x:u32, delta:u32, width:u32| warp_shuffle_u32(x, if (self & (width - 1)) < delta { self } else { self - delta } as i32),
		//shfl_up_f32 = @|x:f32, delta:u32, width:u32| undef[f32](),

		shfl_down_i32 = @|x:i32, delta:u32, width:u32| warp_shuffle_i32(x, if (self & (width - 1)) + delta >= width { self } else { self + delta } as i32),
		shfl_down_u32 = @|x:u32, delta:u32, width:u32| warp_shuffle_u32(x, if (self & (width - 1)) + delta >= width { self } else { self + delta } as i32),
		//shfl_down_f32 = @|x:f32, delta:u32, width:u32| undef[f32](),

		shfl_bfly_i32 = @|x:i32, lane_mask:i32, width:u32| warp_shuffle_i32(x, (((self ^ lane_mask as u32) & (width - 1)) | (self & !(width - 1))) as i32),
		shfl_bfly_u32 = @|x:u32, lane_mask:i32, width:u32| warp_shuffle_u32(x, (((self ^ lane_mask as u32) & (width - 1)) | (self & !(width - 1))) as i32),
		//shfl_bfly_f32 = @|x:f32, lane_mask:i32, width:u32| undef[f32](),

		lanemask =    @|| lanemask_eq(),
		lanemask_le = @|| lanemask_lt() | lanemask_eq(),
		lanemask_lt = @|| lanemask_lt(),
		lanemask_ge = @|| lanemask_gt() | lanemask_eq(),
		lanemask_gt = @|| lanemask_gt()
	});
}

fn @cpu_block(logical_block_id: u32, num_warps: fn() -> u32, num_threads: fn() -> u32, idx: Index, addr: fn(i32) -> (&mut i32), body: fn(gpu_group_context) -> ()) -> () {
	@body(gpu_group_context {
		// TODO: dispatch ndim-block_id
		idx = @|_i:i32| logical_block_id,
		waves = @|body| @|| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, idx, body),
		threads = @|body| @|| cpu_thread(@|_i:i32| idx.warp_id * warp_size + idx.local_id, @|| warp_size, idx, body),
		num_waves = num_warps,
		// TODO: dispatch ndim-thread_id
		num_threads = @|_i:i32| num_threads(),
		barrier = @|| -> () { if idx.local_id == 0 { fibers_sync_block(idx.block_id as i32); } },
		barrier_all = @|predicate:bool| -> bool {
			let warp_pred = if rv_all(predicate) { 1 } else {0 };
			if idx.local_id == 0 {
				// atomic_and_global_i32
				atomic[i32](3, addr(1), warp_pred, 7, "");
				fibers_sync_block_with_result(addr(0), addr(1), 1, idx.block_id as i32);
			}
			let result = *(addr(0));
			result != 0
		},
		barrier_any = @|predicate:bool| -> bool {
			let warp_pred = if rv_any(predicate) { 1 } else {0 };
			if idx.local_id == 0 {
				// atomic_or_global_i32
				atomic[i32](5, addr(2), warp_pred, 7, "");
				fibers_sync_block_with_result(addr(0), addr(2), 0, idx.block_id as i32);
			}
			let result = *(addr(0));
			result != 0
		},
		barrier_count = @|predicate:bool| -> i32 {
			let warp_count = cpu_popcount_u32(rv_ballot_u32(predicate));
			if idx.local_id == 0 {
				// atomic_add_global_i32
				atomic[i32](1, addr(3), warp_count, 7, "");
				fibers_sync_block_with_result(addr(0), addr(3), 0, idx.block_id as i32);
			}
			let result = *(addr(0));
			result
		},
	});
}

struct Index {
	block_id: u32,
	warp_id:  u32,
	local_id: u32
}

fn @cpu_launch_1d(grid_dim: i32, block_dim: i32, body: fn(gpu_grid_context) -> ()) -> () {
	// TODO: assert(warp_size == 32)

	let num_threads_per_block = block_dim as u32;
	let num_warps_per_block = (num_threads_per_block + warp_size - 1) / warp_size;
	let num_blocks = grid_dim as u32;
	let num_warps = num_blocks * num_warps_per_block;
	let num_threads = num_blocks * num_threads_per_block;
	let num_blocks_in_flight = min(grid_dim, num_blocks_in_flight);

	// TODO: assert(warp_size/l == vec_width)
	fn @warp_loop(body: fn(i32)->()) = @|l:u32| vectorize(l as i32, body);
	//let warp_loop = @|l:u32, body: fn(u32)->()| range(0, l as i32, @|i:i32| body(i as u32));

	// we need a buffer to accumulate block level results of barriers
	let num_block_variables = 4; // output, all, any, count
	let block_pred_buffer = alloc(0, num_block_variables as i64 * num_blocks_in_flight as i64 * sizeof[i32]());
	let addr = bitcast[&mut[i32]](block_pred_buffer.data);

	// reset all variables
	for i in range(0, num_blocks_in_flight) {
		addr(num_block_variables*i + 1) = 1;
		addr(num_block_variables*i + 2) = 0;
		addr(num_block_variables*i + 3) = 0;
	}

	for block_id, warp_id in fibers(cpu_threads, num_blocks_in_flight, num_warps_per_block as i32) {
		for logical_block_id in range_step(block_id, num_blocks as i32, num_blocks_in_flight) {

			for local_id in warp_loop(warp_size) {
				let idx = Index {
					block_id = block_id as u32,
					 warp_id =  warp_id as u32,
					local_id = local_id as u32,
				};

				let context = gpu_grid_context {
					device = 0,
					groups = @|body| @|| cpu_block(logical_block_id as u32, @|| num_warps_per_block, @|| num_threads_per_block, idx, @|x| { &mut(addr(num_block_variables*block_id + x)) }, body),
					waves = @|body| @|| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, Index { block_id = logical_block_id as u32, warp_id = logical_block_id as u32 * num_warps_per_block + idx.warp_id, local_id = idx.local_id }, body),
					threads = @|body| @|| cpu_thread(@|_i:i32| logical_block_id as u32 * num_threads_per_block + idx.warp_id * warp_size + idx.local_id, @|| warp_size, idx, body),
					num_groups = @|_i:i32| num_blocks,
					num_waves = @|| num_warps,
					num_threads = @|_i:i32| num_threads
				};
				@body(context)
			}

			// TODO: do we need a sync or yield here to ensure proper finalization of one block?
			//       otherwise fibers may work on different blocks - does this cause issues?
			//fibers_yield();
			fibers_sync_block(block_id);

		}
	}

	release(block_pred_buffer);
}
