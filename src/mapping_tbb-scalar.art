static cpu_threads = 0;
static warp_size:u32 = 1;

fn @createAccDevice(device: i32) = AccDevice {
	its = true,
	launch_1d = @|body|@|num_groups, group_size| {
		scalar_launch_1d(num_groups, group_size, body)
	},
	synchronize = @|| { },
	alloc = @|size| alloc_cpu(size),
	platform_device = runtime_device(0, device),

	platform_name = "tbb-scalar",

	print_i32 = @|_msg: &[u8], _i: i32| {
		//anyq_print_3xi32(msg, i, 0, 0);
	},

	print_2xi32 = @|_format: &[u8], _a1: i32, _a2: i32| {
		//anyq_print_3xi32(format, a1, a2, 0);
	},

	print_3xi32 = @|_format: &[u8], _a1: i32, _a2: i32, _a3: i32| {
		//anyq_print_3xi32(format, a1, a2, a3);
	}
};

fn @createDefaultAccDevice() = createAccDevice(0);


//#[import(cc = "thorin", name = "fibers")] fn thorin_fibers(_num_threads: i32, _num_blocks: i32, _num_warps: i32, _body: fn(i32, i32) -> ()) -> ();
//fn @fibers(body: fn(i32, i32) -> ()) = @|num_threads: i32, num_blocks: i32, num_warps: i32| thorin_fibers(num_threads, num_blocks, num_warps, body);

//#[import(cc = "C", name = "anydsl_fibers_sync_block"              )] fn fibers_sync_block(i32) -> ();
//#[import(cc = "C", name = "anydsl_fibers_sync_block_with_result"  )] fn fibers_sync_block_with_result(&mut i32, &mut i32, i32, i32) -> ();
//#[import(cc = "C", name = "anydsl_fibers_yield"                   )] fn fibers_yield(&[u8]) -> ();
fn fibers_yield(&[u8]) -> () { /* noop */ }
//#[import(cc = "C")] fn anyq_print_3xi32(&[u8], i32, i32, i32) -> i32;

#[import(cc = "device", name = "llvm.ctpop.i32")] fn cpu_popcount_u32(u32) -> i32;


fn @cpu_atomic_load[T](location: &addrspace(1) T, order: memory_order)
	= atomic_load[T](location as &T, builtin_memory_order(order), "");

fn @cpu_atomic_store[T](location: &mut addrspace(1) T, value: T, order: memory_order)
	= atomic_store[T](location as &mut T, value, builtin_memory_order(order), "");

fn @cpu_atomic_rmw[T](op: u32) {
	@|location:&mut addrspace(1) T, value:T, order:memory_order| -> T {
		atomic[T](op, location as &mut T, value, builtin_memory_order(order), "")
	}
}

fn @cpu_atomic_cas[T](location: &mut addrspace(1) T, expected: T, desired: T, memory_order_succ: memory_order, memory_order_fail: memory_order)
	= cmpxchg[T](location as &mut T, expected, desired, builtin_memory_order(memory_order_succ), builtin_memory_order(memory_order_fail), "");

fn @cpu_atomic_cas_weak[T](location: &mut addrspace(1) T, expected: T, desired: T, memory_order_succ: memory_order, memory_order_fail: memory_order)
	= cmpxchg_weak[T](location as &mut T, expected, desired, builtin_memory_order(memory_order_succ), builtin_memory_order(memory_order_fail), "");

// fn @cpu_atomic_wait_and_transition[T](location:&mut addrspace(1) T, expected:T, desired:T, order:memory_order, _debug_msg:&[u8]) -> () {
// 	while !cpu_atomic_cas_weak[T](location, expected, desired, order, memory_order::relaxed).1 {
// 		// fibers_yield(debug_msg);
// 	}
// }

fn @cpu_thread(uid: fn() -> i32, tid: fn(i32) -> u32, _num_threads: fn() -> u32, gid_offset: u32, idx: Index, body: fn(thread_context) -> ()) -> () {

	let self = idx.local_id;

	@body(thread_context {
		idx = tid,

		gid = @|| gid_offset + self,
		uid = uid,

		atomic_load_global_i32 = cpu_atomic_load[i32],
		atomic_load_global_u32 = cpu_atomic_load[u32],
		atomic_load_global_i64 = cpu_atomic_load[i64],
		atomic_load_global_u64 = cpu_atomic_load[u64],

		atomic_load_global_i32_coalesced = cpu_atomic_load[i32],
		atomic_load_global_u32_coalesced = cpu_atomic_load[u32],
		atomic_load_global_i64_coalesced = cpu_atomic_load[i64],
		atomic_load_global_u64_coalesced = cpu_atomic_load[u64],

		atomic_store_global_i32 = cpu_atomic_store[i32],
		atomic_store_global_u32 = cpu_atomic_store[u32],
		atomic_store_global_i64 = cpu_atomic_store[i64],
		atomic_store_global_u64 = cpu_atomic_store[u64],

		atomic_store_global_i32_coalesced = cpu_atomic_store[i32],
		atomic_store_global_u32_coalesced = cpu_atomic_store[u32],
		atomic_store_global_i64_coalesced = cpu_atomic_store[i64],
		atomic_store_global_u64_coalesced = cpu_atomic_store[u64],

		atomic_add_global_i32 = cpu_atomic_rmw[i32](1),
		atomic_add_global_u32 = cpu_atomic_rmw[u32](1),
		atomic_add_global_i64 = cpu_atomic_rmw[i64](1),
		atomic_add_global_u64 = cpu_atomic_rmw[u64](1),

		atomic_sub_global_i32 = cpu_atomic_rmw[i32](2),
		atomic_sub_global_u32 = cpu_atomic_rmw[u32](2),
		atomic_sub_global_u64 = cpu_atomic_rmw[u64](2),

		atomic_and_global_i32 = cpu_atomic_rmw[i32](3),
		atomic_and_global_u32 = cpu_atomic_rmw[u32](3),
		atomic_and_global_u64 = cpu_atomic_rmw[u64](3),

		atomic_or_global_i32 = cpu_atomic_rmw[i32](5),
		atomic_or_global_u32 = cpu_atomic_rmw[u32](5),
		atomic_or_global_u64 = cpu_atomic_rmw[u64](5),

		atomic_xor_global_i32 = cpu_atomic_rmw[i32](6),
		atomic_xor_global_u32 = cpu_atomic_rmw[u32](6),
		atomic_xor_global_u64 = cpu_atomic_rmw[u64](6),

		atomic_exch_global_i32 = cpu_atomic_rmw[i32](0),
		atomic_exch_global_u32 = cpu_atomic_rmw[u32](0),
		atomic_exch_global_u64 = cpu_atomic_rmw[u64](0),

		atomic_min_global_i32 = cpu_atomic_rmw[i32]( 8),
		atomic_min_global_u32 = cpu_atomic_rmw[u32](10),
		atomic_min_global_u64 = cpu_atomic_rmw[u64](10),

		atomic_max_global_i32 = cpu_atomic_rmw[i32](7),
		atomic_max_global_u32 = cpu_atomic_rmw[u32](9),
		atomic_max_global_u64 = cpu_atomic_rmw[u64](9),

		atomic_cas_global_i32 = cpu_atomic_cas[i32],
		atomic_cas_global_u32 = cpu_atomic_cas[u32],
		atomic_cas_global_i64 = cpu_atomic_cas[i64],
		atomic_cas_global_u64 = cpu_atomic_cas[u64],
		atomic_cas_global_i32_weak = cpu_atomic_cas_weak[i32],
		atomic_cas_global_u32_weak = cpu_atomic_cas_weak[u32],
		atomic_cas_global_i64_weak = cpu_atomic_cas_weak[i64],
		atomic_cas_global_u64_weak = cpu_atomic_cas_weak[u64],

		atomic_inc_global_u32 = @|addr, val| {
			assert(cpu_popcount_u32(val) == 1, "atomic_inc requires power of two range");
			let atomic_add = cpu_atomic_rmw[u32](1);
			atomic_add(addr, 1, memory_order::relaxed) % (val + 1)
			// let mut old = cpu_atomic_load[u32](addr, memory_order::relaxed);
			// while true {
			// 	let (curr, succ) = cpu_atomic_cas_weak[u32](addr, old, if old >= val { 0 } else { old + 1 }, memory_order::relaxed, memory_order::relaxed);

			// 	if succ { break() }

			// 	old = curr;
			// }

			// old
		},

		memory_barrier = @|order| { fence(builtin_memory_order(order), ""); },

		timestamp = get_nano_time,
		timestamp32 = @|| get_nano_time() as i32,

		wait = @|f, debug_msg| {
			let mut retry = 50;
			while !f() {
				retry -= 1;
				if retry <= 0 { fibers_yield(debug_msg); }
			}
		}
	});
}

fn @cpu_subwarp(membermask: u64, num_threads: fn() -> u32, gid_offset: u32, uid: fn() -> i32, idx: Index, body: fn(wave_context) -> ()) -> () {

	let self = idx.local_id;

	@body(wave_context{
		idx = @|| idx.warp_id,

		membermask = @|| membermask,

		// TODO: return simd_lane
		threads = @|body| @|| cpu_thread(uid, @|_i:i32| self, num_threads, gid_offset + idx.warp_id * num_threads(), idx, body),

		num_threads = num_threads,

		barrier = @|| { },
		barrier_all = @|predicate:bool| -> bool { predicate },
		barrier_any = @|predicate:bool| -> bool { predicate },
		barrier_count = @|predicate:bool| -> i32 { if predicate { 1 } else { 0 } },
		barrier_vote = @|predicate:bool| -> u64 { if predicate { 1 } else { 0 } },

		/* TODO: assert width <= num_threads == warp_size */
		/* TODO: assert width 2^n */
		shfl_i32 = @|_x:i32, _src_lane:i32, _width:u32| { assert(false, "no warp communication supported"); undef[i32]() },
		shfl_u32 = @|_x:u32, _src_lane:i32, _width:u32| { assert(false, "no warp communication supported"); undef[u32]() },
		//shfl_f32 = @|x:f32, src_lane:i32, width:u32| undef[f32](),

		//shfl_up_i32 = @|x:i32, delta:u32, width:u32| rv_shuffle_i32(x, -(delta as i32)),
		//shfl_up_u32 = @|x:u32, delta:u32, width:u32| rv_shuffle_u32(x, -(delta as i32)),
		shfl_up_i32 = @|_x:i32, _delta:u32, _width:u32| { assert(false, "no warp communication supported"); undef[i32]() },
		shfl_up_u32 = @|_x:u32, _delta:u32, _width:u32| { assert(false, "no warp communication supported"); undef[u32]() },
		//shfl_up_f32 = @|x:f32, delta:u32, width:u32| undef[f32](),

		shfl_down_i32 = @|_x:i32, _delta:u32, _width:u32| { assert(false, "no warp communication supported"); undef[i32]() },
		shfl_down_u32 = @|_x:u32, _delta:u32, _width:u32| { assert(false, "no warp communication supported"); undef[u32]() },
		//shfl_down_f32 = @|x:f32, delta:u32, width:u32| undef[f32](),

		shfl_bfly_i32 = @|_x:i32, _lane_mask:i32, _width:u32| { assert(false, "no warp communication supported"); undef[i32]() },
		shfl_bfly_u32 = @|_x:u32, _lane_mask:i32, _width:u32| { assert(false, "no warp communication supported"); undef[u32]() },
		//shfl_bfly_f32 = @|x:f32, lane_mask:i32, width:u32| undef[f32](),

		lanemask =    @|| 1,
		lanemask_le = @|| 1,
		lanemask_lt = @|| 0,
		lanemask_ge = @|| 1,
		lanemask_gt = @|| 0
	});
}

fn @cpu_block(logical_block_id: u32, num_warps: fn() -> u32, num_threads: fn() -> u32, gid_offset: u32, idx: Index, _addr: fn(i32) -> (&mut i32), body: fn(group_context) -> ()) -> () {
	@body(group_context {
		// TODO: dispatch ndim-block_id
		idx = @|_i:i32| logical_block_id,
		waves = @|body| @|| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, gid_offset + logical_block_id * num_threads(), @|| (idx.block_id * num_threads() + idx.warp_id * warp_size + idx.local_id) as i32, idx, body),
		threads = @|body| @|| cpu_thread(@|| (idx.block_id * num_threads() + idx.warp_id * warp_size + idx.local_id) as i32, @|_i:i32| idx.warp_id * warp_size + idx.local_id, @|| warp_size, gid_offset + logical_block_id * num_threads() + idx.warp_id * warp_size, idx, body),
		num_waves = num_warps,
		// TODO: dispatch ndim-thread_id
		num_threads = @|_i:i32| num_threads(),
		barrier = @|| -> () { assert(false, "no block communication supported"); },
		barrier_all = @|_predicate:bool| -> bool { assert(false, "no block communication supported"); undef[bool]() },
		barrier_any = @|_predicate:bool| -> bool { assert(false, "no block communication supported"); undef[bool]() },
		barrier_count = @|_predicate:bool| -> i32 { assert(false, "no block communication supported"); undef[i32]() },
	});
}

struct Index {
	block_id: u32,
	warp_id:  u32,
	local_id: u32
}

fn @scalar_launch_1d(grid_dim: i32, block_dim: i32, body: fn(grid_context) -> ()) -> () {
	// TODO: assert(warp_size == 32)

	let num_threads_per_block = block_dim as u32;
	let num_warps_per_block = (num_threads_per_block + warp_size - 1) / warp_size;
	let num_blocks = grid_dim as u32;
	let num_warps = num_blocks * num_warps_per_block;
	let num_threads = num_blocks * num_threads_per_block;

	//print_string("launch config (scalar):\n");
	//print_string("  num_blocks: "); print_u32(num_blocks); print_char('\n');
	//print_string("  num_warps_per_block: "); print_u32(num_warps_per_block); print_char('\n');
	//print_string("  num_threads_per_block: "); print_u32(num_threads_per_block); print_char('\n');
	//print_string("  num_blocks_in_flight: "); print_i32(num_blocks_in_flight); print_char('\n');

	fn @warp_loop(body: fn(i32)->()) = @|_:u32| body(0);

	//let addr:&mut[i32] { assert(false, "no block communication supported"); undef[i32]() }

	for warp_id in parallel(cpu_threads, 0, num_warps as i32) {

		let logical_block_id = warp_id as u32 / num_warps_per_block;
		let logical_warp_id = warp_id as u32 % num_warps_per_block;

		for local_id in warp_loop(warp_size) {
			let idx = Index {
				block_id = logical_block_id,
				warp_id = logical_warp_id,
				local_id = local_id as u32,
			};

			let context = grid_context {
				device = 0,
				max_concurrency = @|| -1, // TODO
				groups = @|body| @|| cpu_block(logical_block_id, @|| num_warps_per_block, @|| num_threads_per_block, 0, idx, @|_x| { assert(false, "no block communication supported"); undef[&mut i32]() }, body),
				waves = @|body| @|| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, 0, @|| ((idx.block_id * num_warps_per_block + idx.warp_id) * warp_size + idx.local_id) as i32, Index { block_id = logical_block_id, warp_id = logical_block_id * num_warps_per_block + idx.warp_id, local_id = idx.local_id }, body),
				threads = @|body| @|| cpu_thread(@|| ((idx.block_id * num_warps_per_block + idx.warp_id) * warp_size + idx.local_id) as i32, @|_i:i32| logical_block_id * num_threads_per_block + idx.warp_id * warp_size + idx.local_id, @|| warp_size, logical_block_id * num_threads_per_block + idx.warp_id * warp_size, idx, body),
				num_groups = @|_i:i32| num_blocks,
				num_waves = @|| num_warps,
				num_threads = @|_i:i32| num_threads
			};
			@body(context)
		}

	}

}
