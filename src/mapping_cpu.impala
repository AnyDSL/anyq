
static cpu_threads = 0;
static vec_width = 8u;
static warp_size = 8u;

fn @createAccDevice() -> AccDevice {
	AccDevice {
		launch_1d: @|num_groups, group_size, body| {
			cpu_launch_1d(num_groups, group_size, body)
		},
		synchronize: @|| { },
		alloc: @|size| alloc_cpu(size),
		platform_device: runtime_device(0, 0),
		print_i32: @|msg: &[u8], i: i32| { print_string(msg); print_i32(i); print_char('\n'); },
		print_i32a: @|format: &[u8], args: &[i32]| { print_string(format); print_i32(args(0)); print_char('\n'); /* printf(format, bitcast[&[u8]](&args(0))); */ }
	}
}



extern "thorin" {
	fn fibers(num_threads: i32, num_blocks: i32, num_warps: i32, body: fn(i32, i32) -> ()) -> ();
}

extern "C" {
	fn "anydsl_fibers_sync_block"               fibers_sync_block(i32) -> ();
	fn "anydsl_fibers_sync_block_with_result"   fibers_sync_block_with_result(&mut i32, &mut i32, i32, i32) -> ();
	fn "anydsl_fibers_yield"                    fibers_yield() -> ();
}


extern "C" {
	//fn "rv_shuffle" rv_shuffle_f32(f32, i32) -> f32;
	fn "rv_shuffle" rv_shuffle_i32(i32, i32) -> i32;
	fn "rv_shuffle" rv_shuffle_u32(u32, i32) -> u32;
	//fn "rv_extract" rv_extract_f32(f32, i32) -> f32;
	fn "rv_extract" rv_extract_i32(i32, i32) -> i32;
	fn "rv_extract" rv_extract_u32(u32, i32) -> u32;
}

fn @cpu_thread(idx: fn (i32) -> u32, body: fn (gpu_thread_context) -> ()) -> () {
	@@body(gpu_thread_context {
		idx: idx,

		atomic_load_global_i32: @|addr: &mut[1]i32| atomic_load(bitcast[&mut i32](addr), 7u, ""),
		atomic_load_global_u32: @|addr: &mut[1]u32| atomic_load(bitcast[&mut u32](addr), 7u, ""),
		atomic_load_global_u64: @|addr: &mut[1]u64| atomic_load(bitcast[&mut u64](addr), 7u, ""),

		atomic_store_global_i32: @|addr: &mut[1]i32, val: i32| atomic_store(bitcast[&mut i32](addr), val, 7u, ""),
		atomic_store_global_u32: @|addr: &mut[1]u32, val: u32| atomic_store(bitcast[&mut u32](addr), val, 7u, ""),
		atomic_store_global_u64: @|addr: &mut[1]u64, val: u64| atomic_store(bitcast[&mut u64](addr), val, 7u, ""),

		atomic_add_global_i32: @|addr: &mut[1] i32, val: i32| atomic(1u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_add_global_u32: @|addr: &mut[1] u32, val: u32| atomic(1u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_add_global_u64: @|addr: &mut[1] u64, val: u64| atomic(1u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_sub_global_i32: @|addr: &mut[1] i32, val: i32| atomic(2u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_sub_global_u32: @|addr: &mut[1] u32, val: u32| atomic(2u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_sub_global_u64: @|addr: &mut[1] u64, val: u64| atomic(2u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_and_global_i32: @|addr: &mut[1] i32, val: i32| atomic(3u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_and_global_u32: @|addr: &mut[1] u32, val: u32| atomic(3u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_and_global_u64: @|addr: &mut[1] u64, val: u64| atomic(3u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_or_global_i32: @|addr: &mut[1] i32, val: i32| atomic(5u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_or_global_u32: @|addr: &mut[1] u32, val: u32| atomic(5u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_or_global_u64: @|addr: &mut[1] u64, val: u64| atomic(5u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_xor_global_i32: @|addr: &mut[1] i32, val: i32| atomic(6u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_xor_global_u32: @|addr: &mut[1] u32, val: u32| atomic(6u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_xor_global_u64: @|addr: &mut[1] u64, val: u64| atomic(6u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_exch_global_i32: @|addr: &mut[1] i32, val: i32| atomic(0u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_exch_global_u32: @|addr: &mut[1] u32, val: u32| atomic(0u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_exch_global_u64: @|addr: &mut[1] u64, val: u64| atomic(0u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_min_global_i32: @|addr: &mut[1] i32, val: i32| atomic( 8u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_min_global_u32: @|addr: &mut[1] u32, val: u32| atomic(10u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_min_global_u64: @|addr: &mut[1] u64, val: u64| atomic(10u, bitcast[&mut u64](addr), val, 7u, ""),

		atomic_max_global_i32: @|addr: &mut[1] i32, val: i32| atomic(7u, bitcast[&mut i32](addr), val, 7u, ""),
		atomic_max_global_u32: @|addr: &mut[1] u32, val: u32| atomic(9u, bitcast[&mut u32](addr), val, 7u, ""),
		atomic_max_global_u64: @|addr: &mut[1] u64, val: u64| atomic(9u, bitcast[&mut u64](addr), val, 7u, ""),

		// atomic_cas_global_u16: cuda_atomic_cas_global_u16,
		atomic_cas_global_i32: @|addr: &mut[1] i32, cmp: i32, new: i32| cmpxchg(bitcast[&mut i32](addr), cmp, new, 7u, "")(0),
		atomic_cas_global_u32: @|addr: &mut[1] u32, cmp: u32, new: u32| cmpxchg(bitcast[&mut u32](addr), cmp, new, 7u, "")(0),
		atomic_cas_global_i64: @|addr: &mut[1] u64, cmp: u64, new: u64| cmpxchg(bitcast[&mut u64](addr), cmp, new, 7u, "")(0),

		atomic_inc_global_u32: @|addr: &mut[1] u32, val: u32| -> u32 {
			let ptr = bitcast[&mut u32](addr);

			// this is the generic version
			let mut old:u32 = atomic_load(ptr, 7u, "");
			let mut cmp:bool = false;
			while(!cmp) {
				let new = if old >= val { 0u } else { old + 1u };
				let res = cmpxchg(ptr, old, new, 7u, "");
				old = res(0);
				cmp = res(1);
			}

			// this is optimized for val being 2**n-1
			/*
			// atomic_add
			let old:u32 = atomic(1u, ptr, 1u, 7u, "");
			if old >= val {
				// atomic_and to mask overflow bits
				// however, we must ensure that addr will not be read outside of this function
				atomic(3u, ptr, val, 7u, "");
			}
			*/

			old
		},
		atomic_dec_global_u32: @|addr: &mut[1] u32, val: u32| -> u32 {
			let ptr = bitcast[&mut u32](addr);

			// this is the generic version
			let mut old:u32 = atomic_load(ptr, 7u, "");
			let mut cmp:bool = false;
			while(!cmp) {
				let new = if (old == 0u) || (old > val) { val } else { old - 1u };
				let res = cmpxchg(ptr, old, new, 7u, "");
				old = res(0);
				cmp = res(1);
			}

			// this is optimized for val being 2**n-1
			/*
			// atomic sub
			let old:u32 = atomic(2u, ptr, 1u, 7u, "");
			if old == 0u {
				// atomic and to mask overflow bits
				// however, we must ensure that addr will not be read outside of this function
				atomic(3u, ptr, val, 7u, "");
			}
			*/

			old
		},

		yield: @|| { fibers_yield(); }
	});
}

fn @cpu_subwarp(membermask: u64, num_threads: fn () -> u32, idx: Index, body: fn (gpu_wave_context) -> ()) -> () {

	fn @warp_shuffle_i32(value: i32, src_lane: i32) -> i32 {
		let mut result:i32;
		for i in unroll(0, num_threads() as i32) {
			if i == src_lane {
				result = rv_extract_i32(value, src_lane);
			}
		}
		result
	}

	fn @warp_shuffle_u32(value: u32, src_lane: i32) -> u32 {
		let mut result:u32;
		for i in unroll(0, num_threads() as i32) {
			if i == src_lane {
				result = rv_extract_u32(value, src_lane);
			}
		}
		result
	}

	let self = idx.local_id;

	@@body(gpu_wave_context{
		idx: @|| idx.warp_id,

		membermask: @|| membermask,

		// TODO: return simd_lane
		threads: @|body| cpu_thread(@|i:i32| idx.local_id, body),

		num_threads: num_threads,

		barrier: @|| { },
		barrier_all: @|predicate| { rv_all(predicate) },
		barrier_any: @|predicate| { rv_any(predicate) },
		barrier_count: @|predicate| { cpu_popcount32(rv_ballot(predicate)) },
		barrier_vote: @|predicate| { rv_ballot(predicate) as u64 },

		/* TODO: assert width == num_threads == warp_size */
		shfl_i32: @|x:i32, src_lane:i32, width:u32| { warp_shuffle_i32(x, src_lane) },
		shfl_u32: @|x:u32, src_lane:i32, width:u32| { warp_shuffle_u32(x, src_lane) },
		//shfl_f32: @|x:f32, src_lane:i32, width:u32| { undef[f32]() },

		//shfl_up_i32: @|x:i32, delta:u32, width:u32| { rv_shuffle_i32(x, -(delta as i32)) },
		//shfl_up_u32: @|x:u32, delta:u32, width:u32| { rv_shuffle_u32(x, -(delta as i32)) },
		shfl_up_i32: @|x:i32, delta:u32, width:u32| { warp_shuffle_i32(x, ((self - delta) & (warp_size - 1u)) as i32) },
		shfl_up_u32: @|x:u32, delta:u32, width:u32| { warp_shuffle_u32(x, ((self - delta) & (warp_size - 1u)) as i32) },
		//shfl_up_f32: @|x:f32, delta:u32, width:u32| { undef[f32]() },

		shfl_down_i32: @|x:i32, delta:u32, width:u32| { warp_shuffle_i32(x, (idx.local_id as i32 + delta as i32)) },
		shfl_down_u32: @|x:u32, delta:u32, width:u32| { warp_shuffle_u32(x, (idx.local_id as i32 + delta as i32)) },
		//shfl_down_f32: @|x:f32, delta:u32, width:u32| { undef[f32]() },

		shfl_bfly_i32: @|x:i32, lane_mask:i32, width:u32| { warp_shuffle_i32(x, (idx.local_id ^ lane_mask as u32) as i32) },
		shfl_bfly_u32: @|x:u32, lane_mask:i32, width:u32| { warp_shuffle_u32(x, (idx.local_id ^ lane_mask as u32) as i32) },
		//shfl_bfly_f32: @|x:f32, lane_mask:i32, width:u32| { undef[f32]() },

		lanemask: @|| 1u64 << idx.local_id as u64,
		lanemask_le: @|| undef[u64](),
		lanemask_lt: @|| undef[u64](),
		lanemask_ge: @|| undef[u64](),
		lanemask_gt: @|| undef[u64]()
	});
}

fn @cpu_block(num_warps: fn () -> u32, num_threads: fn () -> u32, idx: Index, addr: fn(i32) -> (&mut i32), body: fn (gpu_group_context) -> ()) -> () {
	@@body(gpu_group_context {
		// TODO: dispatch ndim-block_id
		idx: @|i:i32| idx.block_id,
		waves: @|body| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, idx, body),
		threads: @|body| cpu_thread(@|i:i32| idx.warp_id * warp_size + idx.local_id, body),
		num_waves: num_warps,
		// TODO: dispatch ndim-thread_id
		num_threads: @|i:i32| num_threads(),
		barrier: @|| -> () { fibers_sync_block(idx.block_id as i32); },
		barrier_all: @|predicate:bool| -> bool {
			let warp_pred = rv_all(predicate) as i32;
			// atomic_and_global_i32
			atomic(3u, addr(1), warp_pred, 7u, "");
			fibers_sync_block_with_result(addr(0), addr(1), 1, idx.block_id as i32);
			let result = *(addr(0));
			result != 0
		},
		barrier_any: @|predicate:bool| -> bool {
			let warp_pred = rv_any(predicate) as i32;
			// atomic_or_global_i32
			atomic(5u, addr(2), warp_pred, 7u, "");
			fibers_sync_block_with_result(addr(0), addr(2), 0, idx.block_id as i32);
			let result = *(addr(0));
			result != 0
		},
		barrier_count: @|predicate:bool| -> i32 {
			let warp_count = cpu_popcount32(rv_ballot(predicate));
			// atomic_add_global_i32
			atomic(1u, addr(3), warp_count, 7u, "");
			fibers_sync_block_with_result(addr(0), addr(3), 0, idx.block_id as i32);
			let result = *(addr(0));
			result
		},
	});
}

struct Index {
	block_id: u32,
	warp_id:  u32,
	local_id: u32
}

fn @cpu_launch_1d(grid_dim: i32, block_dim: i32, body: fn (gpu_grid_context, Intrinsics) -> ()) -> () {
	// TODO: assert(warp_size == 32)

	let num_threads_per_block = block_dim as u32;
	let num_warps_per_block = (num_threads_per_block + warp_size - 1u32) / warp_size;
	let num_blocks = grid_dim as u32;
	let num_warps = num_blocks * num_warps_per_block;
	let num_threads = num_blocks * num_threads_per_block;

	// TODO: assert(warp_size/l == vec_width)
	let warp_loop = @|l:u32, body: fn(u32)->()| vectorize(vec_width as i32, @|i:i32| @@body(i as u32));
	//let warp_loop = @|l:u32, body: fn(u32)->()| range(0, l as i32, @|i:i32| body(i as u32));

	// we need a buffer to accumulate block level results of barriers
	let num_block_variables = 4; // output, all, any, count
	let block_pred_buffer = alloc(0, num_block_variables * num_blocks as i32 * sizeof[i32]());
	let addr = bitcast[&mut[i32]](block_pred_buffer.data);

	// reset all variables
	for i in range(0, num_blocks as i32) {
		addr(num_block_variables*i + 1) = 1;
		addr(num_block_variables*i + 2) = 0;
		addr(num_block_variables*i + 3) = 0;
	}

	for block_id, warp_id in fibers(cpu_threads, num_blocks as i32, num_warps_per_block as i32) {

		for local_id in warp_loop(warp_size) {
			let idx = Index {
				block_id: block_id as u32,
				 warp_id:  warp_id as u32,
				local_id: local_id,
			};

			let context = gpu_grid_context {
				device: 0,
				groups: @|body| cpu_block(@|| num_warps_per_block, @|| num_threads_per_block, idx, @|x| { &mut(addr(num_block_variables*block_id + x)) }, body),
				waves: @|body| cpu_subwarp(get_member_mask_u64(warp_size), @|| warp_size, Index { block_id: idx.block_id, warp_id: idx.block_id * num_warps_per_block + idx.warp_id, local_id: idx.local_id }, body),
				threads: @|body| cpu_thread(@|i:i32| idx.block_id * num_threads_per_block + idx.warp_id * warp_size + idx.local_id, body),
				num_groups: @|i:i32| num_blocks,
				num_waves: @|| num_warps,
				num_threads: @|i:i32| num_threads
			};
			@@body(context, cpu_intrinsics)
		}

	}

	release(block_pred_buffer);
}
