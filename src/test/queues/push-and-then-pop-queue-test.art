

fn push_and_then_pop_queue_test[T](num_threads: i32, block_dim: i32, queue_size: i32, create_queue: fn(AccDevice, i32) -> ProducerConsumerQueue[T], test_element: QueueTestElement[T]) -> i32 {
	let device = createDefaultAccDevice();

	let push_buffer_size = num_threads;
	let queue_test = createQueueTest(device, create_queue, (push_buffer_size + 4) as i64 * sizeof[u32]());

	for queue, device_memory, device_failed_flag in queue_test.run_test(queue_size) {
		let push_buffer = device_memory as &mut addrspace(1) [i32];
		let num_pushed = &mut push_buffer(push_buffer_size + 0);
		let num_not_pushed = &mut push_buffer(push_buffer_size + 1);
		let num_popped = &mut push_buffer(push_buffer_size + 2);
		let num_not_popped = &mut push_buffer(push_buffer_size + 3);

		print_string("initialize debug push_buffer ...\n");

		for grid in device.launch_1d(div_up(push_buffer_size + 4, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 < push_buffer_size + 4 {
					push_buffer(thread.idx(0)) = 0;
				}
			}
		}

		device.synchronize();

		print_string("concurrent push until full ...\n");

		fn @fail(i: i32, thread: gpu_thread_context) {
			thread.atomic_or_global_u32(device_failed_flag, 1 << i as u32, memory_order::relaxed);
		}

		for grid in device.launch_1d(div_up(num_threads, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 < num_threads {
					while (thread.atomic_load_global_i32(num_pushed, memory_order::relaxed) < queue_size) {
						if for _ in queue.push(thread) {
							thread.atomic_add_global_i32(push_buffer(thread.idx(0)), 1, memory_order::relaxed);

							test_element.generateElement(thread.idx(0) as i32)
						} > 0 {
							thread.atomic_add_global_i32(num_pushed, 1, memory_order::relaxed);
						}
						else {
							thread.atomic_add_global_i32(num_not_pushed, 1, memory_order::relaxed);
						}
					}
				}
			}
		}

		device.synchronize();

		print_string("check that queue size is consistent ...\n");

		for grid in device.launch_1d(div_up(push_buffer_size, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let size = queue.size(thread);

					if size != *num_pushed {
						device.print_2xi32("ERROR: queue size (%d) does not match number of elements reported pushed (%d)!\n", size, queue_size);
						fail(0, thread);
					}
				}
			}
		}

		device.synchronize();

		print_string("concurrent pop until empty ...\n");

		for grid in device.launch_1d(div_up(num_threads, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 < num_threads {
					while (thread.atomic_load_global_i32(num_popped, memory_order::relaxed) < queue_size) {
						if for el, _ in queue.pop(thread) {
							let i = test_element.unpackRefValue(el);

							thread.atomic_sub_global_i32(push_buffer(i), 1, memory_order::relaxed);
						} > 0 {
							thread.atomic_add_global_i32(num_popped, 1, memory_order::relaxed);
						}
						else {
							thread.atomic_add_global_i32(num_not_popped, 1, memory_order::relaxed);
						}
					}
				}
			}
		}

		device.synchronize();

		print_string("check that all elements have been popped and queue size is consistent ...\n");

		for grid in device.launch_1d(div_up(push_buffer_size, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let size = queue.size(thread);

					if size != 0 {
						device.print_i32("ERROR: queue size (%d) is not 0!\n", size);
						fail(1, thread);
					}
				}

				if thread.idx(0) as i32 < push_buffer_size {
					if push_buffer(thread.idx(0)) != 0 {
						device.print_i32("ERROR: unbalanced push/pop count at slot %u!\n", thread.idx(0) as i32);
						fail(2, thread);
					}
				}
			}
		}
	
		true
	}

	queue_test.finish()
}
