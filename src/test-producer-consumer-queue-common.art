
// mod queue_tests {
	// use super::self;
	// use super::range;
	// use super::sizeof;
	// use super::bitcast;
	// use super::div_up;
	// use super::AccDevice;
	// use super::createAccDevice;
	// use super::ProducerConsumerQueue;
	// use super::QueueElementType;
	// use super::createConcurrentProducerConsumerQueue;
	// use super::createBaseTest;

	struct QueueTest[T] {
		run_test: fn(fn(ProducerConsumerQueue[T], &mut addrspace(1) [i8], &mut addrspace(1) u32) -> bool) -> fn(i32) -> (),
		finish: fn() -> i32
	}
	
	fn createQueueTest[T](device: AccDevice, element_type: QueueElementType[T], additional_mem: i64) -> QueueTest[T] {
		let test = createBaseTest(device, additional_mem);
	
		QueueTest[T] {
			run_test = @|body|@|queue_size| {
				for device_memory, device_failed_flag in test.run_test() {
					let queue = createConcurrentProducerConsumerQueue[T](device, element_type, queue_size);
	
					let block_dim = 256;
					let num_blocks = (queue_size + block_dim - 1) / block_dim;
	
					for grid in device.launch_1d(num_blocks, block_dim) {
						queue.reset(grid);
					}
	
					let cpu_result = body(queue, device_memory, device_failed_flag);
	
					for grid in device.launch_1d(num_blocks, block_dim) {
						queue.validate(device_failed_flag, grid);
					}
	
					device.synchronize();
					queue.release();
	
					cpu_result
				}
			},
	
			finish = @|| {
				test.finish()
			}
		}
	}


	struct QueueTestElement[T] {
		element_type: QueueElementType[T],
		push: fn(ProducerConsumerQueue[T], gpu_thread_context, &mut addrspace(1) [i32], error_cont) -> i32,
		pop: fn(ProducerConsumerQueue[T], gpu_thread_context, &mut addrspace(1) [i32], error_cont) -> i32,
	}


	fn indexTestElement() = QueueTestElement[u32] {
		element_type = indexQueueElementType(),
		push = @|queue: ProducerConsumerQueue[u32], thread: gpu_thread_context, _pushed_buffer: &mut addrspace(1) [i32], _err: error_cont| -> i32 {
			for _i in queue.push(thread) {
				thread.idx(0)
			}
		},
		pop = @|queue: ProducerConsumerQueue[u32], thread: gpu_thread_context, _pushed_buffer: &mut addrspace(1) [i32], _err: error_cont| -> i32 {
			for _el, _i in queue.pop(thread) {
			}
		}
	};


	fn genericTestElement[T](generateElement: fn(i32) -> T, checkElement: fn (T, i32) -> bool) = QueueTestElement[T] {
		element_type = genericQueueElementType[T](),
		push = @|queue: ProducerConsumerQueue[T], thread: gpu_thread_context, pushed_buffer: &mut addrspace(1) [i32], err: error_cont| -> i32 {
			for i in queue.push(thread) {
				let old = thread.atomic_exch_global_i32(&mut pushed_buffer(i), thread.idx(0) as i32);

				if (old != -1) {
					err.throw_print_2xi32("ERROR: queue element %u handed out more than once! (previously to %u)\n", i as i32, old);
				}

				generateElement(thread.idx(0) as i32)
			}
		},
		pop = @|queue: ProducerConsumerQueue[T], thread: gpu_thread_context, pushed_buffer: &mut addrspace(1) [i32], err: error_cont| -> i32 {
			for el, i in queue.pop(thread) {
				let check = thread.atomic_exch_global_i32(&mut pushed_buffer(i), -1);

				if !checkElement(el, check) {
					err.throw_print_2xi32("ERROR: popped queue element %u does not match reference value (%u)!\n", i as i32, check);
				}
			}
		}
	};

	type GenericElementSimple = i64;

	fn genericTestElementSimple() = genericTestElement[GenericElementSimple](
		@|id:i32| -> GenericElementSimple { id as i64 },
		@|el:GenericElementSimple, check:i32| el == check as i64
	);

	struct GenericElementStruct {
		payload: bool,
		check: i32
	}

	fn genericTestElementStruct() = genericTestElement[GenericElementStruct] (
		@|id:i32| GenericElementStruct { payload = id % 3 == 0, check = id },
		@|el:GenericElementStruct, check:i32| el.check == check
	);

	struct GenericElementComplexStruct {
		payload: [u32 * 7],
		check: i32
	}

	fn genericTestElementComplexStruct() = genericTestElement[GenericElementComplexStruct] (
		@|id:i32| GenericElementComplexStruct { payload = [0; 7], check = id },
		@|el:GenericElementComplexStruct, check:i32| el.check == check
	);


	fn push_and_then_pop_queue_test[T](test_element: QueueTestElement[T]) -> i32 {
		let device = createAccDevice();

		let num_elements = 1000000;
		let queue_size = num_elements;
		let queue_test = createQueueTest(device, test_element.element_type, (num_elements + 2) as i64 * sizeof[u32]());
		let block_dim = 256;

		for queue, device_memory, device_failed_flag in queue_test.run_test(queue_size) {
			// keep track of which slots have been handed out to whom.
			let pushed_buffer = bitcast[&mut addrspace(1) [i32]](device_memory);
			let num_pushed = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 0));
			let num_popped = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 1));

			device.print_i32("initialize debug pushed_buffer ...\n", 0);

			for grid in device.launch_1d(div_up(num_elements + 2, block_dim), block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) as i32 >= num_elements {
						if thread.idx(0) as i32 < num_elements + 2 {
							pushed_buffer(thread.idx(0)) = 0;
						}
					}
					else {
						thread.atomic_store_global_i32(&mut pushed_buffer(thread.idx(0)), -1);
					}
				}
			}

			device.synchronize();

			device.print_i32("concurrent push until full ...\n", 0);

			let error_handler = @|thread: gpu_thread_context, flag: u32, cont: fn() -> ()| {
				error_handler(device, @||{ thread.atomic_or_global_u32(device_failed_flag, flag); cont() })
			};

			// concurrent push until full, verify that each element is handed out exactly once
			for grid in device.launch_1d(42, block_dim) {
				for thread in grid.threads() {
					while (
						if test_element.push(queue, thread, pushed_buffer, error_handler(thread, 1 << 0, @||continue())) > 0 {
							thread.atomic_add_global_i32(num_pushed, 1);
							true
						}
						else {
							thread.atomic_load_global_i32(num_pushed) < queue_size
						}
					) {}
				}
			}

			device.synchronize();

			for grid in device.launch_1d(1, block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) == 0 {
						device.print_i32("queue size: %d\n", queue.size(thread));
					}
				}
			}

			// check that all elements have been handed out
			for grid in device.launch_1d(div_up(num_elements, block_dim), block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) < num_elements as u32 {
						//device.print_2xi32("pushed_buffer(%d) = %d\n", thread.idx(0) as i32, pushed_buffer(thread.idx(0)));
						if thread.atomic_load_global_i32(&mut pushed_buffer(thread.idx(0))) < 0 {
							if num_elements == queue_size {
								device.print_i32("ERROR: queue element %u has been skipped!\n", thread.idx(0) as i32);
							}
							thread.atomic_or_global_u32(device_failed_flag, 1 << 1);
						}
					}
				}
			}

			device.synchronize();

			device.print_i32("concurrent pop until empty ...\n", 0);

			// concurrent pop until empty, verify that each element comes from the correct slot
			for grid in device.launch_1d(42, block_dim) {
				for thread in grid.threads() {
					while (
						if test_element.pop(queue, thread, pushed_buffer, error_handler(thread, 1 << 3, @||{continue()})) > 0 {
							thread.atomic_add_global_i32(num_popped, 1);
							true
						}
						else {
							thread.atomic_load_global_i32(num_popped) < queue_size
						}
					) {}
				}
			}

			device.synchronize();

			device.print_i32("check that all elements have been popped ...\n", 0);

			// check that all elements have been popped
			for grid in device.launch_1d(div_up(num_elements, block_dim), block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) as i32 < num_elements {
						if pushed_buffer(thread.idx(0)) != -1 {
							device.print_i32("ERROR: queue element %u has not been popped!\n", thread.idx(0) as i32);
							thread.atomic_or_global_u32(device_failed_flag, 1 << 4);
						}
					}

					if thread.idx(0) == 0 {
						let num_pushed_val = thread.atomic_load_global_i32(num_pushed);
						let num_popped_val = thread.atomic_load_global_i32(num_popped);

						if num_pushed_val != num_elements {
							device.print_2xi32("pushed only %d elements out of %d\n", num_pushed_val, num_elements);
							if num_pushed_val != queue_size {
								thread.atomic_or_global_u32(device_failed_flag, 1 << 5);
							}
						}

						if num_popped_val != num_elements {
							device.print_2xi32("popped only %d elements out of %d\n", num_popped_val, num_elements);
							if num_popped_val != queue_size {
								thread.atomic_or_global_u32(device_failed_flag, 1 << 6);
							}
						}
					}
				}
			}
			true
		}

		queue_test.finish()
	}

	fn concurrent_push_pop_queue_test[T](num_elements: i32, test_element: QueueTestElement[T]) -> i32 {
		let device = createAccDevice();

		let queue_test = createQueueTest(device, test_element.element_type, (num_elements as i64 + 4) * sizeof[u32]());
		let block_dim = 256;

		for queue, device_memory, device_failed_flag in queue_test.run_test(num_elements) {
			// keep track of which slots have been handed out to whom.
			let pushed_buffer = bitcast[&mut addrspace(1) [i32]](device_memory);
			let num_pushed = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 0));
			let num_not_pushed = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 1));
			let num_popped = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 2));
			let num_not_popped = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(num_elements + 3));

			for grid in device.launch_1d(div_up(num_elements + 4, block_dim), block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) as i32 >= num_elements {
						if thread.idx(0) as i32 < num_elements + 4 {
							pushed_buffer(thread.idx(0)) = 0;
						}
					}
					else {
						pushed_buffer(thread.idx(0)) = -1;
					}
				}
			}

			device.synchronize();

			let error_handler = @|thread: gpu_thread_context, flag: u32, cont: fn() -> ()| {
				error_handler(device, @||{ thread.atomic_or_global_u32(device_failed_flag, flag); cont() })
			};

			// concurrent push and pop, verify that each element is handed out exactly once and comes from the correct slot
			for grid in device.launch_1d(42, block_dim) {
				for thread in grid.threads() {
					let mut rand_state = rand::xorseed32(thread.idx(0));

					for _n in range(0, 10) {
						if rand_state as i32 > 0 {
							if test_element.push(queue, thread, pushed_buffer, error_handler(thread, 1 << 0, @||{continue()})) > 0 {
								thread.atomic_add_global_i32(num_pushed, 1);
							}
							else {
								thread.atomic_add_global_i32(num_not_pushed, 1);
							}
						}
						else {
							if test_element.pop(queue, thread, pushed_buffer, error_handler(thread, 1 << 1, @||{continue()})) > 0 {
								thread.atomic_add_global_i32(num_popped, 1);
							}
							else {
								thread.atomic_add_global_i32(num_not_popped, 1);
							}
						}

						rand_state = rand::xorshift32(rand_state);
					}
				}
			}

			device.synchronize();

			// pop remaining elements until empty, verify that each element comes from the correct slot
			for grid in device.launch_1d(42, block_dim) {
				for thread in grid.threads() {
					while (
						if test_element.pop(queue, thread, pushed_buffer, error_handler(thread, 1 << 2, @||{continue()})) > 0 {
							thread.atomic_add_global_i32(num_popped, 1);
							true
						}
						else {
							thread.atomic_load_global_i32(num_popped) < *num_pushed
						}
					) {}
				}
			}

			device.synchronize();

			// check that all elements have been popped
			for grid in device.launch_1d(div_up(num_elements, block_dim), block_dim) {
				for thread in grid.threads() {
					if thread.idx(0) as i32 < num_elements {
						if pushed_buffer(thread.idx(0)) != -1 {
							device.print_i32("ERROR: queue element %u has not been popped!\n", thread.idx(0) as i32);
							thread.atomic_or_global_u32(device_failed_flag, 1 << 3);
						}
					}

					if thread.idx(0) == 0 {
						device.print_3xi32("pushed + not pushed = %d + %d = %d\n", *num_pushed, *num_not_pushed, *num_pushed + *num_not_pushed);
						device.print_3xi32("popped + not popped = %d + %d = %d\n", *num_popped, *num_not_popped, *num_popped + *num_not_popped);

						if *num_popped != *num_pushed {
							device.print_2xi32("ERROR: number of popped elements (%d) does not match number of elements pushed (%d)!\n", *num_popped, *num_pushed);
							thread.atomic_or_global_u32(device_failed_flag, 1 << 4);
						}
					}
				}
			}

			true
		}

		queue_test.finish()
	}
// }
