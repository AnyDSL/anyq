// implementation of queue algorithm described in
// Chaoran Yang and John Mellor-Crummey. 2016. A wait-free queue as fast as fetch-and-add.
// In Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '16), pages 1â€“13.
// https://doi.org/10.1145/2851141.2851168

type PADDING = [u8*4];

struct enq_t {
	id: i32,
	pad: PADDING,
	val: u64, // void*
}

struct deq_t {
	id: i32,
	idx: i32,
}

struct cell_t {
	val: u64, // void*
	enq: &mut enq_t,
	deq: &mut enq_t,
	pad: [u64*5], // void*[5]
}

static WFQUEUE_NODE_SIZE = ((1 << 10) - 2);

struct node_t {
	next: u64, // &mut node_t
	id: i32,
	pad: PADDING,
	cells: [cell_t*1022],
}

type node_ref = u64;

struct queue_t {
	// Index of the next position for enqueue.
	Ei: i32, // DOUBLE_CACHE_ALIGNED
	// Index of the next position for dequeue.
	Di: i32, // DOUBLE_CACHE_ALIGNED
	// Index of the head of the queue.
	Hi: i32, // DOUBLE_CACHE_ALIGNED

	pad1: PADDING,

	// Pointer to the head node of the queue.
	Hp: node_ref, //&mut node_t,

	// Number of processors.
	nprocs: i32,

	pad2: PADDING,
}

type handle_ref = u64;

struct handle_t {
	// Pointer to the next handle.
	next: handle_ref,

	// Hazard pointer.
	Hp: node_ref,
	// Pointer to the node for enqueue.
	Ep: node_ref,
	// Pointer to the node for dequeue.
	Dp: node_ref,

	// Enqueue request.
	Er: enq_t,
	// Dequeue request.
	Dr: deq_t,

	// Handle of the next enqueuer to help.
	Eh: handle_ref,
	Ei: i32,

	pad1: PADDING,

	// Handle of the next dequeuer to help.
	Dh: handle_ref,

	// Pointer to a spare node to use, to speedup adding a new node.
	spare: node_ref,

	// Count the delay rounds of helping another dequeuer.
	delay: i32,

	pad2: PADDING,
}

struct wfqueue_t
{
  q: queue_t,
  size: i32,
  tail: &mut handle_t,
  h: &mut [handle_t],
}

type WFQHandle = &mut wfqueue_t;


static default_num_handles = 128;

static EMPTY:u64 = 0; // void*
static BOT:u64 = 0;
static TOP:u64 = -1 as u64;

static MAX_PATIENCE = 10;
static MAX_SPIN = 100;
fn @MAX_GARBAGE(n:i32) = 2*n;

static mem_acq = memory_order::acquire;
static mem_rel = memory_order::release;
static mem_rlx = memory_order::relaxed;
static mem_seq = memory_order::seq_cst;
static vol_load  = memory_order::relaxed; // memory_order::acquire;
static vol_store = memory_order::relaxed; // memory_order::release;

type NodeAllocator = Allocator[node_t];

fn @do_while(body: fn() -> bool) -> fn()->() {
	@|| {
		let mut cond = body();
		while (cond) {
			cond = body();
		}
	}
}

#[import(cc = "C", name = "memset")] fn cpu_memset(_dst: &mut [i8], _val: i32, _size: i64) -> &mut [i8];


fn @new_node(nodes: NodeAllocator) -> node_ref {
	let n = nodes.alloc();

	/* let p:&mut [u64] = n as &mut [u64];
	for i in range(0, (sizeof[node_t]() / sizeof[u64]()) as i32) {
		p(i) = 0;
	} */

	let p:&mut [i8] = n as &mut [i8];
	for i in range(0, sizeof[node_t]() as i32) {
		p(i) = 0;
	}

	//cpu_memset(n as &mut [i8], 0, sizeof[node_t]());

	n
}

fn wfqueue_create_internal(device: AccDevice, nprocs: i32, nodes: NodeAllocator) -> &mut wfqueue_t
{
	//device.print_3xi32("sizeof(wfqueue_t): %d\nsizeof(queue_t): %d\nsizeof(handle_t): %d\n", sizeof[wfqueue_t]() as i32, sizeof[queue_t]() as i32, sizeof[handle_t]() as i32);
	//device.print_3xi32("sizeof(enq_t): %d\nsizeof(deq_t): %d\nsizeof(node_t): %d\n", sizeof[enq_t]() as i32, sizeof[deq_t]() as i32, sizeof[node_t]() as i32);

	let queue_size = sizeof[wfqueue_t]();

	let handles_offset = round_up_i64(queue_size, 64);
	let handles_size = nprocs as i64 * sizeof[handle_t]();

	let dev_queue_buffer = device.alloc(handles_offset + handles_size);
	let dev_data = dev_queue_buffer.data;

	let queue = &dev_data(0) as &mut wfqueue_t;
	let handles = &dev_data(handles_offset) as &mut [handle_t];

	queue.q.Hi = 0;
	queue.q.Hp = new_node(nodes);

	queue.q.Ei = 1;
	queue.q.Di = 1;

	queue.q.nprocs = nprocs;

	queue.size = 0;
	queue.h = handles;

	for i in range(0, nprocs) {
		let th = &mut (*queue).h(i);

		(*th).next = 0;
		(*th).Hp = 0 as node_ref;
		(*th).Ep = queue.q.Hp;
		(*th).Dp = queue.q.Hp;

		(*th).Er.id = 0;
		(*th).Er.val = BOT;
		(*th).Dr.id = 0;
		(*th).Dr.idx = -1;

		(*th).Ei = 0;
		(*th).spare = new_node(nodes);

		(*th).next = &mut queue.h(i + 1) as u64;
		(*th).Eh = th.next;
		(*th).Dh = th.next;
	}

	let last_th = &mut queue.h(queue.q.nprocs - 1);
	(*last_th).next = &mut queue.h(0) as u64;
	(*last_th).Eh = last_th.next;
	(*last_th).Dh = last_th.next;

	(*queue).tail = &mut queue.h(0);

	queue
}

fn wfqueue_destroy_internal(queue: &mut wfqueue_t, nodes: NodeAllocator) -> ()
{
	let mut n:node_ref = queue.q.Hp;

	while (n != 0) {
		let tmp:node_ref = (n as &mut node_t).next;
		nodes.free(n);
		n = tmp;
	}

	for i in range(0, queue.q.nprocs) {
		//cleanup(&queue->q, &queue->h[i]);
		n = queue.h(i).spare;
		if (n != 0 as node_ref) {
			nodes.free(n);
		}
	}

	runtime_release(0, queue as &mut [i8]);
}


fn @volatile_load_u64(thread: thread_context, ptr: &u64) -> u64 = thread.atomic_load_global_u64(ptr as &addrspace(1) u64, vol_load);
fn @volatile_load_i32(thread: thread_context, ptr: &i32) -> i32 = thread.atomic_load_global_i32(ptr as &addrspace(1) i32, vol_load);
fn @volatile_store_u64(thread: thread_context, ptr: &mut u64, val: u64) -> () = thread.atomic_store_global_u64(ptr as &mut addrspace(1) u64, val, vol_store);
fn @volatile_store_i32(thread: thread_context, ptr: &mut i32, val: i32) -> () = thread.atomic_store_global_i32(ptr as &mut addrspace(1) i32, val, vol_store);



fn @cleanup_internal(device: AccDevice, thread: thread_context, q: &mut queue_t, th: &mut handle_t, nodes: NodeAllocator) -> () {

	fn @update(pPn: &mut node_ref, cur_: node_ref, pHp: &mut node_ref) -> node_ref {
		let mut ptr:node_ref = volatile_load_u64(thread, pPn);
		let mut cur:node_ref = cur_;

		let mut ptr_id:i32 = (*(ptr as &mut node_t)).id;
		let mut cur_id:i32 = (*(cur as &mut node_t)).id;

		if ptr_id < cur_id {
			let res = thread.atomic_cas_global_u64(pPn as &mut addrspace(1) node_ref, ptr, cur, mem_seq, mem_seq);
			if (!res.1) {
				ptr = res.0;
				ptr_id = (*(ptr as &mut node_t)).id;
				if ptr_id < cur_id {
					cur = ptr;
				}
			}

			let Hp:node_ref = volatile_load_u64(thread, pHp);
			if Hp != 0 as node_ref {
				let Hp_id:i32 = (*(Hp as &mut node_t)).id;
				cur_id = (*(cur as &mut node_t)).id;
				if Hp_id < cur_id {
					cur = Hp;
				}
			}
		}

		cur
	}

	let mut oid:i32 = volatile_load_i32(thread, &mut (*q).Hi);
	let mut new:u64 = volatile_load_u64(thread, &mut (*th).Dp);

	if (oid == -1) { return() }
	let mut new_id:i32 = (*(new as &mut node_t)).id;
	if (new_id - oid < MAX_GARBAGE((*q).nprocs)) { return() }
	let res = thread.atomic_cas_global_i32(&mut (*q).Hi as &mut addrspace(1) i32, oid, -1, mem_acq, mem_rlx);
	if (!res.1) { return() }
	oid = res.0;

	let mut old:u64 = volatile_load_u64(thread, (*q).Hp);
	let mut ph:&mut handle_t = th;
	let phs_buffer = device.alloc((*q).nprocs as i64 * sizeof[&mut handle_t]());
	let phs = phs_buffer.data as &mut [&mut handle_t];
	let mut i:i32 = 0;

	for do_while() {
		let Hp:node_ref = thread.atomic_load_global_u64(&mut (*ph).Hp as &mut addrspace(1) u64, mem_acq);
		if Hp != 0 as node_ref {
			let Hp_id:i32 = (*(Hp as &mut node_t)).id;
			new_id = (*(new as &mut node_t)).id;
			if Hp_id < new_id { new = Hp; }
		}

		new = update(&mut (*ph).Ep, new, &mut (*ph).Hp);
		new = update(&mut (*ph).Dp, new, &mut (*ph).Hp);

		phs(i++) = ph;
		ph = (*ph).next as &mut handle_t;
		new_id = (*(new as &mut node_t)).id;

		(new_id > oid) && (ph as u64 != th as u64)
	}

	while (new_id > oid && --i >= 0) {
		let Hp:node_ref = thread.atomic_load_global_u64(&mut (*(phs(i))).Hp as &mut addrspace(1) u64, mem_acq);
		if Hp != 0 as node_ref {
			let Hp_id = (*(Hp as &mut node_t)).id;
			if Hp_id < new_id {
				new = Hp;
				new_id = (*(new as &mut node_t)).id;
			}
		}
	}

	release(phs_buffer);

	let nid:i32 = (*(new as &mut node_t)).id;

	if (nid <= oid) {
		thread.atomic_store_global_i32(&mut (*q).Hi as &mut addrspace(1) i32, oid, mem_rel);
	} else {
		volatile_store_u64(thread, &mut (*q).Hp, new);
		thread.atomic_store_global_i32(&mut (*q).Hi as &mut addrspace(1) i32, nid, mem_rel);

		while (old != new) {
			let tmp:node_ref = volatile_load_u64(thread, &mut (*(old as &mut node_t)).next);
			nodes.free(old);
			old = tmp;
		}
	}
}


fn @find_cell_internal(thread: thread_context, p: &mut node_ref, i: i32, th: &mut handle_t, nodes: NodeAllocator) -> &mut cell_t {

	let mut c:node_ref = volatile_load_u64(thread, p); // as &mut node_t;

	for j in range((*(c as &mut node_t)).id, i / WFQUEUE_NODE_SIZE) {
		let mut n:node_ref = volatile_load_u64(thread, &mut (*(c as &mut node_t)).next); // as &mut node_t;

		if (n == 0) {
			let mut t:node_ref = (*th).spare;

			if (t == 0 as node_ref) {
				t = new_node(nodes);
				(*th).spare = t;
			}

			(*(t as &mut node_t)).id = j + 1;

			let res = thread.atomic_cas_global_u64(&mut (*(c as &mut node_t)).next as &mut addrspace(1) node_ref, n, t, mem_rel, mem_acq);
			if res.1 {
				n = t;
				(*th).spare = 0;
			} else {
				n = res.0;
			}
		}

		c = n;
	}

	volatile_store_u64(thread, p, c);

	let c_ref = c as &mut node_t;
	let c_i:&mut cell_t = &mut (*c_ref).cells(i % WFQUEUE_NODE_SIZE);
	c_i
}

fn @enqueue_internal(thread: thread_context, q: &mut queue_t, th: &mut handle_t, v: u64, nodes: NodeAllocator) -> i32
{
	fn @enq_fast(v: u64, id: &mut i32) -> bool
	{
		let i = thread.atomic_add_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, 1, mem_seq);

		let c = @find_cell_internal(thread, &mut (*th).Ep, i, th, nodes);

		let res = thread.atomic_cas_global_u64(&mut (*c).val as &mut addrspace(1) u64, BOT, v, mem_rlx, mem_rlx);
		if res.1 { true } else { *id = i; false }
	}

	fn @enq_slow(v: u64, id: i32) -> ()
	{
		let enq:&mut enq_t = &mut (*th).Er;
		let mut mid:i32 = id;
	
		volatile_store_u64(thread, &mut (*enq).val, v);
		thread.atomic_store_global_i32(&mut (*enq).id as &mut addrspace(1) i32, mid, mem_rel);

		let mut tail:node_ref = volatile_load_u64(thread, &mut (*th).Ep); // as &mut node_t;

		let mut i:i32;
		let mut c:&mut cell_t;

		// TODO: do-while
		while (volatile_load_i32(thread, &mut (*enq).id) > 0) {
			i = thread.atomic_add_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, 1, mem_rlx);
			c = @find_cell_internal(thread, &mut tail, i, th, nodes);

			let res_c = thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, BOT, enq as u64, mem_seq, mem_seq);
			let val_c = volatile_load_u64(thread, &mut (*c).val);
			if res_c.1 && val_c != TOP {
				let res_id = thread.atomic_cas_global_i32(&mut (*enq).id as &mut addrspace(1) i32, mid, -i, mem_rlx, mem_rlx);
				if res_id.1 {
					mid = -i;
					break()
				}
			}
		} 

		mid = -volatile_load_i32(thread, &mut (*enq).id);
		c = @find_cell_internal(thread, &mut (*th).Ep, mid, th, nodes);

		if (mid > i) {
			let mut Ei:i32 = volatile_load_i32(thread, &mut (*q).Ei);
			while (Ei <= mid) {
				let res_Ei = thread.atomic_cas_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, Ei, mid + 1, mem_rlx, mem_rlx);
				if res_Ei.1 { break() }
				Ei = res_Ei.0;
			}
		}

		volatile_store_u64(thread, &mut (*c).val, v);
	}

	let Ep:node_ref = volatile_load_u64(thread, &mut (*th).Ep);
	volatile_store_u64(thread, &mut (*th).Hp, Ep);

	let mut id:i32;
	let mut p = MAX_PATIENCE;
	while (!enq_fast(v, &mut id) && p-- > 0) { };
	if (p < 0) { enq_slow(v, id); }

	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) node_ref, 0 as node_ref, mem_rel);

	1
}


fn @dequeue_internal(device: AccDevice, thread: thread_context, q: &mut queue_t, th: &mut handle_t, nodes: NodeAllocator) -> u64
{
	fn @spin(ptr: &u64) -> u64 {
		let mut p = MAX_SPIN;
		let mut v = volatile_load_u64(thread, ptr);

		while (v == 0 && p-- > 0) {
			v = volatile_load_u64(thread, ptr);
			//PAUSE();
		}

		v
	}

	fn @help_enq_internal(c: &mut cell_t, i: i32) -> u64
	{
		let v:u64 = spin(&mut (*c).val);

		if (v != TOP && v != BOT) { return(v) }
		if (v == BOT) {
			let res_v = thread.atomic_cas_global_u64(&mut (*c).val as &mut addrspace(1) u64, BOT, TOP, mem_seq, mem_seq);
			if !res_v.1 && res_v.0 != TOP { return (res_v.0) }
		}

		let mut e:u64 = volatile_load_u64(thread, &mut (*c).enq as &mut u64); // as &mut enq_t;

		if (e == BOT) {
			let mut ph:&mut handle_t = (*th).Eh as &mut handle_t;
			let mut pe:&mut enq_t = &mut (*ph).Er;
			let mut id:i32 = volatile_load_i32(thread, &mut (*pe).id);

			if ((*th).Ei != 0 && (*th).Ei != id) {
				(*th).Ei = 0;
				(*th).Eh = (*ph).next;

				ph = (*th).Eh as &mut handle_t;
				pe = &mut (*ph).Er;
				id = volatile_load_i32(thread, &mut (*pe).id);
			}

			if (id > 0 && id <= i) {
				let res = thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, e, pe as u64, mem_rlx, mem_rlx);
				e = res.0;
				if !res.1 {
					(*th).Ei = id;
				} else {
					(*th).Eh = (*ph).next;
				}
			} else {
				(*th).Eh = (*ph).next;
			}

			if (e == BOT) {
				thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, e, TOP, mem_rlx, mem_rlx);
				e = TOP;
			}
		}

		if (e == TOP) {
			let Ei:i32 = volatile_load_i32(thread, &mut (*q).Ei);
			if Ei <= i {
				return (BOT)
			} else {
				return (TOP)
			}
		}

		let er:&mut enq_t = e as &mut enq_t;
		let ei:i32 = thread.atomic_load_global_i32(&mut (*er).id as &mut addrspace(1) i32, mem_acq);
		let ev:u64 = thread.atomic_load_global_u64(&mut (*er).val as &mut addrspace(1) u64, mem_acq);

		if (ei > i) {
			let val:u64 = volatile_load_u64(thread, &mut (*c).val);
			let Ei:i32 = volatile_load_i32(thread, &mut (*q).Ei);
			if (val == TOP && Ei <= i) { return(BOT) }
		} else {
			let res_ei = thread.atomic_cas_global_i32(&mut (*er).id as &mut addrspace(1) i32, ei, -i, mem_rlx, mem_rlx);
			let val:u64 = volatile_load_u64(thread, &mut (*c).val);
			if ((ei > 0 && res_ei.1) || (ei == -i && val == TOP))
			{
				let mut Ei:i32 = volatile_load_i32(thread, &mut (*q).Ei);
				let mut cas:bool = false;
				while (Ei <= i && !cas) {
					let res = thread.atomic_cas_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, Ei, i+1, mem_rlx, mem_rlx);
					Ei = res.0;
					cas = res.1;
				}
				volatile_store_u64(thread, &mut (*c).val, ev);
			}
		}

		volatile_load_u64(thread, &mut (*c).val)
	}
	
	fn @help_deq_internal(ph: &mut handle_t) -> ()
	{
		let deq:&mut deq_t = (*ph).Dr;
		let mut idx:i32 = thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, mem_acq);
		let id:i32 = volatile_load_i32(thread, &mut (*deq).id);

		if (idx < id) { return() }

		let mut Dp:u64 = volatile_load_u64(thread, &mut (*ph).Dp);
		volatile_store_u64(thread, &mut (*ph).Hp, Dp);
		thread.memory_barrier(mem_seq);
		idx = volatile_load_i32(thread, &mut (*deq).idx);

		let mut i:i32 = id + 1;
		let mut old:i32 = id;
		let mut new:i32 = 0;

		while (true) {
			let mut h:u64 = Dp;
			let mut c:&mut cell_t;

			while (idx == old && new == 0) {
				c = @find_cell_internal(thread, &mut h, i, th, nodes);
				let v:u64 = help_enq_internal(c, i);
				let c_deq:u64 = volatile_load_u64(thread, &mut (*c).deq as &mut u64);
				if (v == BOT || (v != TOP && c_deq == BOT)) {
					new = i;
				} else {
					idx = thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, mem_acq);
				}
				++i;
			}

			if (new != 0) {
				let res = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, new, mem_rel, mem_acq);
				if res.1 { idx = new; } else { idx = res.0; }
				if idx >= new { new = 0; }
			}

			let d_id:i32 = volatile_load_i32(thread, &mut (*deq).id);
			if (idx < 0 || d_id != id) { break() }

			c = @find_cell_internal(thread, &mut Dp, idx, th, nodes);
			let mut cd:u64 = BOT; // &mut deq_t
			let c_val:u64 = volatile_load_u64(thread, &mut (*c).val);
			if (c_val == TOP) {
				let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, mem_rlx, mem_rlx);
				idx = res_idx.0;
				break ()
			} else {
				let res_d = thread.atomic_cas_global_u64(&mut (*c).deq as &mut addrspace(1) u64, cd, deq as u64, mem_rlx, mem_rlx);
				cd = res_d.0;
				if res_d.1 {
					let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, mem_rlx, mem_rlx);
					idx = res_idx.0;
					break ()
				} else {
					if cd == deq as u64 {
						let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, mem_rlx, mem_rlx);
						idx = res_idx.0;
						break ()
					}
				}
			}

			old = idx;
			if (idx >= i) { i = idx + 1; }
		}
	}

	fn @deq_fast(id: &mut i32) -> u64
	{
		let i:i32 = thread.atomic_add_global_i32(&mut (*q).Di as &mut addrspace(1) i32, 1, mem_seq);
		let c:&mut cell_t = @find_cell_internal(thread, &mut (*th).Dp, i, th, nodes);
		let v:u64 = help_enq_internal(c, i);

		if (v == BOT) {
			BOT
		} else {
			let res = thread.atomic_cas_global_u64(&mut (*c).deq as &mut addrspace(1) u64, BOT, TOP, mem_rlx, mem_rlx);
			if (v != TOP && res.1) {
				v
			} else {
				*id = i;
				TOP
			}
		}
	}

	fn @deq_slow(id: i32) -> u64
	{
		let deq:&mut deq_t = &mut (*th).Dr;

		thread.atomic_store_global_i32(&mut (*deq).id as &mut addrspace(1) i32, id, mem_rel);
		thread.atomic_store_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, id, mem_rel);

		help_deq_internal(th);
		let i = -volatile_load_i32(thread, &mut (*deq).idx);
		let c:&mut cell_t = @find_cell_internal(thread, &mut (*th).Dp, i, th, nodes);

		let val:u64 = volatile_load_u64(thread, &mut (*c).val);

		let mut Di:i32 = volatile_load_i32(thread, &mut (*q).Di);
		while (Di <= i) {
			let res_Di = thread.atomic_cas_global_i32(&mut (*q).Di as &mut addrspace(1) i32, Di, i + 1, mem_rlx, mem_rlx);
			if res_Di.1 { break() }
			Di = res_Di.0;
		}

		if val == TOP { BOT } else { val }
	}

	let Dp:u64 = volatile_load_u64(thread, &mut (*th).Dp);
	volatile_store_u64(thread, &mut (*th).Hp, Dp);

	let mut v:u64 = TOP;
	let mut id:i32;
	let mut p = MAX_PATIENCE;

	for do_while () {
		v = deq_fast(&mut id);
		v == TOP && p-- > 0
	}

	if (v == TOP) {
		v = deq_slow(id);
	}

	if (v != EMPTY) {
		// TODO: this kills the vectorizer with division by zero
		let Dh = (*th).Dh as &mut handle_t;
		help_deq_internal(Dh);
		(*th).Dh = Dh.next;
	}

	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) u64, 0, mem_rel);

	if ((*th).spare == 0 as node_ref) {
		cleanup_internal(device, thread, q, th, nodes);
		(*th).spare = new_node(nodes);
	}

	v
}



fn @createYangMellorCrummeyQueue(device: AccDevice, _queue_size: i32) -> ProducerConsumerQueue[u32] {

	let use_static_pool = true;
	let num_max_nodes = 1 << 14;
	let nodes_align = 4096;

	let nodes = if use_static_pool {
		createPool[node_t](num_max_nodes, nodes_align)
	} else {
		fn @alloc_node() -> node_ref {
			let s = sizeof[node_t]();
			// alignment of 4k
			let b = alloc_host(device.platform_device, s);
			// alignment of 32
			//let b = cpu_alloc(s);
			b.data as node_ref
		}

		fn @release_node(node: node_ref) -> () {
			runtime_release(0, node as &mut [i8]);
		}

		Allocator[node_t] {
			alloc = @|| { alloc_node() },
			free = @|n:node_ref| { release_node(n); },
			alloc_ptr = @|| { assert(false, "alloc_ptr() not supported for Allocator[node_t]"); 0 as &mut node_t },
			free_ptr = @|_:&mut node_t| { assert(false, "free_ptr() not supported for Allocator[node_t]"); },
			clear = @|| { },
			release = @|| { },
			alignment = nodes_align as i64
		}
	};

	let mut queue:WFQHandle = wfqueue_create_internal(device, default_num_handles, nodes);

	//let q = (*queue).q;
	//device.print_i32("queue.q.Ei  %d\n", q.Ei);
	//device.print_i32("queue.q.Di  %d\n", q.Di);
	//device.print_i32("queue.q.Hi  %d\n", q.Hi);
	//device.print_i32("queue.q.Hp  0x%x\n", q.Hp as i32);
	//device.print_i32("queue.q.nprocs  %d\n", q.nprocs);
	//print_string("queue.tail        "); print_i64((*queue).tail as i64); print_char('\n');
	//print_string("queue.h(0)        "); print_i64((&mut (*queue).h(0)) as i64); print_char('\n');
	//print_string("queue.h(-1).next  "); print_i64((*queue).h(q.nprocs - 1).next as i64); print_char('\n');

	ProducerConsumerQueue[u32] {
		push = @|source:fn()->u32| @|thread| {
			let id = thread.uid();
			assert(0 <= id && id < (*queue).q.nprocs, "thread handle out of range");
			let value = source() as u64 | 0x00ff000000000000;
			let success = enqueue_internal(thread, &mut (*queue).q, &mut (*queue).h(id), value, nodes);
			//device.print_3xi32("push handle: %d - %d / %d\n", handle, value as i32, success);
			if success > 0 {
				thread.atomic_add_global_i32(&mut (*queue).size as &mut addrspace(1) i32, 1, mem_rlx);
				1
			} else { 0 }
		},

		pop = @|sink| @|thread| {
			let id = thread.uid();
			assert(0 <= id && id < (*queue).q.nprocs, "thread handle out of range");
			let mut value:u32 = thread.idx(0);

			let v = dequeue_internal(device, thread, &mut (*queue).q, &mut (*queue).h(id), nodes);
			let success = (v != EMPTY);
			value = (v & 0xffffffff) as u32;

			//device.print_3xi32("pop handle: %d - %d / %d\n", handle, value as i32, success);

			if success {
				thread.atomic_sub_global_i32(&mut (*queue).size as &mut addrspace(1) i32, 1, mem_rlx);
				sink(value);
				1
			} else {
				0
			}
		},

		size = @|thread| {
			thread.atomic_load_global_i32(&mut (*queue).size as &addrspace(1) i32, mem_rlx)
		},

		reset = @|grid| {
			let num_handles = grid.max_concurrency();

			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					wfqueue_destroy_internal(queue, nodes);
					nodes.clear();
					queue = wfqueue_create_internal(device, num_handles, nodes);
				}
			}
		},

		validate = @|_corrupted, _grid| {
		},

		release = @|| {
			wfqueue_destroy_internal(queue, nodes);
			nodes.release();
		}
	}
}
