// implementation of queue algorithm described in
// Chaoran Yang and John Mellor-Crummey. 2016. A wait-free queue as fast as fetch-and-add.
// In Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '16), pages 1â€“13.
// https://doi.org/10.1145/2851141.2851168

type PADDING = [u8*4];

struct enq_t {
  id: i32,
  pad: PADDING,
  val: u64, // void*
}

struct deq_t {
  id: i32,
  idx: i32,
}

struct cell_t {
  val: u64, // void*
  enq: &mut enq_t,
  deq: &mut enq_t,
  pad: [u64*5], // void*[5]
}

static WFQUEUE_NODE_SIZE = ((1 << 10) - 2);

struct node_t {
  next: u64, // &mut node_t
  id: i32,
  pad: PADDING,
  cells: [cell_t*1022],
}

type node_ref = u64;

struct queue_t {
  /**
   * Index of the next position for enqueue.
   */
  // DOUBLE_CACHE_ALIGNED
  Ei: i32,

  /**
   * Index of the next position for dequeue.
   */
  // DOUBLE_CACHE_ALIGNED
  Di: i32,

  /**
   * Index of the head of the queue.
   */
  // DOUBLE_CACHE_ALIGNED
  Hi: i32,

  pad1: PADDING,

  /**
   * Pointer to the head node of the queue.
   */
  Hp: node_ref, //&mut node_t,

  /**
   * Number of processors.
   */
  nprocs: i32,

  pad2: PADDING,
}

struct handle_t {
  /**
   * Pointer to the next handle.
   */
  next: u64, // &mut handle_t

  /**
   * Hazard pointer.
   */
  Hp: node_ref, //&mut node_t,

  /**
   * Pointer to the node for enqueue.
   */
  Ep: node_ref, //&mut node_t,

  /**
   * Pointer to the node for dequeue.
   */
  Dp: node_ref, //&mut node_t,

  /**
   * Enqueue request.
   */
  Er: enq_t,

  /**
   * Dequeue request.
   */
  Dr: deq_t,

  /**
   * Handle of the next enqueuer to help.
   */
  Eh: u64, // &mut handle_t

  Ei: i32,

  pad1: PADDING,

  /**
   * Handle of the next dequeuer to help.
   */
  Dh: u64, // &mut handle_t

  /**
   * Pointer to a spare node to use, to speedup adding a new node.
   */
  spare: node_ref, //&mut node_t,

  /**
   * Count the delay rounds of helping another dequeuer.
   */
  delay: i32,

  pad2: PADDING,
}

struct wfqueue_t
{
  q: queue_t,
  size: i32,
  tail: &mut handle_t,
  h: &mut [handle_t],
}

type WFQHandle = &mut wfqueue_t;


static default_num_handles = 128;

static EMPTY:u64 = 0; // void*
static BOT:u64 = 0;
static TOP:u64 = -1 as u64;

static MAX_PATIENCE = 10;
static MAX_SPIN = 100;

fn new_node(device: AccDevice) -> node_ref {
	let s = sizeof[node_t]();
	let b = device.alloc(s);
	for i in range(0, s as i32) {
		b.data(i) = 0;
	}
	b.data as node_ref
}

fn release_node(node: node_ref) -> () {
	runtime_release(0, node as &mut [i8]);
}

fn wfqueue_create_internal(device: AccDevice, nprocs: i32, queue_ptr: &mut &mut wfqueue_t) -> i32
{
	//device.print_3xi32("sizeof(wfqueue_t): %d\nsizeof(queue_t): %d\nsizeof(handle_t): %d\n", sizeof[wfqueue_t]() as i32, sizeof[queue_t]() as i32, sizeof[handle_t]() as i32);
	//device.print_3xi32("sizeof(enq_t): %d\nsizeof(deq_t): %d\nsizeof(node_t): %d\n", sizeof[enq_t]() as i32, sizeof[deq_t]() as i32, sizeof[node_t]() as i32);

	let queue_size = sizeof[wfqueue_t]();

	let handles_offset = round_up_i64(queue_size, 64);
	let handles_size = nprocs as i64 * sizeof[handle_t]();

	let dev_queue_buffer = device.alloc(handles_offset + handles_size);
	let dev_data = dev_queue_buffer.data;

	let queue = &dev_data(0) as &mut wfqueue_t;
	let handles = &dev_data(handles_offset) as &mut [handle_t];

	queue.q.Hi = 0;
	queue.q.Hp = new_node(device);

	queue.q.Ei = 1;
	queue.q.Di = 1;

	queue.q.nprocs = nprocs;

	queue.size = 0;
	queue.h = handles;

	for i in range(0, nprocs) {
		let th = &mut (*queue).h(i);

		(*th).next = 0;
		(*th).Hp = 0 as node_ref;
		(*th).Ep = queue.q.Hp;
		(*th).Dp = queue.q.Hp;

		(*th).Er.id = 0;
		(*th).Er.val = BOT;
		(*th).Dr.id = 0;
		(*th).Dr.idx = -1;

		(*th).Ei = 0;
		(*th).spare = new_node(device);

		(*th).next = &mut queue.h(i + 1) as u64;
		(*th).Eh = th.next;
		(*th).Dh = th.next;
	}

	let last_th = &mut queue.h(queue.q.nprocs - 1);
	(*last_th).next = &mut queue.h(0) as u64;
	(*last_th).Eh = last_th.next;
	(*last_th).Dh = last_th.next;

	(*queue).tail = &mut queue.h(0);

	*queue_ptr = queue;

	1
}

fn wfqueue_destroy_internal(queue: &mut wfqueue_t) -> i32
{
	let mut n:node_ref = queue.q.Hp;

	while (n != 0) {
		let tmp = (n as &mut node_t).next;
		release_node(n);
		n = tmp;
	}

	for i in range(0, queue.q.nprocs) {
		//cleanup(&queue->q, &queue->h[i]);
		n = queue.h(i).spare;
		if (n != 0 as node_ref) {
			release_node(n);
		}
	}

	runtime_release(0, queue as &mut [i8]);

	1
}


fn @find_cell_internal(device: AccDevice, thread: thread_context, p: &node_ref, i: i32, th: &mut handle_t) -> &mut cell_t {

	let mut c:node_ref = thread.atomic_load_global_u64(p as &addrspace(1) node_ref, memory_order::seq_cst); // as &mut node_t;

	for j in range((*(c as &mut node_t)).id, i / WFQUEUE_NODE_SIZE) {
		let mut n:node_ref = thread.atomic_load_global_u64(&mut (*(c as &mut node_t)).next as &mut addrspace(1) node_ref, memory_order::seq_cst); // as &mut node_t;

		if (n == 0) {
			let mut t:node_ref = (*th).spare;

			if (t == 0 as node_ref) {
				t = new_node(device);
				(*th).spare = t;
			}

			(*(t as &mut node_t)).id = j + 1;

			let res = thread.atomic_cas_global_u64(&mut (*(c as &mut node_t)).next as &mut addrspace(1) node_ref, n, t, memory_order::seq_cst, memory_order::seq_cst);
			if res.1 {
				n = t;
				(*th).spare = 0;
			} else {
				n = res.0;
			}
		}

		c = n;
	}

	thread.atomic_store_global_u64(p as &mut addrspace(1) node_ref, c, memory_order::seq_cst);

	let c_ref = c as &mut node_t;
	let c_i:&mut cell_t = &mut (*c_ref).cells(i % WFQUEUE_NODE_SIZE);
	c_i
}

fn enqueue_internal(device: AccDevice, thread: thread_context, q: &mut queue_t, th: &mut handle_t, v: u64) -> i32
{
	fn enq_fast(v: u64, id: &mut i32) -> bool
	{
		let i = thread.atomic_add_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, 1, memory_order::seq_cst);

		//let c = find_cell(&mut (*th).Ep, i, th);
		let c = @find_cell_internal(device, thread, &mut (*th).Ep, i, th);

		let res = thread.atomic_cas_global_u64(&mut (*c).val as &mut addrspace(1) u64, BOT, v, memory_order::seq_cst, memory_order::seq_cst);
		if res.1 { true } else { *id = i; false }
		//false
	}

	fn enq_slow(v: u64, id: i32) -> ()
	{
		let enq:&mut enq_t = &mut (*th).Er;
		let mut mid:i32 = id;
	
		thread.atomic_store_global_u64(&mut (*enq).val as &mut addrspace(1) u64, v, memory_order::seq_cst);
		thread.atomic_store_global_i32(&mut (*enq).id as &mut addrspace(1) i32, mid, memory_order::seq_cst);

		let mut tail:node_ref = thread.atomic_load_global_u64(&mut (*th).Ep as &mut addrspace(1) node_ref, memory_order::seq_cst); // as &mut node_t;

		let mut i:i32;
		let mut c:&mut cell_t;

		// TODO: do-while
		while (thread.atomic_load_global_i32(&mut (*enq).id as &mut addrspace(1) i32, memory_order::seq_cst) > 0) {
			i = thread.atomic_add_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, 1, memory_order::seq_cst);
			//c = find_cell(&mut tail, i, th);
			c = @find_cell_internal(device, thread, &mut tail, i, th);

			let res_c = thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, BOT, enq as u64, memory_order::seq_cst, memory_order::seq_cst);
			let val_c = thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst);
			if res_c.1 && val_c != TOP {
				let res_id = thread.atomic_cas_global_i32(&mut (*enq).id as &mut addrspace(1) i32, mid, -i, memory_order::seq_cst, memory_order::seq_cst);
				if res_id.1 {
					mid = -i;
					break()
				}
			}
		} 

		mid = -thread.atomic_load_global_i32(&mut (*enq).id as &mut addrspace(1) i32, memory_order::seq_cst);
		//c = find_cell(&mut (*th).Ep, mid, th);
		c = @find_cell_internal(device, thread, &mut (*th).Ep, mid, th);

		if (mid > i) {
			let mut Ei:i32 = thread.atomic_load_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, memory_order::seq_cst);
			while (Ei <= mid) {
				let res_Ei = thread.atomic_cas_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, Ei, mid + 1, memory_order::seq_cst, memory_order::seq_cst);
				if res_Ei.1 { break() }
				Ei = res_Ei.0;
			}
		}

		thread.atomic_store_global_u64(&mut (*c).val as &mut addrspace(1) u64, v, memory_order::seq_cst);
	}

	let Ep:node_ref = thread.atomic_load_global_u64(&mut (*th).Ep as &mut addrspace(1) node_ref, memory_order::seq_cst);
	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) node_ref, Ep, memory_order::seq_cst);

	let mut id:i32;
	let mut p = MAX_PATIENCE;
	while (!enq_fast(v, &mut id) && p-- > 0) { };
	if (p < 0) { enq_slow(v, id); }

	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) node_ref, 0 as node_ref, memory_order::seq_cst);

	1
}


fn dequeue_internal(device: AccDevice, thread: thread_context, q: &mut queue_t, th: &mut handle_t) -> u64
{
	fn spin(ptr: &mut u64) -> u64 {
		let mut p = MAX_SPIN;
		let mut v = thread.atomic_load_global_u64(ptr as &mut addrspace(1) u64, memory_order::seq_cst);

		while (v == 0 && p-- > 0) {
			v = thread.atomic_load_global_u64(ptr as &mut addrspace(1) u64, memory_order::seq_cst);
			//PAUSE();
		}

		v
	}

	fn help_enq_internal(c: &mut cell_t, i: i32) -> u64
	{
		let v:u64 = spin(&mut (*c).val);

		if (v != TOP && v != BOT) { return(v) }
		if (v == BOT) {
			let res_v = thread.atomic_cas_global_u64(&mut (*c).val as &mut addrspace(1) u64, BOT, TOP, memory_order::seq_cst, memory_order::seq_cst);
			if !res_v.1 && res_v.0 != TOP { return (res_v.0) }
		}

		let mut e:u64 = thread.atomic_load_global_u64(&mut (*c).enq as &mut addrspace(1) u64, memory_order::seq_cst); // as &mut enq_t;

		if (e == BOT) {
			let mut ph:&mut handle_t = (*th).Eh as &mut handle_t;
			let mut pe:&mut enq_t = &mut (*ph).Er;
			let mut id:i32 = thread.atomic_load_global_i32(&mut (*pe).id as &mut addrspace(1) i32, memory_order::seq_cst);

			if ((*th).Ei != 0 && (*th).Ei != id) {
				(*th).Ei = 0;
				(*th).Eh = (*ph).next;

				ph = (*th).Eh as &mut handle_t;
				pe = &mut (*ph).Er;
				id = thread.atomic_load_global_i32(&mut (*pe).id as &mut addrspace(1) i32, memory_order::seq_cst);
			}

			if (id > 0 && id <= i) {
				let res = thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, e, pe as u64, memory_order::seq_cst, memory_order::seq_cst);
				e = res.0;
				if !res.1 {
					(*th).Ei = id;
				} else {
					(*th).Eh = (*ph).next;
				}
			} else {
				(*th).Eh = (*ph).next;
			}

			if (e == BOT) {
				thread.atomic_cas_global_u64(&mut (*c).enq as &mut addrspace(1) u64, e, TOP, memory_order::seq_cst, memory_order::seq_cst);
				e = TOP;
			}
		}

		if (e == TOP) {
			let Ei:i32 = thread.atomic_load_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, memory_order::seq_cst);
			if Ei <= i {
				return (BOT)
			} else {
				return (TOP)
			}
		}

		let er:&mut enq_t = e as &mut enq_t;
		let ei:i32 = thread.atomic_load_global_i32(&mut (*er).id as &mut addrspace(1) i32, memory_order::seq_cst);
		let ev:u64 = thread.atomic_load_global_u64(&mut (*er).val as &mut addrspace(1) u64, memory_order::seq_cst);

		if (ei > i) {
			let val:u64 = thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst);
			let Ei:i32 = thread.atomic_load_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, memory_order::seq_cst);
			if (val == TOP && Ei <= i) { return(BOT) }
		} else {
			let res_ei = thread.atomic_cas_global_i32(&mut (*er).id as &mut addrspace(1) i32, ei, -i, memory_order::seq_cst, memory_order::seq_cst);
			let val:u64 = thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst);
			if ((ei > 0 && res_ei.1) || (ei == -i && val == TOP))
			{
				let mut Ei:i32 = thread.atomic_load_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, memory_order::seq_cst);
				let mut cas:bool = false;
				while (Ei <= i && !cas) {
					let res = thread.atomic_cas_global_i32(&mut (*q).Ei as &mut addrspace(1) i32, Ei, i+1, memory_order::seq_cst, memory_order::seq_cst);
					Ei = res.0;
					cas = res.1;
				}
				thread.atomic_store_global_u64(&mut (*c).val as &mut addrspace(1) u64, ev, memory_order::seq_cst);
			}
		}

		thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst)
	}
	
	fn help_deq_internal(ph: &mut handle_t) -> ()
	{
		let deq:&mut deq_t = (*ph).Dr;
		let mut idx:i32 = thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, memory_order::seq_cst);
		let id:i32 = thread.atomic_load_global_i32(&mut (*deq).id as &mut addrspace(1) i32, memory_order::seq_cst);

		if (idx < id) { return() }

		let mut Dp:u64 = thread.atomic_load_global_u64(&mut (*ph).Dp as &mut addrspace(1) u64, memory_order::seq_cst);
		thread.atomic_store_global_u64(&mut (*ph).Hp as &mut addrspace(1) u64, Dp, memory_order::seq_cst);
		//FENCE();
		idx = thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, memory_order::seq_cst);

		let mut i:i32 = id + 1;
		let mut old:i32 = id;
		let mut new:i32 = 0;

		while (true) {
			let mut h:u64 = Dp;
			let mut c:&mut cell_t;

			while (idx == old && new == 0) {
				//c = find_cell(&mut h, i, th);
				c = @find_cell_internal(device, thread, &mut h, i, th);
				//let v:u64 = help_enq(q, th, c, i);
				let v:u64 = help_enq_internal(c, i);
				let c_deq:u64 = thread.atomic_load_global_u64(&mut (*c).deq as &mut addrspace(1) u64, memory_order::seq_cst);
				if (v == BOT || (v != TOP && c_deq == BOT)) {
					new = i;
				} else {
					idx = thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, memory_order::seq_cst);
				}
				++i;
			}

			if (new != 0) {
				let res = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, new, memory_order::seq_cst, memory_order::seq_cst);
				if res.1 { idx = new; } else { idx = res.0; }
				if idx >= new { new = 0; }
			}

			let d_id:i32 = thread.atomic_load_global_i32(&mut (*deq).id as &mut addrspace(1) i32, memory_order::seq_cst);
			if (idx < 0 || d_id != id) { break() }

			//c = find_cell(&mut Dp, idx, th);
			c = @find_cell_internal(device, thread, &mut Dp, idx, th);
			let mut cd:u64 = BOT; // &mut deq_t
			let c_val:u64 = thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst);
			if (c_val == TOP) {
				let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, memory_order::seq_cst, memory_order::seq_cst);
				idx = res_idx.0;
				break ()
			} else {
				let res_d = thread.atomic_cas_global_u64(&mut (*c).deq as &mut addrspace(1) u64, cd, deq as u64, memory_order::seq_cst, memory_order::seq_cst);
				cd = res_d.0;
				if res_d.1 {
					let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, memory_order::seq_cst, memory_order::seq_cst);
					idx = res_idx.0;
					break ()
				} else {
					if cd == deq as u64 {
						let res_idx = thread.atomic_cas_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, idx, -idx, memory_order::seq_cst, memory_order::seq_cst);
						idx = res_idx.0;
						break ()
					}
				}
			}

			old = idx;
			if (idx >= i) { i = idx + 1; }
		}
	}

	fn deq_fast(id: &mut i32) -> u64
	{
		let i:i32 = thread.atomic_add_global_i32(&mut (*q).Di as &mut addrspace(1) i32, 1, memory_order::seq_cst);
		//let c:&mut cell_t = find_cell(&mut (*th).Dp, i, th);
		let c:&mut cell_t = @find_cell_internal(device, thread, &mut (*th).Dp, i, th);
		//let v:u64 = help_enq(q, th, c, i);
		let v:u64 = help_enq_internal(c, i);

		if (v == BOT) {
			BOT
		} else {
			let res = thread.atomic_cas_global_u64(&mut (*c).deq as &mut addrspace(1) u64, BOT, TOP, memory_order::seq_cst, memory_order::seq_cst);
			if (v != TOP && res.1) {
				v
			} else {
				*id = i;
				TOP
			}
		}
	}

	fn deq_slow(id: i32) -> u64
	{
		let deq:&mut deq_t = &mut (*th).Dr;

		thread.atomic_store_global_i32(&mut (*deq).id as &mut addrspace(1) i32, id, memory_order::seq_cst);
		thread.atomic_store_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, id, memory_order::seq_cst);

		//help_deq(q, th, th);
		help_deq_internal(th);
		let i = -thread.atomic_load_global_i32(&mut (*deq).idx as &mut addrspace(1) i32, memory_order::seq_cst);
		//let c:&mut cell_t = find_cell(&mut (*th).Dp, i, th);
		let c:&mut cell_t = find_cell_internal(device, thread, &mut (*th).Dp, i, th);

		let val:u64 = thread.atomic_load_global_u64(&mut (*c).val as &mut addrspace(1) u64, memory_order::seq_cst);

		let mut Di:i32 = thread.atomic_load_global_i32(&mut (*q).Di as &mut addrspace(1) i32, memory_order::seq_cst);
		while (Di <= i) {
			let res_Di = thread.atomic_cas_global_i32(&mut (*q).Di as &mut addrspace(1) i32, Di, i + 1, memory_order::seq_cst, memory_order::seq_cst);
			if res_Di.1 { break() }
			Di = res_Di.0;
		}

		if val == TOP { BOT } else { val }
	}

	let Dp:u64 = thread.atomic_load_global_u64(&mut (*th).Dp as &mut addrspace(1) u64, memory_order::seq_cst);
	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) u64, Dp, memory_order::seq_cst);

	let mut v:u64 = TOP;
	let mut id:i32;
	let mut p = MAX_PATIENCE;

	// TODO: do-while
	while (v == TOP && p-- > 0) {
		v = deq_fast(&mut id);
	}

	if (v == TOP) {
		v = deq_slow(id);
	}

	if (v != EMPTY) {
		// TODO: this kills the vectorizer with division by zero
		let Dh = (*th).Dh as &mut handle_t;
		//help_deq(q, th, Dh);
		help_deq_internal(Dh);
		(*th).Dh = Dh.next;
	}

	thread.atomic_store_global_u64(&mut (*th).Hp as &mut addrspace(1) u64, 0, memory_order::seq_cst);

	if ((*th).spare == 0 as node_ref) {
		//cleanup(q, th);
		(*th).spare = new_node(device);
	}

	v
}




fn @createYangMellorCrummeyQueue(device: AccDevice, _queue_size: i32) -> ProducerConsumerQueue[u32] {
	// let queue_state_size = 80 as i64;

	//let num_handles = 100; //(1 << 21);
	// retrieve max_concurrency from device
	/*
	let dev_num_handles_buffer = device.alloc(sizeof[i32]());
	let dev_num_handles = dev_num_handles_buffer.data as &mut addrspace(1) i32;

	for grid in device.launch_1d(1, block_dim) {
		for thread in grid.threads() {
			if thread.idx(0) == 0 {
				*dev_num_handles = grid.max_concurrency();
			}
		}
	}

	let num_handles_buffer = alloc_cpu(sizeof[i32]());
	copy(dev_num_handles_buffer, num_handles_buffer);
	let num_handles = *(num_handles_buffer.data as &mut i32);
	release(num_handles_buffer);
	release(dev_num_handles_buffer);

	assert(0 < num_handles, "this device does not expose its max_concurrency");

	print_string("num_handles: "); print_i32(num_handles); print_char('\n');
	*/
	// let handle_buffer_size = num_handles * sizeof[YangMellorCrummey::Handle]();
	// let handle_buffer_alignment = alignof[YangMellorCrummey::Handle]();

	// let handle_data_offset = round_up_i64(queue_state_size, handle_buffer_alignment);

	// let queue_device_state_alloc = device.alloc(handle_data_offset + handle_buffer_size);
	// let queue_device_memory = queue_device_state_alloc.data as &mut addrspace(1) [u8];

	// let queue = &mut queue_device_memory(0) as &mut addrspace(1) [u8];
	// let handle = &mut queue_device_memory(handle_data_offset) as &mut addrspace(1) [u8];

	let mut queue:WFQHandle;

	//let success = wfqueue_create(default_num_handles, &mut queue);
	let success = wfqueue_create_internal(device, default_num_handles, &mut queue);
	assert(success > 0, "wfqueue_create() failed");

	let q = (*queue).q;
	device.print_i32("queue.q.Ei  %d\n", q.Ei);
	device.print_i32("queue.q.Di  %d\n", q.Di);
	device.print_i32("queue.q.Hi  %d\n", q.Hi);
	device.print_i32("queue.q.Hp  0x%x\n", q.Hp as i32);
	device.print_i32("queue.q.nprocs  %d\n", q.nprocs);
	print_string("queue.tail        "); print_i64((*queue).tail as i64); print_char('\n');
	print_string("queue.h(0)        "); print_i64((&mut (*queue).h(0)) as i64); print_char('\n');
	print_string("queue.h(-1).next  "); print_i64((*queue).h(q.nprocs - 1).next as i64); print_char('\n');

	ProducerConsumerQueue[u32] {
		push = @|source:fn()->u32| @|thread| {
			let id = thread.uid();
			assert(0 <= id && id < (*queue).q.nprocs, "thread handle out of range");
			let value = source() as u64 | 0x00ff000000000000;
			let success = enqueue_internal(device, thread, &mut (*queue).q, &mut (*queue).h(id), value);
			//device.print_3xi32("push handle: %d - %d / %d\n", handle, value as i32, success);
			if success > 0 {
				thread.atomic_add_global_i32(&mut (*queue).size as &mut addrspace(1) i32, 1, memory_order::seq_cst);
				1
			} else { 0 }
		},

		pop = @|sink| @|thread| {
			let id = thread.uid();
			assert(0 <= id && id < (*queue).q.nprocs, "thread handle out of range");
			let mut value:u32 = thread.idx(0);

			let v = dequeue_internal(device, thread, &mut (*queue).q, &mut (*queue).h(id));
			let success = (v != EMPTY);
			value = (v & 0xffffffff) as u32;

			//device.print_3xi32("pop handle: %d - %d / %d\n", handle, value as i32, success);

			if success {
				thread.atomic_sub_global_i32(&mut (*queue).size as &mut addrspace(1) i32, 1, memory_order::seq_cst);
				sink(value);
				1
			} else {
				0
			}
		},

		size = @|thread| {
			thread.atomic_load_global_i32(&mut (*queue).size as &addrspace(1) i32, memory_order::seq_cst)
		},

		reset = @|grid| {
			let num_handles = grid.max_concurrency();

			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let mut success:i32;
					//success = wfqueue_destroy(queue);
					success = wfqueue_destroy_internal(queue);
					assert(success > 0, "wfqueue_destroy() failed");
					//device.print_i32("num_handles: %d\n", num_handles);
					//success = wfqueue_create(num_handles, &mut queue);
					success = wfqueue_create_internal(device, num_handles, &mut queue);
					assert(success > 0, "wfqueue_create() failed");
				}
			}
			/*
			for thread in grid.threads() {
				for i in range_step(thread.idx(0) as i32, num_handles, grid.num_threads(0) as i32) {
					wfqueue_init(queue, i);
				}
			}
			*/
		},

		validate = @|_corrupted, _grid| {
		},

		release = @|| {
			wfqueue_destroy_internal(queue);
		}
	}
}
