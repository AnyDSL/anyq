// implementation of queue algorithm described in
// Maged M. Michael and Michael L. Scott. 1996. Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms.
// In Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing (PODC '96), pages 267â€“275
// https://doi.org/10.1145/248052.248106

mod MichaelScott {
	struct Queue {
		size: i32,
		head: u32,
		tail: u32,
		alloc_next: u32
	}

	struct QueueNode[T] {
		payload: T,
		next: u32
	}
}

fn @createMichaelScottQueue[T](device: AccDevice, _queue_size: i32) -> ProducerConsumerQueue[T] {
	let pool_size = (1 << 21) * 20 * 2;
	let buffer_size = (pool_size as i64 + 1) * sizeof[MichaelScott::QueueNode[T]]();
	let buffer_alignment = alignof[MichaelScott::QueueNode[T]]();

	let buffer_data_offset = round_up_i64(sizeof[MichaelScott::Queue](), buffer_alignment);

	let queue_device_state_alloc = device.alloc(buffer_data_offset + buffer_size);
	let queue_device_memory = queue_device_state_alloc.data as &mut addrspace(1) [u8];

	let queue = &mut queue_device_memory(0) as &mut addrspace(1) MichaelScott::Queue;
	let node = &mut queue_device_memory(buffer_data_offset) as &mut addrspace(1) [MichaelScott::QueueNode[T]];

	let nil:u32 = -1;

	let alloc_node = @|thread:thread_context| {
		let new_node = thread.atomic_add_global_u32(queue.alloc_next, 1, memory_order::relaxed);

		if new_node < pool_size as u32 {
			new_node
		}
		else {
			thread.atomic_sub_global_u32(queue.alloc_next, 1, memory_order::relaxed);
			nil
		}
	};

	let free_node = @|_:u32, _thread:thread_context| {
		// TODO: stack-based recycling of nodes, don't forget about ABA problem
	};

	ProducerConsumerQueue[T] {
		push = @|source|@|thread| {
			let new_node = alloc_node(thread);

			if (new_node != nil) {
				thread.atomic_add_global_i32(queue.size, 1, memory_order::relaxed);

				node(new_node).payload = source();
				node(new_node).next = nil;

				while true {
					let tail = thread.atomic_load_global_u32(queue.tail, memory_order::acquire);
					let next = thread.atomic_load_global_u32(node(tail).next, memory_order::acquire);

					if tail == thread.atomic_load_global_u32(queue.tail, memory_order::acquire) {
						if next == nil {
							if thread.atomic_cas_global_u32_weak(node(tail).next, next, new_node, memory_order::release, memory_order::relaxed).1 {
								thread.atomic_cas_global_u32(queue.tail, tail, new_node, memory_order::release, memory_order::relaxed);
								break()
							}
						}
						else {
							thread.atomic_cas_global_u32_weak(queue.tail, tail, next, memory_order::release, memory_order::relaxed);
						}
					}
				}

				1
			}
			else {
				0
			}
		},

		pop = @|sink|@|thread| {
			while true {
				let head = thread.atomic_load_global_u32(queue.head, memory_order::acquire);
				let tail = thread.atomic_load_global_u32(queue.tail, memory_order::acquire);
				let next = thread.atomic_load_global_u32(node(head).next, memory_order::acquire);

				if head == thread.atomic_load_global_u32(queue.head, memory_order::acquire) {
					if head == tail {
						if next == nil {
							return(0)
						}

						thread.atomic_cas_global_u32_weak(queue.tail, tail, next, memory_order::release, memory_order::relaxed);
					}
					else {
						let payload = node(next).payload;

						if thread.atomic_cas_global_u32_weak(queue.head, head, next, memory_order::acq_rel, memory_order::relaxed).1 {
							free_node(head, thread);
							sink(payload);
							break()
						}
					}
				}
			}

			thread.atomic_sub_global_i32(queue.size, 1, memory_order::relaxed);

			1
		},

		size = @|thread| {
			thread.atomic_load_global_i32(queue.size, memory_order::relaxed)
		},

		reset = @|grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					queue.size = 0;
					queue.alloc_next = 0;

					// set up dummy node
					queue.head = pool_size as u32;
					queue.tail = pool_size as u32;
					node(pool_size as u32).next = nil;
				}
			}
		},

		validate = @|corrupted, grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let mut size = 0;
					let mut n = queue.head;

					// this regularly causes endless loops
					// (size < pool_size) is the emergency abortion criteria
					while node(n).next != nil && size < pool_size {
						n = node(n).next;
						++size;
					}

					if size != queue.size {
						thread.atomic_store_global_u32(corrupted, 1, memory_order::relaxed);
						device.print_2xi32("VALIDATION ERROR: size of queue (%d) does not match queue size (%d)!\n", size, queue.size);
					}

					if queue.tail != n {
						thread.atomic_store_global_u32(corrupted, 2, memory_order::relaxed);
						device.print_2xi32("VALIDATION ERROR: tail (%u) does not match last node (%u)!\n", queue.tail as i32, n as i32);
					}
				}
			}
		},

		release = @|| {
			release(queue_device_state_alloc);
		}
	}
}
