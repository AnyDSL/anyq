// implementation of queue algorithm described in
// Bernhard Kerbl, Michael Kenzel, Joerg H. Mueller, Dieter Schmalstieg and Markus Steinberger. 2018. The Broker Queue: A Fast, Linearizable FIFO Queue for Fine-Granular Work Distribution on the GPU.
// In Proceedings of the 2018 International Conference on Supercomputing (ICS '18), pages 76â€“85
// https://doi.org/10.1145/3205289.3205291

mod BWD {
	struct Queue {
		head: u32,
		tail: u32,
		size: i32,
	}
}

enum bwd_alloc_result {
	Ok(Buffer),
	Failed
}

struct BQAllocator {
	alloc: fn(i64) -> bwd_alloc_result,
	release: fn(Buffer) -> ()
}

fn @createBrokerQueue_internal[T](queue_size: i32, allocator: BQAllocator, bwd: bool, reference_impl: bool) -> create_queue_result[T] {
	if queue_size < 0 {
		return(create_queue_result[T]::Err("invalid queue size"))
	}

	if !is_pot(queue_size as u32) {
		return(create_queue_result[T]::Err("queue size must be power of two"))
	}

	let tickets_offset = 0 as i64;
	let buffer_offset = round_up_i64(tickets_offset + queue_size as i64 * sizeof[u32](), alignof[T]());
	let queue_offset = round_up_i64(buffer_offset + queue_size as i64 * sizeof[T](), sizeof[BWD::Queue]());

	let alloc_size = queue_offset + sizeof[BWD::Queue]();

	let queue_device_alloc = match allocator.alloc(alloc_size) {
		bwd_alloc_result::Ok(buffer) => { buffer },
		bwd_alloc_result::Failed => {
			return(create_queue_result[T]::Err("failed to allocate queue memory"))
		}
	};

	let queue_device_memory = queue_device_alloc.data as &mut addrspace(1) [u8];

	let tickets = &mut queue_device_memory(tickets_offset) as &mut addrspace(1) [u32];
	let buffer = &mut queue_device_memory(buffer_offset) as &mut addrspace(1) [T];
	let queue = &mut queue_device_memory(queue_offset) as &mut addrspace(1) BWD::Queue;

	// fn @atomic_head_tail(thread: thread_context) {
	// 	let ht = thread.atomic_load_global_u64(&mut queue.head as &mut addrspace(1) u64, memory_order::relaxed);
	// 	((ht & 0xFFFFFFFF) as u32, (ht >> 32) as u32)
	// }

	fn @wait_for_ticket(i: u32, number: u32, thread: thread_context) -> () {
		let load_ticket = if reference_impl {
			@|| thread.atomic_load_global_u32(&mut tickets(i), memory_order::relaxed)
		}
		else {
			// @|| thread.atomic_xor_global_u32(&mut tickets(i), 0, memory_order::relaxed)
			@|| thread.atomic_load_global_u32(&mut tickets(i), memory_order::relaxed)
		};

		thread.wait(@|| load_ticket() == number, "BWD waiting for ticket");
	}

	fn @ensure_dequeue(thread: thread_context) -> bool {
		let mut num = if reference_impl {
			thread.atomic_load_global_i32(&mut queue.size, memory_order::relaxed)
		}
		else {
			thread.atomic_xor_global_i32(&mut queue.size, 0, memory_order::relaxed)
		};

		while true {
			if num <= 0 {
				return(false)
			}

			if thread.atomic_sub_global_i32(&mut queue.size, 1, memory_order::relaxed) > 0 {
				break()
			}

			num = thread.atomic_add_global_i32(&mut queue.size, 1, memory_order::relaxed) + 1;
		}

		true
	}

	fn @ensure_enqueue(thread: thread_context) -> bool {
		let mut num = if reference_impl {
			thread.atomic_load_global_i32(&mut queue.size, memory_order::relaxed)
		}
		else {
			thread.atomic_xor_global_i32(&mut queue.size, 0, memory_order::relaxed)
		};

		while true {
			if num >= queue_size {
				return(false)
			}

			if thread.atomic_add_global_i32(&mut queue.size, 1, memory_order::relaxed) < queue_size {
				break()
			}

			num = thread.atomic_sub_global_i32(&mut queue.size, 1, memory_order::relaxed) - 1;
		}

		true
	}

	fn @read_data(sink: fn(T) -> (), thread: thread_context) -> () {
		let store_ticket = if reference_impl {
			@|p:u32, value:u32| { thread.atomic_store_global_u32(&mut tickets(p), value, memory_order::release); }
		}
		else {
			// @|p:u32, value:u32| { thread.atomic_exch_global_u32(&mut tickets(p), value, memory_order::release); }
			@|p:u32, value:u32| { thread.atomic_store_global_u32(&mut tickets(p), value, memory_order::release); }
		};

		let pos = thread.atomic_add_global_u32(&mut queue.head, 1, memory_order::relaxed);
		let p = pos % queue_size as u32;
		wait_for_ticket(p, 2 * (pos / queue_size as u32) + 1, thread);
		thread.memory_barrier(memory_order::acquire);
		let val = buffer(p);
		store_ticket(p, 2 * ((pos + queue_size as u32) / queue_size as u32));

		sink(val);
	}

	fn @put_data(source: fn() -> T, thread: thread_context) -> () {
		let store_ticket = if reference_impl {
			@|p:u32, value:u32| { thread.atomic_store_global_u32(&mut tickets(p), value, memory_order::release); }
		}
		else {
			// @|p:u32, value:u32| { thread.atomic_exch_global_u32(&mut tickets(p), value, memory_order::release); }
			@|p:u32, value:u32| { thread.atomic_store_global_u32(&mut tickets(p), value, memory_order::release); }
		};

		let val = source();

		let pos = thread.atomic_add_global_u32(&mut queue.tail, 1, memory_order::relaxed);
		let p = pos % queue_size as u32;
		wait_for_ticket(p, 2 * (pos / queue_size as u32), thread);
		buffer(p) = val;
		store_ticket(p, 2 * (pos / queue_size as u32) + 1);
	}

	create_queue_result[T]::Ok(ProducerConsumerQueue[T] {
		push = @|source| @|thread| {
			if bwd {
				if ensure_enqueue(thread) {
					put_data(source, thread);
					1
				}
				else {
					0
				}
			}
			else {
				0
				// while !ensure_enqueue() {
				// 	let (head, tail) = atomic_head_tail(thread);
				// 	if N <= tail - head < N + MaxThreads/2 {
				// 		return(0)
				// 	}
				// }

				// put_data(source, thread);
				// 1
			}
		},

		pop = @|sink| @|thread| {
			if bwd {
				if ensure_dequeue(thread) {
					read_data(sink, thread);
					1
				}
				else {
					0
				}
			}
			else {
				0
				// while !ensure_enqueue() {
				// 	let (head, tail) = atomic_head_tail(thread);
				// 	if N + MaxThreads/2 <= tail - head - 1 {
				// 		return(0)
				// 	}
				// }

				// read_data(sink, thread);
				// 1
			}
		},

		size = @|thread| {
			if reference_impl {
				thread.atomic_load_global_i32(&mut queue.size, memory_order::relaxed)
			}
			else {
				thread.atomic_load_global_i32(&mut queue.size, memory_order::relaxed)
			}
		},

		reset = @|grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					queue.size = 0;
					queue.head = 0;
					queue.tail = 0;
				}

				for i in range_step(thread.idx(0) as i32, queue_size, grid.num_threads(0) as i32) {
					tickets(i) = 0;
				}
			}
		},

		validate = @|_corrupted, _grid| {
		},

		release = @|| {
			allocator.release(queue_device_alloc);
		}
	})
}

fn @bwd_dynamic_alloc(device: AccDevice) = BQAllocator {
	alloc = @|size: i64| bwd_alloc_result::Ok(device.alloc(size)),
	release = @|buffer| release(buffer)
};

fn @createBrokerWorkDistributorQueue[T](device: AccDevice, queue_size: i32) {
	createBrokerQueue_internal[T](queue_size, bwd_dynamic_alloc(device), true, false)
}

fn @createBrokerWorkDistributorQueueOrig[T](device: AccDevice, queue_size: i32) {
	createBrokerQueue_internal[T](queue_size, bwd_dynamic_alloc(device), true, true)
}

static mut bwd_static_queue_buffer: [u8 * 536870912];

fn @bwd_static_alloc(device: AccDevice) = BQAllocator {
	alloc = @|size: i64| {
		if size <= 536870912 && (device.platform_device & 0xF) != 0 {
			//                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
			//            cannot access static buffer from non-host device
			bwd_alloc_result::Ok(Buffer {
				data = &mut bwd_static_queue_buffer as &mut [i8],
				size = size,
				device = device.platform_device
			})
		}
		else {
			bwd_alloc_result::Failed
		}
	},
	release = @|_buffer| {}
};

fn @createBrokerWorkDistributorQueueStatic[T](device: AccDevice, queue_size: i32) {
	createBrokerQueue_internal[T](queue_size, bwd_static_alloc(device), true, false)
}

fn @createBrokerWorkDistributorQueueOrigStatic[T](device: AccDevice, queue_size: i32) {
	createBrokerQueue_internal[T](queue_size, bwd_static_alloc(device), true, true)
}
