// implementation of queue algorithm described in
// Bernhard Kerbl, Michael Kenzel, Joerg H. Mueller, Dieter Schmalstieg and Markus Steinberger. 2018. The Broker Queue: A Fast, Linearizable FIFO Queue for Fine-Granular Work Distribution on the GPU.
// ICS '18: Proceedings of the 2018 International Conference on Supercomputing, pages 76â€“85
// https://doi.org/10.1145/3205289.3205291

struct BrokerWorkDistributorQueue {
	head: u32,
	tail: u32,
	size: i32,
}

fn @createBrokerWorkDistributorQueue_internal[T](queue_size: i32, alloc: fn(i64) -> Buffer, dealloc: fn(Buffer) -> (), reference_impl: bool) -> ProducerConsumerQueue[T] {
	let tickets_offset = 0 as i64;
	let buffer_offset = round_up_i64(tickets_offset + queue_size as i64 * sizeof[u32](), alignof[T]());
	let queue_offset = round_up_i64(buffer_offset + queue_size as i64 * sizeof[T](), sizeof[BrokerWorkDistributorQueue]());

	let alloc_size = queue_offset + sizeof[BrokerWorkDistributorQueue]();

	let queue_device_alloc = alloc(alloc_size);
	let queue_device_memory = queue_device_alloc.data as &mut addrspace(1) [u8];

	let tickets = &mut queue_device_memory(tickets_offset) as &mut addrspace(1) [u32];
	let buffer = &mut queue_device_memory(buffer_offset) as &mut addrspace(1) [T];
	let queue = &mut queue_device_memory(queue_offset) as &mut addrspace(1) BrokerWorkDistributorQueue;


	fn @wait_for_ticket(i: u32, number: u32, thread: thread_context) -> () {
		let load_ticket = if reference_impl {
			@|| thread.atomic_load_global_u32_coalesced(tickets(i), memory_order::relaxed)
		}
		else {
			@|| thread.atomic_load_global_u32(tickets(i), memory_order::relaxed)
		};

		thread.wait(@|| load_ticket() == number, "BWD waiting for ticket");
	}

	fn @ensure_dequeue(thread: thread_context) -> bool {
		let load_size = if reference_impl {
			@|| queue.size
		}
		else {
			@|| thread.atomic_load_global_i32(queue.size, memory_order::relaxed)
		};

		let mut num = load_size();

		let mut ensurance = false;

		while !ensurance && num > 0 {
			if thread.atomic_sub_global_i32(queue.size, 1, memory_order::relaxed) > 0 {
				ensurance = true;
			}
			else {
				num = thread.atomic_add_global_i32(queue.size, 1, memory_order::relaxed) + 1;
			}
		}

		ensurance
	}

	fn @ensure_enqueue(thread: thread_context) -> bool {
		let load_size = if reference_impl {
			@|| queue.size
		}
		else {
			@|| thread.atomic_load_global_i32(queue.size, memory_order::relaxed)
		};

		let mut num = load_size();

		let mut ensurance = false;

		while !ensurance && num < queue_size {
			if thread.atomic_add_global_i32(queue.size, 1, memory_order::relaxed) < queue_size {
				ensurance = true;
			}
			else {
				num = thread.atomic_sub_global_i32(queue.size, 1, memory_order::relaxed) - 1;
			}
		}

		ensurance
	}

	fn @read_data(sink: fn(T) -> (), thread: thread_context) -> () {
		let store_ticket = if reference_impl {
			@|p:u32, value:u32| { thread.atomic_store_global_u32_coalesced(tickets(p), value, memory_order::relaxed); }
		}
		else {
			@|p:u32, value:u32| { tickets(p) = value; }
		};

		let pos = thread.atomic_add_global_u32(queue.head, 1, memory_order::relaxed);
		let p = pos % queue_size as u32;

		wait_for_ticket(p, 2 * (pos / queue_size as u32) + 1, thread);
		let val = buffer(p);
		thread.memory_barrier(memory_order::acq_rel);
		store_ticket(p, 2 * ((pos + queue_size as u32) / queue_size as u32));

		sink(val);
	}

	fn @put_data(source: fn() -> T, thread: thread_context) -> () {
		let store_ticket = if reference_impl {
			@|p:u32, value:u32| { thread.atomic_store_global_u32_coalesced(tickets(p), value, memory_order::relaxed); }
		}
		else {
			@|p:u32, value:u32| { tickets(p) = value; }
		};

		let pos = thread.atomic_add_global_u32(queue.tail, 1, memory_order::relaxed);
		let p = pos % queue_size as u32;
		let b = 2 * (pos / queue_size as u32);

		let val = source();

		wait_for_ticket(p, b, thread);
		buffer(p) = val;
		thread.memory_barrier(memory_order::release);
		store_ticket(p, b + 1);
	}

	ProducerConsumerQueue[T] {
		push = @|source:fn()->T| @|thread:thread_context| -> i32 {
			if ensure_enqueue(thread) {
				put_data(source, thread);
				1
			}
			else {
				0
			}
		},

		pop = @|sink:fn(T)->()| @|thread:thread_context| -> i32 {
			if ensure_dequeue(thread) {
				read_data(sink, thread);
				1
			}
			else {
				0
			}
		},

		size = @|thread| {
			if reference_impl {
				queue.size
			}
			else {
				thread.atomic_load_global_i32(queue.size, memory_order::relaxed)
			}
		},

		reset = @|grid| {
			for thread in grid.threads() {
				let lid = thread.idx(0) as i32;

				if lid == 0 {
					queue.size = 0;
					queue.head = 0;
					queue.tail = 0;
				}
		
				for i in range_step(lid, queue_size, grid.num_threads(0) as i32) {
					tickets(i) = 0;
				}
			}
		},

		validate = @|_corrupted: &mut addrspace(1) u32, _grid| {
		},

		release = @|| {
			dealloc(queue_device_alloc);
		}
	}
}

fn @createBrokerWorkDistributorQueue[T](device: AccDevice, queue_size: i32) {
	createBrokerWorkDistributorQueue_internal[T](queue_size, device.alloc, @|buffer| release(buffer), false)
}

static mut static_queue_buffer: [u8 * 268435456];

fn @bwd_static_alloc(size: i64) {
	assert(size <= 268435456, "size of statically-allocated queue exceeds static allocation size");

	Buffer {
		data = &mut static_queue_buffer as &mut [i8],
		size = -1,
		device = -1
	}
}

fn @createBrokerWorkDistributorQueueStatic[T](_device: AccDevice, queue_size: i32) {
	createBrokerWorkDistributorQueue_internal[T](queue_size, bwd_static_alloc, @|_|{}, false)
}

fn @createBrokerWorkDistributorQueueOrig[T](_device: AccDevice, queue_size: i32) {
	createBrokerWorkDistributorQueue_internal[T](queue_size, bwd_static_alloc, @|_|{}, true)
}
