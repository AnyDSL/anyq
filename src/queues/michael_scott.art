
struct MichaelScottQueue {
	size: i32,
	head: u32,
	tail: u32,
	alloc_next: u32
}

struct MichaelScottQueueNode[T] {
	payload: T,
	next: u32
}

fn @createMichaelScottQueue[T](device: AccDevice, pool_size: i32) -> ProducerConsumerQueue[T] {
	let buffer_size = (pool_size as i64 + 1) * sizeof[MichaelScottQueueNode[T]]();
	let buffer_alignment = alignof[MichaelScottQueueNode[T]]();

	let buffer_data_offset = round_up_i64(sizeof[MichaelScottQueue](), buffer_alignment);

	let queue_device_state_alloc = device.alloc(buffer_data_offset + buffer_size);
	let queue_device_memory = bitcast[&mut addrspace(1) [u8]](queue_device_state_alloc.data);

	let queue = bitcast[&mut addrspace(1) MichaelScottQueue](&mut queue_device_memory(0));
	let node = bitcast[&mut addrspace(1) [MichaelScottQueueNode[T]]](&mut queue_device_memory(buffer_data_offset));

	let nil:u32 = -1;

	let alloc_node = @|thread:gpu_thread_context| {
		let new_node = thread.atomic_add_global_u32(queue.alloc_next, 1, memory_order::relaxed);

		if new_node < pool_size as u32 {
			new_node
		}
		else {
			thread.atomic_sub_global_u32(queue.alloc_next, 1, memory_order::relaxed);
			nil
		}
	};

	let free_node = @|_:u32, _thread:gpu_thread_context| {
		// TODO: stack-based recycling of nodes, don't forget about ABA problem
	};

	ProducerConsumerQueue[T] {
		push = @|source:fn(u32)->T| @|thread:gpu_thread_context| -> i32 {
			let new_node = alloc_node(thread);

			if (new_node != nil) {
				thread.atomic_add_global_i32(queue.size, 1, memory_order::relaxed);

				node(new_node).payload = source(new_node);
				node(new_node).next = nil;

				while true {
					let tail = thread.atomic_load_global_u32(queue.tail, memory_order::relaxed);
					let next = thread.atomic_load_global_u32(node(tail).next, memory_order::relaxed);

					if tail == thread.atomic_load_global_u32(queue.tail, memory_order::relaxed) {
						if next == nil {
							if thread.atomic_cas_global_u32/*_weak*/(node(tail).next, next, new_node, memory_order::/*release*/seq_cst, memory_order::relaxed) == next {
								//                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
								//                                                                     TODO: fix CAS fail memory order
								thread.atomic_cas_global_u32/*_strong*/(queue.tail, tail, new_node, memory_order::relaxed, memory_order::relaxed);
								break()
							}
						}
						else {
							thread.atomic_cas_global_u32/*_weak*/(queue.tail, tail, next, memory_order::relaxed, memory_order::relaxed);
						}
					}
				}

				1
			}
			else {
				0
			}
		},

		pop = @|sink:fn(T, u32)->()| @|thread:gpu_thread_context| -> i32 {
			while true {
				let head = thread.atomic_load_global_u32(queue.head, memory_order::relaxed);
				let tail = thread.atomic_load_global_u32(queue.tail, memory_order::relaxed);
				let next = thread.atomic_load_global_u32(node(head).next, memory_order::acquire);

				if head == thread.atomic_load_global_u32(queue.head, memory_order::relaxed) {
					if head == tail {
						if next == nil {
							return(0)
						}

						thread.atomic_cas_global_u32/*_weak*/(queue.tail, tail, next, memory_order::relaxed, memory_order::relaxed);
					}
					else {
						let payload = node(next).payload;

						if thread.atomic_cas_global_u32/*_weak*/(queue.head, head, next, memory_order::acquire, memory_order::relaxed) == head {
							free_node(head, thread);
							sink(payload, next);
							break()
						}
					}
				}
			}

			thread.atomic_sub_global_i32(queue.size, 1, memory_order::relaxed);

			1
		},

		size = @|thread| {
			thread.atomic_load_global_i32(queue.size, memory_order::relaxed)
		},

		reset = @|grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					queue.size = 0;
					queue.alloc_next = 0;

					// set up dummy node
					queue.head = pool_size as u32;
					queue.tail = pool_size as u32;
					node(pool_size as u32).next = nil;
				}
			}
		},

		validate = @|corrupted: &mut addrspace(1) u32, grid| {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let mut size = 0;
					let mut n = queue.head;

					while node(n).next != nil {
						n = node(n).next;
						++size;
					}

					if size != queue.size {
						thread.atomic_store_global_u32(corrupted, 1, memory_order::relaxed);
						device.print_2xi32("VALIDATION ERROR: size of queue (%d) does not match queue size (%d)!\n", size, queue.size);
					}

					if queue.tail != n {
						thread.atomic_store_global_u32(corrupted, 2, memory_order::relaxed);
						device.print_2xi32("VALIDATION ERROR: tail (%u) does not match last node (%u)!\n", queue.tail as i32, n as i32);
					}
				}
			}
		},

		release = @|| {
			release(queue_device_state_alloc);
		}
	}
}
