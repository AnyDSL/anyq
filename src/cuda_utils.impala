

struct gpu_intrinsics {
	expf: fn(f32) -> f32,
	exp2f: fn(f32) -> f32,
	logf: fn(f32) -> f32,
	log2f: fn(f32) -> f32,
	powf: fn(f32, f32) -> f32,
	rsqrtf: fn(f32) -> f32,
	sqrtf: fn(f32) -> f32,
	fabsf: fn(f32) -> f32,
	sinf: fn(f32) -> f32,
	cosf: fn(f32) -> f32,
	tanf: fn(f32) -> f32,
	asinf: fn(f32) -> f32,
	acosf: fn(f32) -> f32,
	atanf: fn(f32) -> f32,
	erff: fn(f32) -> f32,
	atan2f: fn(f32, f32) -> f32,
	fmaxf: fn(f32, f32) -> f32,
	fminf: fn(f32, f32) -> f32,
	fmodf: fn(f32, f32) -> f32,
	floorf: fn(f32) -> f32,
	fmaf: fn(f32, f32, f32) -> f32,
	madf: fn(f32, f32, f32) -> f32,
	isinff: fn(f32) -> i32,
	isnanf: fn(f32) -> i32,
	isfinitef: fn(f32) -> i32,
	copysignf: fn(f32, f32) -> f32,

	float_as_uint: fn(f32) -> u32,
	uint_as_float: fn(u32) -> f32
}

struct gpu_grid_context {
	device: i32,

	groups: fn (fn (gpu_group_context) -> ()) -> (),
	waves: fn (fn (gpu_wave_context) -> ()) -> (),
	threads: fn (fn (gpu_thread_context) -> ()) -> (),

	num_groups: fn () -> u32,
	num_waves: fn () -> u32,
	num_threads: fn () -> u32,
}

struct gpu_group_context {
	idx: fn () -> u32,

	waves: fn (fn (gpu_wave_context) -> ()) -> (),
	threads: fn (fn (gpu_thread_context) -> ()) -> (),

	barrier: fn () -> (),
	barrier_all: fn (i32) -> i32,
	barrier_any: fn (i32) -> i32,
	barrier_count: fn (i32) -> i32,

	num_waves: fn () -> u32,
	num_threads: fn () -> u32,
}

struct gpu_wave_context {
	idx: fn () -> u32,

	membermask: fn () -> u32,

	threads: fn (fn (gpu_thread_context) -> ()) -> (),

	num_threads: fn () -> u32,

	barrier: fn () -> (),
	barrier_all: fn (i32) -> i32,
	barrier_any: fn (i32) -> i32,
	barrier_count: fn (i32) -> i32,
	barrier_vote: fn (i32) -> u32,

	activemask: fn () -> u32,

	shfl_i32: fn (i32, i32, i32) -> i32,
	shfl_u32: fn (u32, i32, i32) -> u32,
	shfl_i64: fn (i64, i32, i32) -> i64,
	shfl_u64: fn (u64, i32, i32) -> u64,
	shfl_f32: fn (f32, i32, i32) -> f32,
	shfl_f64: fn (f64, i32, i32) -> f64,

	shfl_up_i32: fn (i32, u32, i32) -> i32,
	shfl_up_u32: fn (u32, u32, i32) -> u32,
	shfl_up_i64: fn (i64, u32, i32) -> i64,
	shfl_up_u64: fn (u64, u32, i32) -> u64,
	shfl_up_f32: fn (f32, u32, i32) -> f32,
	shfl_up_f64: fn (f64, u32, i32) -> f64,

	shfl_down_i32: fn (i32, u32, i32) -> i32,
	shfl_down_u32: fn (u32, u32, i32) -> u32,
	shfl_down_i64: fn (i64, u32, i32) -> i64,
	shfl_down_u64: fn (u64, u32, i32) -> u64,
	shfl_down_f32: fn (f32, u32, i32) -> f32,
	shfl_down_f64: fn (f64, u32, i32) -> f64,

	shfl_xor_i32: fn (i32, i32, i32) -> i32,
	shfl_xor_u32: fn (u32, i32, i32) -> u32,
	shfl_xor_i64: fn (i64, i32, i32) -> i64,
	shfl_xor_u64: fn (u64, i32, i32) -> u64,
	shfl_xor_f32: fn (f32, i32, i32) -> f32,
	shfl_xor_f64: fn (f64, i32, i32) -> f64,

	// match_any_i32: fn (i32) -> u32,
	// match_any_u32: fn (u32) -> u32,
	// match_any_i64: fn (i64) -> u32,
	// match_any_u64: fn (u64) -> u32,
	// match_any_f32: fn (f32) -> u32,
	// match_any_f64: fn (f64) -> u32,

	// match_all_i32: fn (i32, &mut i32) -> u32,
	// match_all_u32: fn (u32, &mut i32) -> u32,
	// match_all_i64: fn (i64, &mut i32) -> u32,
	// match_all_u64: fn (u64, &mut i32) -> u32,
	// match_all_f32: fn (f32, &mut i32) -> u32,
	// match_all_f64: fn (f64, &mut i32) -> u32,

	lanemask: fn () -> u32,
	lanemask_le: fn () -> u32,
	lanemask_lt: fn () -> u32,
	lanemask_ge: fn () -> u32,
	lanemask_gt: fn () -> u32
}

struct gpu_thread_context {
	idx: fn () -> u32,

	atomic_add_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_add_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_add_global_u64: fn (&mut[1]u64, u64) -> u64,
	atomic_add_global_f32: fn (&mut[1]f32, f32) -> f32,
	atomic_add_global_f64: fn (&mut[1]f64, f64) -> f64,

	atomic_sub_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_sub_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_sub_global_u64: fn (&mut[1]u64, u64) -> u64,
	atomic_sub_global_f32: fn (&mut[1]f32, f32) -> f32,
	atomic_sub_global_f64: fn (&mut[1]f64, f64) -> f64,

	atomic_and_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_and_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_and_global_u64: fn (&mut[1]u64, u64) -> u64,

	atomic_or_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_or_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_or_global_u64: fn (&mut[1]u64, u64) -> u64,

	atomic_xor_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_xor_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_xor_global_u64: fn (&mut[1]u64, u64) -> u64,

	atomic_exch_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_exch_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_exch_global_u64: fn (&mut[1]u64, u64) -> u64,
	atomic_exch_global_f32: fn (&mut[1]f32, f32) -> f32,

	atomic_min_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_min_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_min_global_u64: fn (&mut[1]u64, u64) -> u64,

	atomic_max_global_i32: fn (&mut[1]i32, i32) -> i32,
	atomic_max_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_max_global_u64: fn (&mut[1]u64, u64) -> u64,

	atomic_cas_global_u16: fn (&mut[1]u16, u16, u16) -> u16,
	atomic_cas_global_i32: fn (&mut[1]i32, i32, i32) -> i32,
	atomic_cas_global_u32: fn (&mut[1]u32, u32, u32) -> u32,
	atomic_cas_global_i64: fn (&mut[1]u64, u64, u64) -> u64,

	atomic_inc_global_u32: fn (&mut[1]u32, u32) -> u32,
	atomic_dec_global_u32: fn (&mut[1]u32, u32) -> u32,

	yield: fn () -> ()
}

fn @cuda_thread(idx: fn () -> u32, body: fn (gpu_thread_context) -> ()) -> () {
	@@body(gpu_thread_context {
		idx: idx,

		atomic_add_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_add_global_i32(location, value),
		atomic_add_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_add_global_u32(location, value),
		atomic_add_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_add_global_u64(location, value),
		atomic_add_global_f32: @|location:&mut[1]f32, value:f32| cuda_atomic_add_global_f32(location, value),
		atomic_add_global_f64: @|location:&mut[1]f64, value:f64| cuda_atomic_add_global_f64(location, value),

		atomic_sub_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_sub_global_i32(location, value),
		atomic_sub_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_sub_global_u32(location, value),
		atomic_sub_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_sub_global_u64(location, value),
		atomic_sub_global_f32: @|location:&mut[1]f32, value:f32| cuda_atomic_sub_global_f32(location, value),
		atomic_sub_global_f64: @|location:&mut[1]f64, value:f64| cuda_atomic_sub_global_f64(location, value),

		atomic_and_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_and_global_i32(location, value),
		atomic_and_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_and_global_u32(location, value),
		atomic_and_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_and_global_u64(location, value),

		atomic_or_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_or_global_i32(location, value),
		atomic_or_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_or_global_u32(location, value),
		atomic_or_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_or_global_u64(location, value),

		atomic_xor_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_xor_global_i32(location, value),
		atomic_xor_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_xor_global_u32(location, value),
		atomic_xor_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_xor_global_u64(location, value),

		atomic_exch_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_exch_global_i32(location, value),
		atomic_exch_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_exch_global_u32(location, value),
		atomic_exch_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_exch_global_u64(location, value),
		atomic_exch_global_f32: @|location:&mut[1]f32, value:f32| cuda_atomic_exch_global_f32(location, value),

		atomic_min_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_min_global_i32(location, value),
		atomic_min_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_min_global_u32(location, value),
		atomic_min_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_min_global_u64(location, value),

		atomic_max_global_i32: @|location:&mut[1]i32, value:i32| cuda_atomic_max_global_i32(location, value),
		atomic_max_global_u32: @|location:&mut[1]u32, value:u32| cuda_atomic_max_global_u32(location, value),
		atomic_max_global_u64: @|location:&mut[1]u64, value:u64| cuda_atomic_max_global_u64(location, value),

		atomic_cas_global_u16: @|location:&mut[1]u16, old:u16, new:u16| cuda_atomic_cas_global_u16(location, old, new),
		atomic_cas_global_i32: @|location:&mut[1]i32, old:i32, new:i32| cuda_atomic_cas_global_i32(location, old, new),
		atomic_cas_global_u32: @|location:&mut[1]u32, old:u32, new:u32| cuda_atomic_cas_global_u32(location, old, new),
		atomic_cas_global_i64: @|location:&mut[1]u64, old:u64, new:u64| cuda_atomic_cas_global_u64(location, old, new),

		atomic_inc_global_u32: @|location:&mut[1]u32, max:u32| cuda_atomic_inc_global_u32(location, max),
		atomic_dec_global_u32: @|location:&mut[1]u32, max:u32| cuda_atomic_dec_global_u32(location, max),

		yield: @|| cuda_nanosleep(0u32)
	});
}

fn @cuda_subwarp(membermask: u32, num_threads: u32, idx: fn () -> u32, body: fn (gpu_wave_context) -> ()) -> () {
	@@body(gpu_wave_context{
		idx: idx,

		membermask: @|| membermask,

		threads: @|body| cuda_thread(cuda_laneid, body),

		num_threads: @|| num_threads,

		barrier: @|| cuda_warp_sync(membermask),
		barrier_all: @|predicate| cuda_warp_sync_all(membermask, predicate),
		barrier_any: @|predicate| cuda_warp_sync_any(membermask, predicate),
		barrier_count: @|predicate| cuda_popc_u32(cuda_warp_sync_vote(membermask, predicate)),
		barrier_vote: @|predicate| cuda_warp_sync_vote(membermask, predicate),

		activemask: cuda_warp_activemask,

		shfl_i32: @|x:i32, src_lane:i32, width:i32| cuda_warp_shfl_i32(membermask, x, src_lane, width),
		shfl_u32: @|x:u32, src_lane:i32, width:i32| cuda_warp_shfl_u32(membermask, x, src_lane, width),
		shfl_i64: @|x:i64, src_lane:i32, width:i32| cuda_warp_shfl_i64(membermask, x, src_lane, width),
		shfl_u64: @|x:u64, src_lane:i32, width:i32| cuda_warp_shfl_u64(membermask, x, src_lane, width),
		shfl_f32: @|x:f32, src_lane:i32, width:i32| cuda_warp_shfl_f32(membermask, x, src_lane, width),
		shfl_f64: @|x:f64, src_lane:i32, width:i32| cuda_warp_shfl_f64(membermask, x, src_lane, width),

		shfl_up_i32: @|x:i32, delta:u32, width:i32| cuda_warp_shfl_up_i32(membermask, x, delta, width),
		shfl_up_u32: @|x:u32, delta:u32, width:i32| cuda_warp_shfl_up_u32(membermask, x, delta, width),
		shfl_up_i64: @|x:i64, delta:u32, width:i32| cuda_warp_shfl_up_i64(membermask, x, delta, width),
		shfl_up_u64: @|x:u64, delta:u32, width:i32| cuda_warp_shfl_up_u64(membermask, x, delta, width),
		shfl_up_f32: @|x:f32, delta:u32, width:i32| cuda_warp_shfl_up_f32(membermask, x, delta, width),
		shfl_up_f64: @|x:f64, delta:u32, width:i32| cuda_warp_shfl_up_f64(membermask, x, delta, width),

		shfl_down_i32: @|x:i32, delta:u32, width:i32| cuda_warp_shfl_down_i32(membermask, x, delta, width),
		shfl_down_u32: @|x:u32, delta:u32, width:i32| cuda_warp_shfl_down_u32(membermask, x, delta, width),
		shfl_down_i64: @|x:i64, delta:u32, width:i32| cuda_warp_shfl_down_i64(membermask, x, delta, width),
		shfl_down_u64: @|x:u64, delta:u32, width:i32| cuda_warp_shfl_down_u64(membermask, x, delta, width),
		shfl_down_f32: @|x:f32, delta:u32, width:i32| cuda_warp_shfl_down_f32(membermask, x, delta, width),
		shfl_down_f64: @|x:f64, delta:u32, width:i32| cuda_warp_shfl_down_f64(membermask, x, delta, width),

		shfl_xor_i32: @|x:i32, lane_mask:i32, width:i32| cuda_warp_shfl_xor_i32(membermask, x, lane_mask, width),
		shfl_xor_u32: @|x:u32, lane_mask:i32, width:i32| cuda_warp_shfl_xor_u32(membermask, x, lane_mask, width),
		shfl_xor_i64: @|x:i64, lane_mask:i32, width:i32| cuda_warp_shfl_xor_i64(membermask, x, lane_mask, width),
		shfl_xor_u64: @|x:u64, lane_mask:i32, width:i32| cuda_warp_shfl_xor_u64(membermask, x, lane_mask, width),
		shfl_xor_f32: @|x:f32, lane_mask:i32, width:i32| cuda_warp_shfl_xor_f32(membermask, x, lane_mask, width),
		shfl_xor_f64: @|x:f64, lane_mask:i32, width:i32| cuda_warp_shfl_xor_f64(membermask, x, lane_mask, width),

		// match_any_i32: @|x:i32| cuda_warp_match_any_i32(membermask, x),
		// match_any_u32: @|x:u32| cuda_warp_match_any_u32(membermask, x),
		// match_any_i64: @|x:i64| cuda_warp_match_any_i64(membermask, x),
		// match_any_u64: @|x:u64| cuda_warp_match_any_u64(membermask, x),
		// match_any_f32: @|x:f32| cuda_warp_match_any_f32(membermask, x),
		// match_any_f64: @|x:f64| cuda_warp_match_any_f64(membermask, x),

		// match_all_i32: @|x:i32, predicate:&mut i32| cuda_warp_match_all_i32(membermask, x, predicate),
		// match_all_u32: @|x:u32, predicate:&mut i32| cuda_warp_match_all_u32(membermask, x, predicate),
		// match_all_i64: @|x:i64, predicate:&mut i32| cuda_warp_match_all_i64(membermask, x, predicate),
		// match_all_u64: @|x:u64, predicate:&mut i32| cuda_warp_match_all_u64(membermask, x, predicate),
		// match_all_f32: @|x:f32, predicate:&mut i32| cuda_warp_match_all_f32(membermask, x, predicate),
		// match_all_f64: @|x:f64, predicate:&mut i32| cuda_warp_match_all_f64(membermask, x, predicate),

		lanemask: cuda_lanemask,
		lanemask_le: cuda_lanemask_le,
		lanemask_lt: cuda_lanemask_lt,
		lanemask_ge: cuda_lanemask_ge,
		lanemask_gt: cuda_lanemask_gt
	});
}

fn @cuda_block(num_threads: fn () -> u32, idx: fn () -> u32, body: fn (gpu_group_context) -> ()) -> () {
	@@body(gpu_group_context {
		idx: idx,
		waves: @|body| cuda_subwarp(!0u32, 32u32, @|| cuda_threadIdx_x() as u32 / 32u32, body),
		threads: @|body| cuda_thread(@|| cuda_threadIdx_x() as u32, body),
		num_waves: @|| (num_threads() + 31u32) / 32u32,
		num_threads: num_threads,
		barrier: cuda_block_sync,
		barrier_all: @|predicate| cuda_block_sync_all(predicate),
		barrier_any: @|predicate| cuda_block_sync_any(predicate),
		barrier_count: @|predicate| cuda_block_sync_count(predicate)
	});
}

fn cuda_launch_1d(device: i32, grid_dim: i32, block_dim: i32, body: fn (gpu_grid_context, gpu_intrinsics) -> ()) -> () {
	// TODO: assert(warp_size == 32)

	let num_threads_per_block = @|| {
		if ?(block_dim as u32) {
			block_dim as u32
		}
		else {
			cuda_blockDim_x() as u32
		}
	};

	let num_warps_per_block = @|| {
		(num_threads_per_block() + 31u32) / 32u32
	};

	let num_blocks = @|| {
		if ?(grid_dim as u32) {
			grid_dim as u32
		}
		else {
			cuda_gridDim_x() as u32
		}
	};

	let num_warps = @|| {
		num_blocks() * num_warps_per_block()
	};

	let num_threads = @|| {
		num_blocks() * num_threads_per_block()
	};

	cuda(device, (grid_dim * block_dim, 1, 1), (block_dim, 1, 1), @|| @@body(gpu_grid_context {
		device: device,
		groups: @|body| cuda_block(num_threads_per_block, @|| cuda_blockIdx_x() as u32, body),
		waves: @|body| cuda_subwarp(!0u32, 32u32, @|| cuda_blockIdx_x() as u32 * num_warps_per_block() + cuda_threadIdx_x() as u32 / 32u32, body),
		threads: @|body| cuda_thread(@|| cuda_blockIdx_x() as u32 * num_threads_per_block() + cuda_threadIdx_x() as u32, body),
		num_groups: num_blocks,
		num_waves: num_warps,
		num_threads: num_threads
	}, cuda_gpu_intrinsics()));
}

extern "device" {
	fn "__float_as_uint" cuda_float_as_uint(f32) -> u32;
	fn "__uint_as_float" cuda_uint_as_float(u32) -> f32;
}

fn @cuda_gpu_intrinsics() -> gpu_intrinsics {
	gpu_intrinsics {
		expf: cuda_expf,
		exp2f: cuda_exp2f,
		logf: cuda_logf,
		log2f: cuda_log2f,
		powf: cuda_powf,
		rsqrtf: cuda_rsqrtf,
		sqrtf: cuda_sqrtf,
		fabsf: cuda_fabsf,
		sinf: cuda_sinf,
		cosf: cuda_cosf,
		tanf: cuda_tanf,
		asinf: cuda_asinf,
		acosf: cuda_acosf,
		atanf: cuda_atanf,
		erff: cuda_erff,
		atan2f: cuda_atan2f,
		fmaxf: cuda_fmaxf,
		fminf: cuda_fminf,
		fmodf: cuda_fmodf,
		floorf: cuda_floorf,
		fmaf: cuda_fmaf,
		madf: cuda_madf,
		isinff: cuda_isinff,
		isnanf: cuda_isnanf,
		isfinitef: cuda_isfinitef,
		copysignf: cuda_copysignf,

		float_as_uint: cuda_float_as_uint,
		uint_as_float : cuda_uint_as_float
	}
}
