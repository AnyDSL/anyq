
fn concurrent_push_pop_queue_test[T](num_threads: i32, block_dim: i32, attempts_per_thread: i32, queue_size: i32, test_element: QueueTestElement[T]) -> i32 {
	let device = createAccDevice();

	let pushed_buffer_size = num_threads * attempts_per_thread;
	let queue_test = createQueueTest(device, test_element.element_type, (pushed_buffer_size as i64 + 5) * sizeof[u32]());

	for queue, device_memory, device_failed_flag in queue_test.run_test(queue_size) {
		let pushed_buffer = bitcast[&mut addrspace(1) [i32]](device_memory);
		let num_pushed = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(pushed_buffer_size + 0));
		let num_not_pushed = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(pushed_buffer_size + 1));
		let num_popped = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(pushed_buffer_size + 2));
		let num_not_popped = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(pushed_buffer_size + 3));
		let num_left_in_push_buffer = bitcast[&mut addrspace(1) i32](&mut pushed_buffer(pushed_buffer_size + 4));

		fn pushed_buffer_addr(idx: u32) -> &mut addrspace(1) i32 { &mut pushed_buffer(idx) };
		fn set_pushed_buffer(thread: gpu_thread_context, idx: u32, value: i32) -> () {
			//device.print_2xi32("pushed_buffer(%d) = %d\n", idx as i32, value);
			thread.atomic_store_global_i32(pushed_buffer_addr(idx), value, memory_order::relaxed);
			//*(pushed_buffer_addr(idx)) = value;
		}
		fn get_pushed_buffer(thread: gpu_thread_context, idx: u32) -> i32 {
			thread.atomic_load_global_i32(pushed_buffer_addr(idx), memory_order::relaxed)
			//*(pushed_buffer_addr(idx))
		}

		device.print_i32("initialize debug pushed_buffer with %d elements ...\n", pushed_buffer_size + 5);

		for grid in device.launch_1d(div_up(pushed_buffer_size + 5, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 >= pushed_buffer_size {
					if thread.idx(0) as i32 < (pushed_buffer_size + 5) {
						set_pushed_buffer(thread, thread.idx(0), 0);
					}
				}
				else {
					set_pushed_buffer(thread, thread.idx(0), -2);
				}
			}
		}

		device.synchronize();

		print_string("concurrent push and pop, verify that each element is within bounds and does not get corrupted ...\n");

		let push = @|thread: gpu_thread_context, ref_value_slot: i32| {
			if for i in queue.push(thread) {
				if i >= queue_size as u32 {
					device.print_2xi32("%d | ERROR: got queue element %d out of bounds to push to!\n", thread.gid() as i32, i as i32);
					thread.atomic_or_global_u32(device_failed_flag, 1 << 0, memory_order::relaxed);
				}

				set_pushed_buffer(thread, ref_value_slot as u32, i as i32);
				thread.memory_barrier(memory_order::release);

				device.print_3xi32("%d | PUSH - element %d for slot %d\n", thread.gid() as i32, i as i32, ref_value_slot);

				test_element.generateElement(ref_value_slot)
			} > 0 {
				thread.atomic_add_global_i32(num_pushed, 1, memory_order::relaxed);
			}
			else {
				thread.atomic_add_global_i32(num_not_pushed, 1, memory_order::relaxed);
			}
		};

		let pop = @|thread: gpu_thread_context| {
			if for el, i in queue.pop(thread) {

				if i >= queue_size as u32 {
					device.print_2xi32("%d | ERROR: got queue element %d out of bounds to pop from!\n", thread.gid() as i32, i as i32);
					thread.atomic_or_global_u32(device_failed_flag, 1 << 1, memory_order::relaxed);
				}

				let ref_value_slot = test_element.unpackRefValue(el);

				device.print_3xi32("%d | POP  - element %d for slot %d\n", thread.gid() as i32, i as i32, ref_value_slot);

				if ref_value_slot >= 0 && ref_value_slot < pushed_buffer_size {
					thread.memory_barrier(memory_order::acquire);

					let pushed_buffer_value = get_pushed_buffer(thread, ref_value_slot as u32);
					if pushed_buffer_value != i as i32 {
						device.print_3xi32("ERROR: popped queue element %d (at index %d) was corrupted (got back index %d)!\n", ref_value_slot, i as i32, pushed_buffer_value);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 2, memory_order::relaxed);
					}

					set_pushed_buffer(thread, ref_value_slot as u32, -1);
					thread.memory_barrier(memory_order::release);
				}
				else {
					device.print_3xi32("ERROR: popped queue element %d (at index %d) was corrupted (got back ref slot %d out of bounds)!\n", ref_value_slot, i as i32, ref_value_slot);
					thread.atomic_or_global_u32(device_failed_flag, 1 << 3, memory_order::relaxed);
				}
			} > 0 {
				thread.atomic_add_global_i32(num_popped, 1, memory_order::relaxed);
			}
			else { 
				thread.atomic_add_global_i32(num_not_popped, 1, memory_order::relaxed);
			}
		};

		// TODO: using multi-threading causes deadlock or atleast an extrem slowdown

		for grid in device.launch_1d(div_up(num_threads, block_dim), block_dim) {
			for wave in grid.waves() {

				// make sure entire wave consistently either pushes or pops
				// TODO: change queue interface to work on entire wave context
				let mut rand_state = wave.idx() % 2;
				//let mut rand_state = rng::xorseed32(wave.idx());

				for thread in wave.threads() {
					let thread_id = wave.idx() * wave.num_threads() + thread.idx(0);

					if thread_id as i32 < num_threads {

						for n in range(0, attempts_per_thread) {
							//device.print_2xi32("wave %d with state %d\n", wave.idx() as i32, rand_state as i32);
							//let n = 0;
							if rand_state as i32 > 0 {
								push(thread, thread_id as i32 * attempts_per_thread + n);
							}
							else {
								pop(thread);
							}

							wave.barrier();

							//rand_state = rng::xorshift32(rand_state);
							thread.yield("wait for other waves before my next iteration");
						}
					}

				}
			}
		}

		device.synchronize();

		print_string("verify queue size, push and pop counters, and pushed_buffer state are consistent ...\n");

		for grid in device.launch_1d(div_up(pushed_buffer_size, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 < pushed_buffer_size {
					if pushed_buffer(thread.idx(0)) >= 0 {
						thread.atomic_add_global_i32(num_left_in_push_buffer, 1, memory_order::relaxed);
					}
				}
			}
		}

		device.synchronize();

		for grid in device.launch_1d(1, 1) {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let size = queue.size(thread);
					let elements_left_reported = *num_pushed - *num_popped;

					if size != elements_left_reported {
						device.print_2xi32("ERROR: queue size (%d) is not consistent with number of elements reported pushed and not popped (%d)!\n", size, elements_left_reported);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 4, memory_order::relaxed);
					}

					if size != *num_left_in_push_buffer {
						device.print_2xi32("ERROR: queue size (%d) is not consistent with number of elements left in pushed_buffer (%d)!\n", size, *num_left_in_push_buffer);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 4, memory_order::relaxed);
					}

					let num_attempts = num_threads * attempts_per_thread;
					let num_attempts_reported = *num_pushed + *num_popped + *num_not_pushed + *num_not_popped;

					if num_attempts_reported != num_attempts {
						device.print_2xi32("ERROR: number of elements reported pushed/popped and not pushed/popped (%d) does not match number of attempts to push/pop (%d)!\n", num_attempts_reported, num_attempts);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 5, memory_order::relaxed);
					}
				}
			}
		}

		device.synchronize();

		print_string("pop remaining elements until empty, verify that each element is within bounds and did not get corrupted ...\n");

		for grid in device.launch_1d(div_up(num_threads, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) as i32 < num_threads {
					while (thread.atomic_load_global_i32(num_popped, memory_order::relaxed) < *num_pushed) {
						pop(thread);
					}
				}
			}
		}

		device.synchronize();

		print_string("check that all elements have been popped and queue size is consistent ...\n");

		for grid in device.launch_1d(div_up(pushed_buffer_size, block_dim), block_dim) {
			for thread in grid.threads() {
				if thread.idx(0) == 0 {
					let size = queue.size(thread);

					if size != 0 {
						device.print_i32("ERROR: queue size (%d) is not 0!\n", size);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 6, memory_order::relaxed);
					}
				}

				if thread.idx(0) as i32 < pushed_buffer_size {
					if pushed_buffer(thread.idx(0)) >= 0 {
						device.print_i32("ERROR: queue element %u has not been popped!\n", thread.idx(0) as i32);
						thread.atomic_or_global_u32(device_failed_flag, 1 << 7, memory_order::relaxed);
					}
				}
			}
		}

		true
	}

	queue_test.finish()
}
